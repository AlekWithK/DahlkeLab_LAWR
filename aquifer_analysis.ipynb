{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES:\n",
    "<ul>\n",
    "<li>Sections labeled <strong>CORE</strong> must be run in order to begin Aquifer Analysis</li>\n",
    "<li>Cells labeled <strong>Control</strong> contain inputs for the immeadiately proceeding section(s)</li>\n",
    "<li>Sections labeled <strong>EXTRA</strong> contain additional plotting or analysis tools but are not necessary for Aquifer Analysis</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nadp.py:44: UserWarning: GDAL not installed. Some functions will not work.\n",
      "  warnings.warn('GDAL not installed. Some functions will not work.')\n"
     ]
    }
   ],
   "source": [
    "#Python3.10\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import contextily as cx\n",
    "import requests\n",
    "import calendar\n",
    "from importlib import reload\n",
    "from typing import IO\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import Point\n",
    "from io import StringIO\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# USGS Data retreival tool\n",
    "from dataretrieval import nwis, utils, codes\n",
    "\n",
    "# Custom modules are imported in multiple locations to faciliate easy reloading when edits are made to their respective files\n",
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "reload(fn)\n",
    "\n",
    "# TODO: Look into the warning that this is disabling. It doesn't appear to be significant for the purposes of this code but should be understood\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#pd.options.mode.chained_assignment = 'warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Single Site Data<br>\n",
    "This function produces the streamflow analysis (using the functions above) for a single site given a list of thresholds and time windows to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "\n",
    "# df2 holds all-time data, df is analyzed range\n",
    "curr_guage = cl.SRB_Guage\n",
    "df = nwis.get_record(sites='09306200', service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2020-09-30')\n",
    "df2 = nwis.get_record(sites=curr_guage.id, service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2014-09-30')\n",
    "\n",
    "# Set to true if running indepedently to verify data with Kocis paper (or other source)\n",
    "# Set to false if running Aquifer Analysis\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Site Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site No: 0    09306200\n",
      "1    09306200\n",
      "2    09306200\n",
      "0    09306200\n",
      "Name: site_no, dtype: object\n",
      "Analyzed Range: 30\n",
      "30\n",
      "50\n",
      "50\n",
      "Quantile: 0.90\n",
      "0.95\n",
      "0.90\n",
      "0.95\n",
      "Valid: True\n",
      "True\n",
      "True\n",
      "True\n",
      "% Missing: 3\n",
      "3\n",
      "2\n",
      "2\n",
      "90%: 34.520\n",
      "53.855\n",
      "53.000\n",
      "76.000\n",
      "HMF Years: 21\n",
      "14\n",
      "30\n",
      "20\n",
      "Annual Duration: 50.714286\n",
      "38.071429\n",
      "59.166667\n",
      "44.450000\n",
      "Event Duration: 15.213152\n",
      "19.197619\n",
      "18.188333\n",
      "17.307063\n",
      "Event HMF: 0.001363\n",
      "0.003407\n",
      "0.002810\n",
      "0.002751\n",
      "Inter-annual Frequency: 70\n",
      "47\n",
      "60\n",
      "40%\n",
      "Intra-annual Frequency: 2.761905\n",
      "2.428571\n",
      "3.133333\n",
      "3.150000\n",
      "Total HMF in km^3/year: 0.005196\n",
      "0.005295\n",
      "0.007876\n",
      "0.008238\n",
      "Center of Mass: 236.571429\n",
      "228.142857\n",
      "214.800000\n",
      "225.800000\n",
      "6 Month HMF in km^3/year: 0.001471\n",
      "0.001448\n",
      "0.002101\n",
      "0.002069\n",
      "3 Month HMF in km^3/year: 0.000025\n",
      "0.000003\n",
      "0.000074\n",
      "0.000006\n"
     ]
    }
   ],
   "source": [
    "# If looking at a post impairment period, but calculating threshold based on the full record of data, pass a second dataframe with a different \n",
    "# start/end date as the final parameter. This method was used in Kocis 2017 and is needed for some data verification, but is not the methodology\n",
    "# used for the Aquifer Analysis and so *args will most often be empty.\n",
    "import Src.func as fn\n",
    "reload(fn)\n",
    "\n",
    "def single_site_data(df: pd.DataFrame, quantiles_list: list, data_ranges_list: list, *args):\n",
    "    df = df.reset_index()    \n",
    "    threshold = None\n",
    "    df_complete_site_data = pd.DataFrame()    \n",
    "    df_complete_mk_mag = pd.DataFrame()\n",
    "    df_complete_mk_dur = pd.DataFrame()\n",
    "    df_complete_mk_intra = pd.DataFrame()\n",
    "    \n",
    "    for data_range in data_ranges_list:\n",
    "        for quantile in quantiles_list:\n",
    "            # Copying original dataframe to avoid any conflicts\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Validate that site is not missing > 10% of data\n",
    "            date_threshold = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * data_range)              \n",
    "            \n",
    "            # Filter dataframe based on current data_range (don't do this during testing if testing unique dataset/range)\n",
    "            if not testing:\n",
    "                df_copy = df_copy[df_copy['datetime'].dt.date >= date_threshold]\n",
    "            \n",
    "            # Validate <10% missing data based on filtered dataframe\n",
    "            missing = fn.validate(df_copy, date_threshold, fn.DEFAULT_END)\n",
    "            valid = missing < fn.MAX_MISSING_THRESHOLD\n",
    "            missing = int(round(missing, 2) * 100)                               \n",
    "    \n",
    "            # Check for optional second dataframe containing full record\n",
    "            if args and isinstance(args[0], pd.DataFrame) and not args[0].empty:\n",
    "                #print('Threshold calculation across the full record')\n",
    "                df2 = args[0].reset_index()\n",
    "                threshold = fn.calc_threshold(df2, quantile)\n",
    "                # threshold = 52350 # SRB_Guage post-impairment threshold for verification with Kocis_2017 \n",
    "            else:\n",
    "                #print('Threshold calculation across a limited record')\n",
    "                threshold = fn.calc_threshold(df_copy, quantile)                             \n",
    "\n",
    "            # Create a dataframe with only days over HMF threshold as well as continuous dataframe for MK trend test\n",
    "            hmf_series_defl, hmf_series_cont = fn.filter_hmf(df_copy, threshold)\n",
    "            #print(zero_deflated_hmf)\n",
    "            \n",
    "            # Aggregate data before performing MK magnitude test\n",
    "            df_agg_cont = hmf_series_cont.copy()\n",
    "            df_agg_cont = fn.convert_hmf(df_agg_cont, threshold)\n",
    "            df_agg_cont['00060_Mean'] = df_agg_cont['00060_Mean'].apply(lambda x: x * fn.CUBIC_FT_KM_FACTOR if x >= 0 else 0)\n",
    "            df_agg_cont.set_index('datetime', inplace=True)\n",
    "            df_agg_cont = df_agg_cont.resample(fn.HYDRO_YEAR).agg({'00060_Mean': ['sum', 'count']})\n",
    "            df_agg_cont.columns = ['Sum', 'Count']\n",
    "            df_agg_cont_defl = df_agg_cont[df_agg_cont['Sum'] > 0]          \n",
    "            \n",
    "            # MK Magnitude\n",
    "            df_mk_mag = fn.mann_kendall(df_agg_cont_defl['Sum'], df_agg_cont['Sum'], fn.MK_TREND_ALPHA)\n",
    "\n",
    "            # Find number of years with HMF\n",
    "            hmf_years = fn.num_hmf_years(hmf_series_defl)            \n",
    "\n",
    "            # Mask out months that don't fall within 3 and 6 month Winter range\n",
    "            df_six_month, df_three_month = fn.three_six_range(hmf_series_defl, 12, 2, 11, 4)\n",
    "\n",
    "            # Convert to daily average flow in cfpd, and take only flow above the threshold\n",
    "            hmf_series_defl = fn.convert_hmf(hmf_series_defl, threshold)\n",
    "            total_hmf_flow = hmf_series_defl[\"00060_Mean\"].sum()\n",
    "            hmf_per_month = fn.monthly_hmf(hmf_series_defl, data_range, quantile)\n",
    "\n",
    "            # Calculate 3 and 6 month HMF\n",
    "            df_six_month = fn.convert_hmf(df_six_month, threshold)\n",
    "            six_month_hmf = df_six_month[\"00060_Mean\"].sum()\n",
    "            df_three_month = fn.convert_hmf(df_three_month, threshold)\n",
    "            three_month_hmf = df_three_month[\"00060_Mean\"].sum()\n",
    "            \n",
    "            total_hmf_flow = (total_hmf_flow * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            six_month_hmf = (six_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            three_month_hmf = (three_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "\n",
    "            # Inter-annual\n",
    "            inter_annual, delta = fn.calc_inter_annual(hmf_series_cont, hmf_years)            \n",
    "\n",
    "            # Average Duration and Intra-annual Frequency\n",
    "            hmf_series_cont = fn.convert_hmf(hmf_series_cont, threshold)\n",
    "            event_duration, annual_duration, intra_annual, event_hmf, df_results = fn.calc_duration_intra_annual(hmf_series_cont, hmf_years)\n",
    "            dur_series_defl = df_results['duration'][df_results['duration'] > 0]\n",
    "            dur_series_cont = df_results['duration']\n",
    "            df_mk_dur = fn.mann_kendall(dur_series_defl, dur_series_cont, fn.MK_TREND_ALPHA)\n",
    "                        \n",
    "            intra_series_defl = df_results['total_events'][df_results['total_events'] > 0]\n",
    "            intra_series_cont = df_results['total_events']\n",
    "            df_mk_intra = fn.mann_kendall(intra_series_defl, intra_series_cont, fn.MK_TREND_ALPHA) \n",
    "            \n",
    "            # Timing Calculation using DOHY\n",
    "            timing = fn.calc_timing(hmf_series_defl)           \n",
    "\n",
    "            # TODO: One-day peaks (avg. # of times hmf occurs on one day only)          \n",
    "            \n",
    "            # Merging site dataframe with Mann-Kendall dataframe. This start date is the beginning of the actual data, not necessarily \n",
    "            # the beginning of the analyzed range. Validation (above) starts from the start of the official range (1970/90-2020)\n",
    "            start = df_copy['datetime'].min().date()\n",
    "            end = df_copy['datetime'].max().date()\n",
    "            data = {'dataset_ID': (data_range * quantile), 'site_no': df_copy.iloc[0]['site_no'], 'analyze_start': start, 'analyze_end': end, 'analyze_range': delta, 'quantile': quantile, \n",
    "                    'valid': valid, 'missing_data%': missing, 'threshold': threshold, 'hmf_years': hmf_years, 'annual_hmf': total_hmf_flow, 'six_mo_hmf': six_month_hmf, 'three_mo_hmf': three_month_hmf, \n",
    "                    'annual_duration': annual_duration, 'event_duration': event_duration, 'event_hmf': event_hmf, 'inter_annual%': inter_annual, 'intra_annual': intra_annual, 'timing': timing}              \n",
    "            \n",
    "            # Merging MK magnitiude\n",
    "            df_mk_mag.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_mag.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_mag = pd.concat([df_complete_mk_mag.reset_index(drop=True), df_mk_mag.reset_index(drop=True)], axis=0) \n",
    "            \n",
    "            # Merging MK duration\n",
    "            df_mk_dur.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_dur.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_dur = pd.concat([df_complete_mk_dur.reset_index(drop=True), df_mk_dur.reset_index(drop=True)], axis=0)   \n",
    "            \n",
    "            # Merging MK intra-annual\n",
    "            df_mk_intra.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_intra.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_intra = pd.concat([df_complete_mk_intra.reset_index(drop=True), df_mk_intra.reset_index(drop=True)], axis=0)        \n",
    "            \n",
    "            # Merging metric results\n",
    "            df_single_iteration = pd.DataFrame(data, index=['0'])\n",
    "            df_single_iteration = pd.concat([df_single_iteration.reset_index(drop=True), hmf_per_month.reset_index(drop=True)], axis=1)\n",
    "            df_complete_site_data = pd.concat([df_complete_site_data.reset_index(drop=True), df_single_iteration.reset_index(drop=True)], axis=0)\n",
    "        \n",
    "    return df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra\n",
    "\n",
    "# For testing purposes, to run this cell independently\n",
    "df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "\n",
    "'''\n",
    "try:\n",
    "    with pd.ExcelWriter('df_single_site.xlsx') as writer:\n",
    "        df_complete_site_data.to_excel(writer, sheet_name='site_metrics', index=False)\n",
    "        df_complete_mk_mag.to_excel(writer, sheet_name='mk_magnitude', index=False)\n",
    "        df_complete_mk_intra.to_excel(writer, sheet_name='mk_intra', index=False)\n",
    "        df_complete_mk_dur.to_excel(writer, sheet_name='mk_duration', index=False)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "'''\n",
    "   \n",
    "fn.single_site_report(df_complete_site_data)\n",
    "df_complete_site_data = fn.gages_2_filtering(df_complete_site_data)\n",
    "#fn.save_data(df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra, 'TEST')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Multi-Site Filtering<br>\n",
    "This function creates the list of sites to analyze by filtering the complete list of state sites with 00060_Mean data down to those that lie within a specific watershed boundary using decimal long/lat positional data and a region shapefile (e.g. state or watershed boundary)<br><br>\n",
    "\n",
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed when running the following cell independently\n",
    "shapefile_path = 'ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site List Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sites: 86 in the state of CA in the given WB\n",
      "['09429000', '09429100', '09429155', '09429180', '09429200', '09429500', '09429600', '09521100', '09523000', '09523200', '09523400', '09523600', '09523800', '09524000', '09524700', '09525000', '09526200', '09530000', '09530500', '11251000', '11253310', '11255575', '11261100', '11261500', '11262900', '11273400', '11274000', '11274500', '11274550', '11274630', '11289850', '11290000', '11303000', '11303500', '11304810', '11311300', '11312672', '11312676', '11312685', '11312968', '11313240', '11313315', '11313405', '11313431', '11313433', '11313434', '11313440', '11313460', '11336600', '11336685', '11336790', '11336930', '11337080', '11370500', '11370700', '11374000', '11376000', '11376550', '11379500', '11389500', '11390500', '11421000', '11424000', '11425500', '11446500', '11447360', '11447650', '11447830', '11447850', '11447890', '11447903', '11447905', '11451800', '11452500', '11452800', '11452900', '11453000', '11455095', '11455140', '11455280', '11455315', '11455338', '11455385', '351813119150601', '360013118575201', '375450121331701']\n"
     ]
    }
   ],
   "source": [
    "def filter_state_site(shapefile_path: str, state_uri: str):\n",
    "    \"\"\"Creates a list of sites with over 50 years of 00060_Mean streamflow data for a given region\"\"\"\n",
    "    # Request page from USGS site, ignore all informational lines\n",
    "    response = requests.get(state_uri)\n",
    "    data = response.text\n",
    "    lines = data.splitlines()\n",
    "    lines = [line for line in lines if not line.startswith('#')]\n",
    "\n",
    "    # Create dataframe where site_no is a list of all sites in a state with 00060 data\n",
    "    tsd = \"\\n\".join(lines)\n",
    "    df = pd.read_csv(StringIO(tsd), sep='\\t')\n",
    "    df_state_sites = df.iloc[1:]\n",
    "        \n",
    "    # Filter out sites outside of HU boundary\n",
    "    if fn.SORT_BY_WB:\n",
    "        shapefile = gpd.read_file(shapefile_path)\n",
    "        df_state_sites['geometry'] = [Point(lon, lat) for lon, lat in zip(df_state_sites['dec_long_va'], df_state_sites['dec_lat_va'])]\n",
    "        gdf_data = gpd.GeoDataFrame(df_state_sites, crs=shapefile.crs)\n",
    "        df_state_sites = gpd.sjoin(gdf_data, shapefile, predicate='within')\n",
    "            \n",
    "    #print(df_state_sites.columns.to_list())\n",
    "    #print(df_state_sites)\n",
    "    \n",
    "    return df_state_sites\n",
    "\n",
    "df_state_sites = filter_state_site(shapefile_path, fn.SITES_URI)\n",
    "print(f'Total Sites: {len(df_state_sites)} in the state of {fn.STATE_CODE.upper()} in the given WB')\n",
    "site_list = df_state_sites['site_no'].to_list()\n",
    "print(site_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: National Metrics Dataset\n",
    "Generates a nationwide dataset for all sites meeting datarange criteria (lengthy runtime!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc2_gdf = gpd.read_file('ShapeFiles/HUC2/_Master_HUC2/master_huc2.shp')\n",
    "huc4_gdf = gpd.read_file('ShapeFiles/HUC4/_Master_HUC4/master_huc4.shp')\n",
    "aq_gdf = gpd.read_file('ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_limit = 50\n",
    "dataset_name = 'National_Metrics'\n",
    "shapefile_path = 'ShapeFiles/Lower48/lower48.shp'\n",
    "\n",
    "# States with few sites for testing purposes\n",
    "test_state_list = ['ME', 'DE']\n",
    "\n",
    "# Set to False to ignore the blacklist and analyze all sites. This could be occasionally\n",
    "# worth doing as USGS could update site data making it valid in the future\n",
    "allow_blacklist = True\n",
    "\n",
    "curr_blacklist = []\n",
    "try:\n",
    "    with open('Prelim_Data/_National_Metrics/National_Sites_Blacklist.txt', 'r') as file:\n",
    "        curr_blacklist = file.read().split(', ')\n",
    "        curr_blacklist = [x.replace(\"'\", \"\") for x in curr_blacklist]\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on ME---]\n",
      "Total Sites: 68 in the state of ME\n",
      "Working on ME site 1/68 (01010000)\n",
      "IGNORED: Site 01010070 is in aquifer blacklist\n",
      "Working on ne site 3/68 (01010500)\n",
      "Working on ne site 4/68 (01011000)\n",
      "Working on ne site 5/68 (01013500)\n",
      "Working on ne site 6/68 (01014000)\n",
      "Working on ne site 7/68 (01015800)\n",
      "Working on ne site 8/68 (01017000)\n",
      "IGNORED: Site 01017290 is in aquifer blacklist\n",
      "IGNORED: Site 01017550 is in aquifer blacklist\n",
      "IGNORED: Site 01017960 is in aquifer blacklist\n",
      "IGNORED: Site 01018009 is in aquifer blacklist\n",
      "IGNORED: Site 01018035 is in aquifer blacklist\n",
      "Working on ne site 14/68 (01019000)\n",
      "Working on ne site 15/68 (01021000)\n",
      "IGNORED: Site 01021470 is in aquifer blacklist\n",
      "IGNORED: Site 01021480 is in aquifer blacklist\n",
      "Working on ne site 18/68 (01022500)\n",
      "IGNORED: Site 01022840 is in aquifer blacklist\n",
      "IGNORED: Site 01027200 is in aquifer blacklist\n",
      "IGNORED: Site 01027240 is in aquifer blacklist\n",
      "IGNORED: Site 01029200 is in aquifer blacklist\n",
      "Working on ne site 23/68 (01029500)\n",
      "IGNORED: Site 01030350 is in aquifer blacklist\n",
      "Working on ne site 25/68 (01030500)\n",
      "IGNORED: Site 01031300 is in aquifer blacklist\n",
      "IGNORED: Site 01031450 is in aquifer blacklist\n",
      "Working on ne site 28/68 (01031500)\n",
      "IGNORED: Site 01031510 is in aquifer blacklist\n",
      "Working on ne site 30/68 (01033000)\n",
      "Working on ne site 31/68 (01034000)\n",
      "Working on ne site 32/68 (01034500)\n",
      "Working on ne site 33/68 (01037000)\n",
      "IGNORED: Site 01037380 is in aquifer blacklist\n",
      "Working on ne site 35/68 (01038000)\n",
      "Working on ne site 36/68 (01042500)\n",
      "Working on ne site 37/68 (01043500)\n",
      "IGNORED: Site 01044550 is in aquifer blacklist\n",
      "Working on ne site 39/68 (01046000)\n",
      "Working on ne site 40/68 (01046500)\n",
      "Working on ne site 41/68 (01047000)\n",
      "IGNORED: Site 01047150 is in aquifer blacklist\n",
      "IGNORED: Site 01047200 is in aquifer blacklist\n",
      "Working on ne site 44/68 (01048000)\n",
      "IGNORED: Site 01048220 is in aquifer blacklist\n",
      "IGNORED: Site 01049265 is in aquifer blacklist\n",
      "Working on ne site 47/68 (01049500)\n",
      "Working on ne site 48/68 (01054200)\n",
      "Working on ne site 49/68 (01054300)\n",
      "Working on ne site 50/68 (01054500)\n",
      "Working on ne site 51/68 (01055000)\n",
      "Working on ne site 52/68 (01055500)\n",
      "Working on ne site 53/68 (01057000)\n",
      "Working on ne site 54/68 (01059000)\n",
      "Working on ne site 55/68 (01060000)\n",
      "IGNORED: Site 01063310 is in aquifer blacklist\n",
      "IGNORED: Site 01064140 is in aquifer blacklist\n",
      "Working on ne site 58/68 (01066000)\n",
      "IGNORED: Site 01067950 is in aquifer blacklist\n",
      "IGNORED: Site 01068910 is in aquifer blacklist\n",
      "Working on ne site 61/68 (01069500)\n",
      "IGNORED: Site 01069700 is in aquifer blacklist\n",
      "Working on ne site 63/68 (01052500)\n",
      "Working on NH site 64/68 (01053500)\n",
      "IGNORED: Site 01053600 is in aquifer blacklist\n",
      "Working on NH site 66/68 (01054000)\n",
      "IGNORED: Site 01054114 is in aquifer blacklist\n",
      "Working on NH site 68/68 (01064500)\n",
      "[---Working on DE---]\n",
      "Total Sites: 36 in the state of DE\n",
      "Working on DE site 1/36 (01463500)\n",
      "Working on NJ site 2/36 (01477800)\n",
      "Working on DE site 3/36 (01478000)\n",
      "IGNORED: Site 01478650 is in aquifer blacklist\n",
      "IGNORED: Site 01478950 is in aquifer blacklist\n",
      "Working on DE site 6/36 (01479000)\n",
      "Working on DE site 7/36 (01480000)\n",
      "IGNORED: Site 01480015 is in aquifer blacklist\n",
      "IGNORED: Site 01480017 is in aquifer blacklist\n",
      "Working on DE site 10/36 (01481500)\n",
      "IGNORED: Site 01482100 is in aquifer blacklist\n",
      "IGNORED: Site 01482695 is in aquifer blacklist\n",
      "IGNORED: Site 01483155 is in aquifer blacklist\n",
      "Working on DE site 14/36 (01483200)\n",
      "Working on DE site 15/36 (01483700)\n",
      "IGNORED: Site 01484080 is in aquifer blacklist\n",
      "Working on DE site 17/36 (01484100)\n",
      "IGNORED: Site 01484525 is in aquifer blacklist\n",
      "IGNORED: Site 01484695 is in aquifer blacklist\n",
      "Working on DE site 20/36 (01487000)\n",
      "Working on DE site 21/36 (01488500)\n",
      "IGNORED: Site 01478245 is in aquifer blacklist\n",
      "IGNORED: Site 01479820 is in aquifer blacklist\n",
      "Working on DE site 24/36 (01481000)\n",
      "IGNORED: Site 0148471320 is in aquifer blacklist\n",
      "Working on PA site 26/36 (01485000)\n",
      "Working on MD site 27/36 (01485500)\n",
      "Working on MD site 28/36 (01486000)\n",
      "Working on MD site 29/36 (01490000)\n",
      "Working on MD site 30/36 (01491000)\n",
      "Working on MD site 31/36 (01491500)\n",
      "Working on MD site 32/36 (01492500)\n",
      "Working on MD site 33/36 (01493000)\n",
      "IGNORED: Site 01493112 is in aquifer blacklist\n",
      "Working on MD site 35/36 (01493500)\n",
      "Working on MD site 36/36 (01495000)\n"
     ]
    }
   ],
   "source": [
    "df_natl_metrics = pd.DataFrame()\n",
    "df_natl_mk_mag = pd.DataFrame()\n",
    "df_natl_mk_dur = pd.DataFrame()\n",
    "df_natl_mk_intra = pd.DataFrame()\n",
    "\n",
    "natl_blacklist = []\n",
    "\n",
    "for state_index, state in enumerate(fn.STATE_LIST):\n",
    "    if state_index >= state_limit: break\n",
    "    \n",
    "    print(f'[---Working on {state}---]')\n",
    "    state_uri = fn.create_state_uri(state, fn.PARAM_CODE)\n",
    "    df_state_sites = filter_state_site(shapefile_path, state_uri)\n",
    "    df_state_sites = df_state_sites.reset_index()\n",
    "    print(f'Total Sites: {len(df_state_sites)} in the state of {state}')\n",
    "    \n",
    "    # Modified version of the create_multi_site_data() function\n",
    "    for site_index, row in df_state_sites.iterrows():\n",
    "        \n",
    "        if allow_blacklist and str(row['site_no']) in curr_blacklist:\n",
    "            print(f'IGNORED: Site {row[\"site_no\"]} is in aquifer blacklist')\n",
    "            continue\n",
    "        \n",
    "        df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "        df = df.reset_index()\n",
    "        print(f'Working on {state} site {site_index + 1}/{len(df_state_sites)} ({row[\"site_no\"]})')\n",
    "        \n",
    "        if df.empty:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: No data for site {row[\"site_no\"]}')\n",
    "            continue\n",
    "        \n",
    "        if '00060_Mean' not in df.columns:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: No 00060_Mean data for site {row[\"site_no\"]}')\n",
    "            continue\n",
    "        \n",
    "        start = df['datetime'].min().date()\n",
    "        end = df['datetime'].max().date()\n",
    "        range = round((end - start).days / 365.25, 1)\n",
    "        \n",
    "        if range < fn.MIN_DATA_PERIOD:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: Not enough data for site {row[\"site_no\"]}')\n",
    "            continue\n",
    "        \n",
    "        point = Point(row['dec_long_va'], row['dec_lat_va'])\n",
    "        #state_code = row['station_nm'].strip()[-2:]\n",
    "        \n",
    "        huc2 = huc4 = aquifer = 'NA'\n",
    "        # HUC2 assignment\n",
    "        for i, geo_row in huc2_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                huc2 = geo_row['huc2_code']\n",
    "                continue\n",
    "            \n",
    "        # HUC4 assignment\n",
    "        for i, geo_row in huc4_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                huc4 = geo_row['huc4_code']\n",
    "                continue\n",
    "            \n",
    "        # Aquifer assignment    \n",
    "        for i, geo_row in aq_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                aquifer = geo_row['aq_name']\n",
    "                continue        \n",
    "        \n",
    "        # A few very broken sites with almost no data can have 0 hmf years and cause errors (i.e. '03592000')\n",
    "        # so we'll catch these and add them to the site blacklist\n",
    "        try:\n",
    "            df_single_site_metric, df_mk_mag, df_mk_dur, df_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "            add_data = {'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va'], 'data_start': start, 'data_end': end, 'total_record': range, \n",
    "                        'state': state, 'huc2_code': huc2, 'huc4_code': huc4, 'within_aq': aquifer}\t\t\t\n",
    "            add_data = pd.DataFrame(add_data, index=['0'])\n",
    "        except Exception as e:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f\"ERROR: Single site data failure for site {row['site_no']}:\\n{e}\")\n",
    "            continue\n",
    "        \n",
    "        add_data = pd.DataFrame(np.tile(add_data.values, (len(df_single_site_metric), 1)), columns=add_data.columns)\n",
    "        df_single_site_metric = pd.concat([df_single_site_metric.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_mag = pd.concat([df_mk_mag.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_dur = pd.concat([df_mk_dur.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_intra = pd.concat([df_mk_intra.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "        # Append single site data to multi-site dataframes\n",
    "        df_natl_metrics = pd.concat([df_natl_metrics, df_single_site_metric], ignore_index=True)\n",
    "        df_natl_mk_mag = pd.concat([df_natl_mk_mag, df_mk_mag], ignore_index=True)\n",
    "        df_natl_mk_dur = pd.concat([df_natl_mk_dur, df_mk_dur], ignore_index=True)\n",
    "        df_natl_mk_intra = pd.concat([df_natl_mk_intra, df_mk_intra], ignore_index=True)\n",
    "        \n",
    "df_natl_metrics = fn.gages_2_filtering(df_natl_metrics)\n",
    "\n",
    "# Create national site blacklist\n",
    "try:\n",
    "    with open(f'Prelim_Data/National_Sites_Blacklist.txt', 'w') as f:\n",
    "        natl_blacklist = [\"'\" + str(x) + \"'\" for x in natl_blacklist]        \n",
    "        f.write(', '.join(natl_blacklist))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:    \n",
    "    fn.save_data(df_natl_metrics, df_natl_mk_mag, df_natl_mk_dur, df_natl_mk_intra, dataset_name)\n",
    "except Exception as e:\n",
    "    print(f'Error saving data: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: Nationwide Validity Dataset\n",
    "Generates a dataset for every water gauge with 00060_Mean data in the contiguous US indicating gauge validity for this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = 30\n",
    "shapefile_path = 'ShapeFiles/Lower48/lower48.shp'\n",
    "test_limit = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on AL---]\n",
      "Total Sites: 173 in the state of AL\n",
      "Ignored 35 sites (20.23%)\n",
      "[---Working on AZ---]\n",
      "Total Sites: 218 in the state of AZ\n",
      "Ignored 11 sites (5.05%)\n",
      "[---Working on AR---]\n",
      "Total Sites: 139 in the state of AR\n",
      "Ignored 16 sites (11.51%)\n",
      "[---Working on CA---]\n",
      "Total Sites: 481 in the state of CA\n",
      "Ignored 54 sites (11.23%)\n",
      "[---Working on CO---]\n",
      "Total Sites: 349 in the state of CO\n",
      "Ignored 14 sites (4.01%)\n",
      "[---Working on CT---]\n",
      "Total Sites: 74 in the state of CT\n",
      "Ignored 4 sites (5.41%)\n",
      "[---Working on DE---]\n",
      "Total Sites: 36 in the state of DE\n",
      "Ignored 3 sites (8.33%)\n",
      "[---Working on FL---]\n",
      "Total Sites: 434 in the state of FL\n",
      "Ignored 39 sites (8.99%)\n",
      "[---Working on GA---]\n",
      "Total Sites: 308 in the state of GA\n",
      "Ignored 17 sites (5.52%)\n",
      "[---Working on ID---]\n",
      "Total Sites: 229 in the state of ID\n",
      "Ignored 9 sites (3.93%)\n",
      "[---Working on IL---]\n",
      "Total Sites: 193 in the state of IL\n",
      "Ignored 7 sites (3.63%)\n",
      "[---Working on IN---]\n",
      "Total Sites: 205 in the state of IN\n",
      "Ignored 20 sites (9.76%)\n",
      "[---Working on IA---]\n",
      "Total Sites: 143 in the state of IA\n",
      "Ignored 7 sites (4.9%)\n",
      "[---Working on KS---]\n",
      "Total Sites: 195 in the state of KS\n",
      "Ignored 5 sites (2.56%)\n",
      "[---Working on KY---]\n",
      "Total Sites: 169 in the state of KY\n",
      "Ignored 8 sites (4.73%)\n",
      "[---Working on LA---]\n",
      "Total Sites: 98 in the state of LA\n",
      "Ignored 27 sites (27.55%)\n",
      "[---Working on ME---]\n",
      "Total Sites: 68 in the state of ME\n",
      "Ignored 0 sites (0.0%)\n",
      "[---Working on MD---]\n",
      "Total Sites: 190 in the state of MD\n",
      "Ignored 8 sites (4.21%)\n",
      "[---Working on MA---]\n",
      "Total Sites: 146 in the state of MA\n",
      "Ignored 15 sites (10.27%)\n",
      "[---Working on MI---]\n",
      "Total Sites: 197 in the state of MI\n",
      "Ignored 16 sites (8.12%)\n",
      "[---Working on MN---]\n",
      "Total Sites: 130 in the state of MN\n",
      "Ignored 7 sites (5.38%)\n",
      "[---Working on MS---]\n",
      "Total Sites: 99 in the state of MS\n",
      "Ignored 16 sites (16.16%)\n",
      "[---Working on MO---]\n",
      "Total Sites: 244 in the state of MO\n",
      "Ignored 2 sites (0.82%)\n",
      "[---Working on MT---]\n",
      "Total Sites: 226 in the state of MT\n",
      "Ignored 7 sites (3.1%)\n",
      "[---Working on NE---]\n",
      "Total Sites: 120 in the state of NE\n",
      "Ignored 1 sites (0.83%)\n",
      "[---Working on NV---]\n",
      "Total Sites: 185 in the state of NV\n",
      "Ignored 9 sites (4.86%)\n",
      "[---Working on NH---]\n",
      "Total Sites: 101 in the state of NH\n",
      "Ignored 2 sites (1.98%)\n",
      "[---Working on NJ---]\n",
      "Total Sites: 141 in the state of NJ\n",
      "Ignored 10 sites (7.09%)\n",
      "[---Working on NM---]\n",
      "Total Sites: 174 in the state of NM\n",
      "Ignored 10 sites (5.75%)\n",
      "[---Working on NY---]\n",
      "Total Sites: 321 in the state of NY\n",
      "Ignored 16 sites (4.98%)\n",
      "[---Working on NC---]\n",
      "Total Sites: 240 in the state of NC\n",
      "Ignored 10 sites (4.17%)\n",
      "[---Working on ND---]\n",
      "Total Sites: 123 in the state of ND\n",
      "Ignored 9 sites (7.32%)\n",
      "[---Working on OH---]\n",
      "Total Sites: 222 in the state of OH\n",
      "Ignored 16 sites (7.21%)\n",
      "[---Working on OK---]\n",
      "Total Sites: 178 in the state of OK\n",
      "Ignored 9 sites (5.06%)\n",
      "[---Working on OR---]\n",
      "Total Sites: 213 in the state of OR\n",
      "Ignored 7 sites (3.29%)\n",
      "[---Working on PA---]\n",
      "Total Sites: 311 in the state of PA\n",
      "Ignored 6 sites (1.93%)\n",
      "[---Working on RI---]\n",
      "Total Sites: 34 in the state of RI\n",
      "Ignored 2 sites (5.88%)\n",
      "[---Working on SC---]\n",
      "Total Sites: 131 in the state of SC\n",
      "Ignored 12 sites (9.16%)\n",
      "[---Working on SD---]\n",
      "Total Sites: 122 in the state of SD\n",
      "Ignored 4 sites (3.28%)\n",
      "[---Working on TN---]\n",
      "Total Sites: 136 in the state of TN\n",
      "Ignored 15 sites (11.03%)\n",
      "[---Working on TX---]\n",
      "Total Sites: 597 in the state of TX\n",
      "Ignored 64 sites (10.72%)\n",
      "[---Working on UT---]\n",
      "Total Sites: 153 in the state of UT\n",
      "Ignored 4 sites (2.61%)\n",
      "[---Working on VT---]\n",
      "Total Sites: 98 in the state of VT\n",
      "Ignored 2 sites (2.04%)\n",
      "[---Working on VA---]\n",
      "Total Sites: 200 in the state of VA\n",
      "Ignored 8 sites (4.0%)\n",
      "[---Working on WA---]\n",
      "Total Sites: 265 in the state of WA\n",
      "Ignored 11 sites (4.15%)\n",
      "[---Working on WV---]\n",
      "Total Sites: 110 in the state of WV\n",
      "Ignored 2 sites (1.82%)\n",
      "[---Working on WI---]\n",
      "Total Sites: 220 in the state of WI\n",
      "Ignored 31 sites (14.09%)\n",
      "[---Working on WY---]\n",
      "Total Sites: 116 in the state of WY\n",
      "Ignored 1 sites (0.86%)\n"
     ]
    }
   ],
   "source": [
    "df_natl_validity = pd.DataFrame()\n",
    "\n",
    "for i, state in enumerate(fn.STATE_LIST):\n",
    "    if i >= test_limit: break\n",
    "    print(f'[---Working on {state}---]')\n",
    "    state_uri = fn.create_state_uri(state, fn.PARAM_CODE)\n",
    "    df_state_sites = filter_state_site(shapefile_path, state_uri)\n",
    "    print(f'Total Sites: {len(df_state_sites)} in the state of {state}')\n",
    "    \n",
    "    ignored_count = 0\n",
    "    for loc, row in df_state_sites.iterrows():\n",
    "        df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        if '00060_Mean' not in df.columns:\n",
    "            ignored_count += 1\n",
    "            continue\n",
    "        \n",
    "        start = df['datetime'].min().date()\n",
    "        end = df['datetime'].max().date()\n",
    "        range = round((end - start).days / 365.25, 1)\n",
    "        \n",
    "        valid_range = range > date_range\n",
    "        \n",
    "        date_threshold = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * date_range)\n",
    "        df = df[df['datetime'].dt.date >= date_threshold]\n",
    "        missing = fn.validate(df, date_threshold, fn.DEFAULT_END)\n",
    "        valid = missing < fn.MAX_MISSING_THRESHOLD\n",
    "        \n",
    "        data = {'site_no': row['site_no'], 'state': state, 'data_range': valid_range, 'data_cont': valid, 'start': start, 'end': end, \n",
    "                'total_record': range, 'missing': missing, 'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va']}\n",
    "        \n",
    "        df_natl_validity = pd.concat([df_natl_validity, pd.DataFrame(data, index=['0'])], axis=0, ignore_index=True)\n",
    "        \n",
    "    print(f'Ignored {ignored_count} sites ({round((ignored_count / len(df_state_sites)) * 100, 2)}%)')        \n",
    "        \n",
    "try:\n",
    "    with pd.ExcelWriter('Prelim_Data/National_Validity_30.xlsx') as writer:\n",
    "        df_natl_validity.to_excel(writer, sheet_name='national_validity', index=False) \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: HUC2/HUC4/Aquifer Master Shapefile Creation\n",
    "The following code combines HUC2, HUC4, and Aquifer shapefiles into master shapefiles, respectively. It's not necessary to run if these files already exist within the ShapeFiles directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc2_path = 'ShapeFiles/HUC2'\n",
    "huc4_path = 'ShapeFiles/HUC4'\n",
    "aquifer_path = 'ShapeFiles/Aquifers'\n",
    "aq_ext = '.shp'\n",
    "huc2 = 'WBDHU2.shp'\n",
    "huc4 = 'WBDHU4.shp'\n",
    "\n",
    "create_huc2 = False\n",
    "create_huc4 = False\n",
    "create_aquifer = True\n",
    "\n",
    "huc2_gdf = gpd.GeoDataFrame()\n",
    "huc4_gdf = gpd.GeoDataFrame()\n",
    "aquifer_df = gpd.GeoDataFrame()\n",
    "\n",
    "# HUC2's\n",
    "if create_huc2:\n",
    "    for root, dirs, files in os.walk(huc2_path):\n",
    "        if os.path.basename(root).startswith('WBD_'):\n",
    "            code = root[-2:]\n",
    "            if huc2 in files:\n",
    "                shape = gpd.read_file(os.path.join(root, huc2))\n",
    "                shape['huc2_code'] = code\n",
    "                huc2_gdf = pd.concat([huc2_gdf, shape], ignore_index=True)\n",
    "\n",
    "    huc2_gdf = huc2_gdf.to_crs(4269)\n",
    "    huc2_gdf.to_file('ShapeFiles/HUC2/_Master_HUC2/master_huc2.shp')\n",
    "\n",
    "# HUC4's    \n",
    "if create_huc4:\n",
    "    for root, dirs, files in os.walk(huc4_path):\n",
    "        if os.path.basename(root).startswith('NHD_H_'):\n",
    "            code = root[-4:]\n",
    "            if huc4 in files:\n",
    "                shape = gpd.read_file(os.path.join(root, huc4))\n",
    "                shape['huc4_code'] = code\n",
    "                huc4_gdf = pd.concat([huc4_gdf, shape], ignore_index=True)\n",
    "\n",
    "    huc4_gdf = huc4_gdf.to_crs(4269)\n",
    "    huc4_gdf.to_file('ShapeFiles/HUC4/_Master_HUC4/master_huc4.shp')\n",
    "    \n",
    "# Aquifers\n",
    "if create_aquifer:\n",
    "    for root, dirs, files in os.walk(aquifer_path):\n",
    "        for file in files:\n",
    "            if file.endswith(aq_ext) and dirs:\n",
    "                if file.startswith('Penn'):\n",
    "                    shape = gpd.read_file(os.path.join(root, file))\n",
    "                    shape = shape.iloc[[17]]                    \n",
    "                else:\n",
    "                    shape = gpd.read_file(os.path.join(root, file))\n",
    "                    \n",
    "                shape = shape.to_crs(4269)\n",
    "                shape['aq_name'] = file[:-4]\n",
    "                aquifer_df = pd.concat([aquifer_df, shape], ignore_index=True)\n",
    "                \n",
    "    aquifer_df = aquifer_df.to_crs(4269)\n",
    "    aquifer_df.to_file('ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: HUC2/4/Aquifer Sorting\n",
    "Takes a national validity or national metrics dataset (any dataset with dec_lat_va and dec_long_va data), and adds corresponding HUC2, HUC4, and Aquifer columns indicating a water guages presence or lack thereof in each of these boundaries.\n",
    "\n",
    "<strong>NOTE:</strong> This feature has already been implemented directly into the National Metrics dataset creation. Its only real use is on National Validity datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Prelim_Data/_National_Metrics/'\n",
    "datasets = ['National_Metrics_30_90.xlsx']\n",
    "huc2_gdf = gpd.read_file('ShapeFiles/HUC2/_Master_HUC2/master_huc2.shp')\n",
    "huc4_gdf = gpd.read_file('ShapeFiles/HUC4/_Master_HUC4/master_huc4.shp')\n",
    "aq_gdf = gpd.read_file('ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data = pd.DataFrame({'site_no': 'NA', 'huc2_code': 'NA', 'huc4_code': 'NA', 'aquifer': 'NA'}, index=['0'])\n",
    "temp = pd.DataFrame()\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.read_excel(f'{path}{dataset}', dtype={'site_no': str})\n",
    "    sheets = pd.ExcelFile(f'{path}{dataset}')\n",
    "    sheet = sheets.sheet_names[0]          \n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        point = Point(row['dec_long_va'], row['dec_lat_va'])\n",
    "        add_data['site_no'] = row['site_no']\n",
    "        \n",
    "        # HUC2 assignment\n",
    "        for j, geo_row in huc2_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                add_data[\"huc2_code\"] = geo_row['huc2_code']\n",
    "                continue\n",
    "            \n",
    "        # HUC4 assignment\n",
    "        for j, geo_row in huc4_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                add_data[\"huc4_code\"] = geo_row['huc4_code']\n",
    "                continue\n",
    "            \n",
    "        for j, geo_row in aq_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                add_data[\"aquifer\"] = geo_row['aq_name']\n",
    "                continue\n",
    "            \n",
    "        temp = pd.concat([temp, add_data], axis=0, ignore_index=True)\n",
    "    \n",
    "    df = pd.merge(df, temp, on='site_no')\n",
    "    \n",
    "    with pd.ExcelWriter(f'{path}{dataset}', mode='a', if_sheet_exists='replace') as writer:\n",
    "        df.to_excel(writer, sheet_name=sheet, index=False)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: National Dataset Splitting by Aquifer\n",
    "This script splits a national metric dataset or datasets into smaller per-aquifer datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "natl_path = 'Prelim_Data/_National_Metrics'\n",
    "\n",
    "# National datasets to split\n",
    "datasets = ['National_Metrics_30_90.xlsx', 'National_Metrics_30_95.xlsx', 'National_Metrics_50_90.xlsx', 'National_Metrics_50_95.xlsx']\n",
    "# The datasets to generate from the national dataset\n",
    "target_aquifers = cl.ALL_AQUIFERS\n",
    "sheet_names = ['site_metrics', 'mk_magnitude', 'mk_duration', 'mk_intra_annual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over national datasets\n",
    "for dataset in datasets:\n",
    "    df_list = []\n",
    "    for sheet in sheet_names:\n",
    "        df = pd.read_excel(f'{natl_path}/{dataset}', sheet_name=sheet, dtype=fn.DATASET_DTYPES)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Iterate over target aquifers\n",
    "    for aquifer in target_aquifers:\n",
    "        save_path = f\"{aquifer.datasets_dir}/{aquifer.name}_{dataset[-10:-8]}_{dataset[-7:-5]}.xlsx\"\n",
    "        for i, df in enumerate(df_list):\n",
    "            df = df[df['huc4_code'].isin(aquifer.huc4s)]\n",
    "            \n",
    "            # Append new sheets to existing file\n",
    "            if os.path.exists(save_path):\n",
    "                with pd.ExcelWriter(save_path, mode='a', if_sheet_exists='replace') as writer:\n",
    "                    df.to_excel(writer, sheet_name=sheet_names[i], index=False)\n",
    "            # Create file if it doesn't exist (first iteration)                        \n",
    "            else: \n",
    "                with pd.ExcelWriter(save_path, mode='w') as writer:\n",
    "                    df.to_excel(writer, sheet_name=sheet_names[i], index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
