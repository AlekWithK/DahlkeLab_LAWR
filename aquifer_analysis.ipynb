{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES:\n",
    "<ul>\n",
    "<li>Sections labeled <strong>CORE</strong> must be run in order to begin Aquifer Analysis</li>\n",
    "<li>Cells labeled <strong>Control</strong> contain inputs for the immeadiately proceeding section(s)</li>\n",
    "<li>Sections labeled <strong>EXTRA</strong> contain additional plotting or analysis tools but are not necessary for Aquifer Analysis</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nadp.py:44: UserWarning: GDAL not installed. Some functions will not work.\n",
      "  warnings.warn('GDAL not installed. Some functions will not work.')\n"
     ]
    }
   ],
   "source": [
    "#Python3.10\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import contextily as cx\n",
    "import requests\n",
    "import calendar\n",
    "from importlib import reload\n",
    "from typing import IO\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import Point\n",
    "from io import StringIO\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# USGS Data retreival tool\n",
    "from dataretrieval import nwis, utils, codes\n",
    "\n",
    "# Custom modules are imported in multiple locations to faciliate easy reloading when edits are made to their respective files\n",
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "reload(fn)\n",
    "\n",
    "# TODO: Look into the warning that this is disabling. It doesn't appear to be significant for the purposes of this code but should be understood\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#pd.options.mode.chained_assignment = 'warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Single Site Data<br>\n",
    "This function produces the streamflow analysis (using the functions above) for a single site given a list of thresholds and time windows to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "\n",
    "# df2 holds all-time data, df is analyzed range\n",
    "curr_guage = cl.SRB_Guage\n",
    "df = nwis.get_record(sites='09306200', service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2020-09-30')\n",
    "df2 = nwis.get_record(sites=curr_guage.id, service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2014-09-30')\n",
    "\n",
    "# Set to true if running indepedently to verify data with Kocis paper (or other source)\n",
    "# Set to false if running Aquifer Analysis\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Site Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site No: 0    09306200\n",
      "1    09306200\n",
      "2    09306200\n",
      "0    09306200\n",
      "Name: site_no, dtype: object\n",
      "Analyzed Range: 30\n",
      "30\n",
      "50\n",
      "50\n",
      "Quantile: 0.90\n",
      "0.95\n",
      "0.90\n",
      "0.95\n",
      "Valid: True\n",
      "True\n",
      "True\n",
      "True\n",
      "% Missing: 3\n",
      "3\n",
      "2\n",
      "2\n",
      "90%: 34.520\n",
      "53.855\n",
      "53.000\n",
      "76.000\n",
      "HMF Years: 21\n",
      "14\n",
      "30\n",
      "20\n",
      "Annual Duration: 50.714286\n",
      "38.071429\n",
      "59.166667\n",
      "44.450000\n",
      "Event Duration: 15.213152\n",
      "19.197619\n",
      "18.188333\n",
      "17.307063\n",
      "Event HMF: 0.001363\n",
      "0.003407\n",
      "0.002810\n",
      "0.002751\n",
      "Inter-annual Frequency: 70\n",
      "47\n",
      "60\n",
      "40%\n",
      "Intra-annual Frequency: 2.761905\n",
      "2.428571\n",
      "3.133333\n",
      "3.150000\n",
      "Total HMF in km^3/year: 0.005196\n",
      "0.005295\n",
      "0.007876\n",
      "0.008238\n",
      "Center of Mass: 236.571429\n",
      "228.142857\n",
      "214.800000\n",
      "225.800000\n",
      "6 Month HMF in km^3/year: 0.001471\n",
      "0.001448\n",
      "0.002101\n",
      "0.002069\n",
      "3 Month HMF in km^3/year: 0.000025\n",
      "0.000003\n",
      "0.000074\n",
      "0.000006\n"
     ]
    }
   ],
   "source": [
    "# If looking at a post impairment period, but calculating threshold based on the full record of data, pass a second dataframe with a different \n",
    "# start/end date as the final parameter. This method was used in Kocis 2017 and is needed for some data verification, but is not the methodology\n",
    "# used for the Aquifer Analysis and so *args will most often be empty.\n",
    "import Src.func as fn\n",
    "reload(fn)\n",
    "\n",
    "def single_site_data(df: pd.DataFrame, quantiles_list: list, data_ranges_list: list, *args):\n",
    "    df = df.reset_index()    \n",
    "    threshold = None\n",
    "    df_complete_site_data = pd.DataFrame()    \n",
    "    df_complete_mk_mag = pd.DataFrame()\n",
    "    df_complete_mk_dur = pd.DataFrame()\n",
    "    df_complete_mk_intra = pd.DataFrame()\n",
    "    \n",
    "    for data_range in data_ranges_list:\n",
    "        for quantile in quantiles_list:\n",
    "            # Copying original dataframe to avoid any conflicts\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Validate that site is not missing > 10% of data\n",
    "            date_threshold = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * data_range)              \n",
    "            \n",
    "            # Filter dataframe based on current data_range (don't do this during testing if testing unique dataset/range)\n",
    "            if not testing:\n",
    "                df_copy = df_copy[df_copy['datetime'].dt.date >= date_threshold]\n",
    "            \n",
    "            # Validate <10% missing data based on filtered dataframe\n",
    "            missing = fn.validate(df_copy, date_threshold, fn.DEFAULT_END)\n",
    "            valid = missing < fn.MAX_MISSING_THRESHOLD\n",
    "            missing = int(round(missing, 2) * 100)                               \n",
    "    \n",
    "            # Check for optional second dataframe containing full record\n",
    "            if args and isinstance(args[0], pd.DataFrame) and not args[0].empty:\n",
    "                #print('Threshold calculation across the full record')\n",
    "                df2 = args[0].reset_index()\n",
    "                threshold = fn.calc_threshold(df2, quantile)\n",
    "                # threshold = 52350 # SRB_Guage post-impairment threshold for verification with Kocis_2017 \n",
    "            else:\n",
    "                #print('Threshold calculation across a limited record')\n",
    "                threshold = fn.calc_threshold(df_copy, quantile)                             \n",
    "\n",
    "            # Create a dataframe with only days over HMF threshold as well as continuous dataframe for MK trend test\n",
    "            hmf_series_defl, hmf_series_cont = fn.filter_hmf(df_copy, threshold)\n",
    "            #print(zero_deflated_hmf)\n",
    "            \n",
    "            # Aggregate data before performing MK magnitude test\n",
    "            df_agg_cont = hmf_series_cont.copy()\n",
    "            df_agg_cont = fn.convert_hmf(df_agg_cont, threshold)\n",
    "            df_agg_cont['00060_Mean'] = df_agg_cont['00060_Mean'].apply(lambda x: x * fn.CUBIC_FT_KM_FACTOR if x >= 0 else 0)\n",
    "            df_agg_cont.set_index('datetime', inplace=True)\n",
    "            df_agg_cont = df_agg_cont.resample(fn.HYDRO_YEAR).agg({'00060_Mean': ['sum', 'count']})\n",
    "            df_agg_cont.columns = ['Sum', 'Count']\n",
    "            df_agg_cont_defl = df_agg_cont[df_agg_cont['Sum'] > 0]          \n",
    "            \n",
    "            # MK Magnitude\n",
    "            df_mk_mag = fn.mann_kendall(df_agg_cont_defl['Sum'], df_agg_cont['Sum'], fn.MK_TREND_ALPHA)\n",
    "\n",
    "            # Find number of years with HMF\n",
    "            hmf_years = fn.num_hmf_years(hmf_series_defl)            \n",
    "\n",
    "            # Mask out months that don't fall within 3 and 6 month Winter range\n",
    "            df_six_month, df_three_month = fn.three_six_range(hmf_series_defl, 12, 2, 11, 4)\n",
    "\n",
    "            # Convert to daily average flow in cfpd, and take only flow above the threshold\n",
    "            hmf_series_defl = fn.convert_hmf(hmf_series_defl, threshold)\n",
    "            total_hmf_flow = hmf_series_defl[\"00060_Mean\"].sum()\n",
    "            hmf_per_month = fn.monthly_hmf(hmf_series_defl, data_range, quantile)\n",
    "\n",
    "            # Calculate 3 and 6 month HMF\n",
    "            df_six_month = fn.convert_hmf(df_six_month, threshold)\n",
    "            six_month_hmf = df_six_month[\"00060_Mean\"].sum()\n",
    "            df_three_month = fn.convert_hmf(df_three_month, threshold)\n",
    "            three_month_hmf = df_three_month[\"00060_Mean\"].sum()\n",
    "            \n",
    "            total_hmf_flow = (total_hmf_flow * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            six_month_hmf = (six_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            three_month_hmf = (three_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "\n",
    "            # Inter-annual\n",
    "            inter_annual, delta = fn.calc_inter_annual(hmf_series_cont, hmf_years)            \n",
    "\n",
    "            # Average Duration and Intra-annual Frequency\n",
    "            hmf_series_cont = fn.convert_hmf(hmf_series_cont, threshold)\n",
    "            event_duration, annual_duration, intra_annual, event_hmf, df_results = fn.calc_duration_intra_annual(hmf_series_cont, hmf_years)\n",
    "            dur_series_defl = df_results['duration'][df_results['duration'] > 0]\n",
    "            dur_series_cont = df_results['duration']\n",
    "            df_mk_dur = fn.mann_kendall(dur_series_defl, dur_series_cont, fn.MK_TREND_ALPHA)\n",
    "                        \n",
    "            intra_series_defl = df_results['total_events'][df_results['total_events'] > 0]\n",
    "            intra_series_cont = df_results['total_events']\n",
    "            df_mk_intra = fn.mann_kendall(intra_series_defl, intra_series_cont, fn.MK_TREND_ALPHA) \n",
    "            \n",
    "            # Timing Calculation using DOHY\n",
    "            timing = fn.calc_timing(hmf_series_defl)           \n",
    "\n",
    "            # TODO: One-day peaks (avg. # of times hmf occurs on one day only)          \n",
    "            \n",
    "            # Merging site dataframe with Mann-Kendall dataframe. This start date is the beginning of the actual data, not necessarily \n",
    "            # the beginning of the analyzed range. Validation (above) starts from the start of the official range (1970/90-2020)\n",
    "            start = df_copy['datetime'].min().date()\n",
    "            end = df_copy['datetime'].max().date()\n",
    "            data = {'dataset_ID': (data_range * quantile), 'site_no': df_copy.iloc[0]['site_no'], 'analyze_start': start, 'analyze_end': end, 'analyze_range': delta, 'quantile': quantile, \n",
    "                    'valid': valid, 'missing_data%': missing, 'threshold': threshold, 'hmf_years': hmf_years, 'annual_hmf': total_hmf_flow, 'six_mo_hmf': six_month_hmf, 'three_mo_hmf': three_month_hmf, \n",
    "                    'annual_duration': annual_duration, 'event_duration': event_duration, 'event_hmf': event_hmf, 'inter_annual%': inter_annual, 'intra_annual': intra_annual, 'timing': timing}              \n",
    "            \n",
    "            # Merging MK magnitiude\n",
    "            df_mk_mag.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_mag.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_mag = pd.concat([df_complete_mk_mag.reset_index(drop=True), df_mk_mag.reset_index(drop=True)], axis=0) \n",
    "            \n",
    "            # Merging MK duration\n",
    "            df_mk_dur.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_dur.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_dur = pd.concat([df_complete_mk_dur.reset_index(drop=True), df_mk_dur.reset_index(drop=True)], axis=0)   \n",
    "            \n",
    "            # Merging MK intra-annual\n",
    "            df_mk_intra.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_intra.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_intra = pd.concat([df_complete_mk_intra.reset_index(drop=True), df_mk_intra.reset_index(drop=True)], axis=0)        \n",
    "            \n",
    "            # Merging metric results\n",
    "            df_single_iteration = pd.DataFrame(data, index=['0'])\n",
    "            df_single_iteration = pd.concat([df_single_iteration.reset_index(drop=True), hmf_per_month.reset_index(drop=True)], axis=1)\n",
    "            df_complete_site_data = pd.concat([df_complete_site_data.reset_index(drop=True), df_single_iteration.reset_index(drop=True)], axis=0)\n",
    "        \n",
    "    return df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra\n",
    "\n",
    "# For testing purposes, to run this cell independently\n",
    "df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "\n",
    "'''\n",
    "try:\n",
    "    with pd.ExcelWriter('df_single_site.xlsx') as writer:\n",
    "        df_complete_site_data.to_excel(writer, sheet_name='site_metrics', index=False)\n",
    "        df_complete_mk_mag.to_excel(writer, sheet_name='mk_magnitude', index=False)\n",
    "        df_complete_mk_intra.to_excel(writer, sheet_name='mk_intra', index=False)\n",
    "        df_complete_mk_dur.to_excel(writer, sheet_name='mk_duration', index=False)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "'''\n",
    "   \n",
    "fn.single_site_report(df_complete_site_data)\n",
    "df_complete_site_data = fn.gages_2_filtering(df_complete_site_data)\n",
    "#fn.save_data(df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra, 'TEST')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Multi-Site Filtering<br>\n",
    "This function creates the list of sites to analyze by filtering the complete list of state sites with 00060_Mean data down to those that lie within a specific watershed boundary using decimal long/lat positional data and a region shapefile (e.g. state or watershed boundary)<br><br>\n",
    "\n",
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed when running the following cell independently\n",
    "shapefile_path = 'ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site List Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sites: 86 in the state of CA in the given WB\n",
      "['09429000', '09429100', '09429155', '09429180', '09429200', '09429500', '09429600', '09521100', '09523000', '09523200', '09523400', '09523600', '09523800', '09524000', '09524700', '09525000', '09526200', '09530000', '09530500', '11251000', '11253310', '11255575', '11261100', '11261500', '11262900', '11273400', '11274000', '11274500', '11274550', '11274630', '11289850', '11290000', '11303000', '11303500', '11304810', '11311300', '11312672', '11312676', '11312685', '11312968', '11313240', '11313315', '11313405', '11313431', '11313433', '11313434', '11313440', '11313460', '11336600', '11336685', '11336790', '11336930', '11337080', '11370500', '11370700', '11374000', '11376000', '11376550', '11379500', '11389500', '11390500', '11421000', '11424000', '11425500', '11446500', '11447360', '11447650', '11447830', '11447850', '11447890', '11447903', '11447905', '11451800', '11452500', '11452800', '11452900', '11453000', '11455095', '11455140', '11455280', '11455315', '11455338', '11455385', '351813119150601', '360013118575201', '375450121331701']\n"
     ]
    }
   ],
   "source": [
    "def filter_state_site(shapefile_path: str, state_uri: str):\n",
    "    \"\"\"Creates a list of sites with over 50 years of 00060_Mean streamflow data for a given region\"\"\"\n",
    "    # Request page from USGS site, ignore all informational lines\n",
    "    response = requests.get(state_uri)\n",
    "    data = response.text\n",
    "    lines = data.splitlines()\n",
    "    lines = [line for line in lines if not line.startswith('#')]\n",
    "\n",
    "    # Create dataframe where site_no is a list of all sites in a state with 00060 data\n",
    "    tsd = \"\\n\".join(lines)\n",
    "    df = pd.read_csv(StringIO(tsd), sep='\\t')\n",
    "    df_state_sites = df.iloc[1:]\n",
    "        \n",
    "    # Filter out sites outside of HU boundary\n",
    "    if fn.SORT_BY_WB:\n",
    "        shapefile = gpd.read_file(shapefile_path)\n",
    "        df_state_sites['geometry'] = [Point(lon, lat) for lon, lat in zip(df_state_sites['dec_long_va'], df_state_sites['dec_lat_va'])]\n",
    "        gdf_data = gpd.GeoDataFrame(df_state_sites, crs=shapefile.crs)\n",
    "        df_state_sites = gpd.sjoin(gdf_data, shapefile, predicate='within')\n",
    "            \n",
    "    #print(df_state_sites.columns.to_list())\n",
    "    #print(df_state_sites)\n",
    "    \n",
    "    return df_state_sites\n",
    "\n",
    "df_state_sites = filter_state_site(shapefile_path, fn.SITES_URI)\n",
    "print(f'Total Sites: {len(df_state_sites)} in the state of {fn.STATE_CODE.upper()} in the given WB')\n",
    "site_list = df_state_sites['site_no'].to_list()\n",
    "print(site_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: National Metrics Dataset\n",
    "Generates a nationwide dataset for all sites meeting datarange criteria (lengthy runtime!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc2_gdf = gpd.read_file('ShapeFiles/HUC2/_Master_HUC2/master_huc2.shp')\n",
    "huc4_gdf = gpd.read_file('ShapeFiles/HUC4/_Master_HUC4/master_huc4.shp')\n",
    "aq_gdf = gpd.read_file('ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_limit = 50\n",
    "dataset_name = 'National_Metrics'\n",
    "shapefile_path = 'ShapeFiles/Lower48/lower48.shp'\n",
    "\n",
    "# States with few sites for testing purposes\n",
    "test_state_list = ['ME', 'DE']\n",
    "\n",
    "# Set to False to ignore the blacklist and analyze all sites. This could be occasionally\n",
    "# worth doing as USGS could update site data making it valid in the future\n",
    "allow_blacklist = True\n",
    "\n",
    "curr_blacklist = []\n",
    "try:\n",
    "    with open('Prelim_Data/_National_Metrics/National_Sites_Blacklist.txt', 'r') as file:\n",
    "        curr_blacklist = file.read().split(', ')\n",
    "        curr_blacklist = [x.replace(\"'\", \"\") for x in curr_blacklist]\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on CA---]\n",
      "Total Sites: 483 in the state of CA\n",
      "Working on CA site 1/483 (09423350)\n",
      "Working on CA site 2/483 (09429000)\n",
      "Working on CA site 3/483 (09429100)\n",
      "Working on CA site 4/483 (09429155)\n",
      "Working on CA site 5/483 (09429180)\n",
      "Working on CA site 6/483 (09429200)\n",
      "IGNORED: Site 09429500 is in aquifer blacklist\n",
      "IGNORED: Site 09429600 is in aquifer blacklist\n",
      "Working on CA site 9/483 (09521100)\n",
      "Working on CA site 10/483 (09523000)\n",
      "IGNORED: Site 09523200 is in aquifer blacklist\n",
      "Working on CA site 12/483 (09523400)\n",
      "Working on CA site 13/483 (09523600)\n",
      "Working on CA site 14/483 (09523800)\n",
      "Working on CA site 15/483 (09524000)\n",
      "IGNORED: Site 09524700 is in aquifer blacklist\n",
      "Working on CA site 17/483 (09525000)\n",
      "IGNORED: Site 09526200 is in aquifer blacklist\n",
      "IGNORED: Site 09527590 is in aquifer blacklist\n",
      "IGNORED: Site 09527594 is in aquifer blacklist\n",
      "IGNORED: Site 09527597 is in aquifer blacklist\n",
      "IGNORED: Site 09527630 is in aquifer blacklist\n",
      "IGNORED: Site 09527660 is in aquifer blacklist\n",
      "IGNORED: Site 09527700 is in aquifer blacklist\n",
      "Working on CA site 25/483 (09530000)\n",
      "IGNORED: Site 09530500 is in aquifer blacklist\n",
      "IGNORED: Site 10251290 is in aquifer blacklist\n",
      "Working on CA site 28/483 (10251300)\n",
      "IGNORED: Site 10251330 is in aquifer blacklist\n",
      "IGNORED: Site 10251335 is in aquifer blacklist\n",
      "Working on CA site 31/483 (10254050)\n",
      "Working on CA site 32/483 (10254730)\n",
      "IGNORED: Site 10254970 is in aquifer blacklist\n",
      "Working on CA site 34/483 (10255550)\n",
      "Working on CA site 35/483 (10256500)\n",
      "Working on CA site 36/483 (10257500)\n",
      "IGNORED: Site 10257548 is in aquifer blacklist\n",
      "IGNORED: Site 10257549 is in aquifer blacklist\n",
      "Working on CA site 39/483 (10257600)\n",
      "IGNORED: Site 10257720 is in aquifer blacklist\n",
      "Working on CA site 41/483 (10258000)\n",
      "Working on CA site 42/483 (10258500)\n",
      "IGNORED: Site 10258700 is in aquifer blacklist\n",
      "Working on CA site 44/483 (10259000)\n",
      "IGNORED: Site 10259050 is in aquifer blacklist\n",
      "IGNORED: Site 10259100 is in aquifer blacklist\n",
      "Working on CA site 47/483 (10259200)\n",
      "Working on CA site 48/483 (10259300)\n",
      "Working on CA site 49/483 (10259540)\n",
      "Working on CA site 50/483 (10260500)\n",
      "IGNORED: Site 10260855 is in aquifer blacklist\n",
      "IGNORED: Site 10260950 is in aquifer blacklist\n",
      "Working on CA site 53/483 (10261500)\n",
      "IGNORED: Site 10262000 is in aquifer blacklist\n",
      "Working on CA site 55/483 (10262500)\n",
      "IGNORED: Site 10262700 is in aquifer blacklist\n",
      "Working on CA site 57/483 (10263000)\n",
      "Working on CA site 58/483 (10263500)\n",
      "IGNORED: Site 10265100 is in aquifer blacklist\n",
      "Working on CA site 60/483 (10289000)\n",
      "Working on CA site 61/483 (10289500)\n",
      "Working on CA site 62/483 (10290500)\n",
      "Working on CA site 63/483 (10291500)\n",
      "Working on CA site 64/483 (10293000)\n",
      "Working on CA site 65/483 (10296000)\n",
      "Working on CA site 66/483 (10296500)\n",
      "IGNORED: Site 10296750 is in aquifer blacklist\n",
      "Working on CA site 68/483 (10308200)\n",
      "IGNORED: Site 10308783 is in aquifer blacklist\n",
      "IGNORED: Site 103087835 is in aquifer blacklist\n",
      "IGNORED: Site 10308784 is in aquifer blacklist\n",
      "IGNORED: Site 10308785 is in aquifer blacklist\n",
      "IGNORED: Site 103087855 is in aquifer blacklist\n",
      "IGNORED: Site 103087865 is in aquifer blacklist\n",
      "IGNORED: Site 103087889 is in aquifer blacklist\n",
      "IGNORED: Site 10308789 is in aquifer blacklist\n",
      "IGNORED: Site 10308792 is in aquifer blacklist\n",
      "IGNORED: Site 10308794 is in aquifer blacklist\n",
      "Working on CA site 79/483 (10310000)\n",
      "IGNORED: Site 103366092 is in aquifer blacklist\n",
      "IGNORED: Site 10336610 is in aquifer blacklist\n",
      "IGNORED: Site 10336645 is in aquifer blacklist\n",
      "Working on CA site 83/483 (10336660)\n",
      "IGNORED: Site 10336676 is in aquifer blacklist\n",
      "Working on CA site 85/483 (10336780)\n",
      "Working on CA site 86/483 (10337500)\n",
      "Working on CA site 87/483 (10338000)\n",
      "Working on CA site 88/483 (10338500)\n",
      "IGNORED: Site 10338700 is in aquifer blacklist\n",
      "IGNORED: Site 10339410 is in aquifer blacklist\n",
      "Working on CA site 91/483 (10340500)\n",
      "Working on CA site 92/483 (10343000)\n",
      "Working on CA site 93/483 (10343500)\n",
      "Working on CA site 94/483 (10344400)\n",
      "Working on CA site 95/483 (10344500)\n",
      "IGNORED: Site 10344505 is in aquifer blacklist\n",
      "Working on CA site 97/483 (10346000)\n",
      "Working on CA site 98/483 (11012500)\n",
      "Working on CA site 99/483 (11014000)\n",
      "Working on CA site 100/483 (11015000)\n",
      "IGNORED: Site 11016200 is in aquifer blacklist\n",
      "IGNORED: Site 11022200 is in aquifer blacklist\n",
      "Working on CA site 103/483 (11022480)\n",
      "IGNORED: Site 11023000 is in aquifer blacklist\n",
      "Working on CA site 105/483 (11023340)\n",
      "Working on CA site 106/483 (11025500)\n",
      "Working on CA site 107/483 (11027000)\n",
      "Working on CA site 108/483 (11028500)\n",
      "Working on CA site 109/483 (11042000)\n",
      "Working on CA site 110/483 (11042400)\n",
      "IGNORED: Site 11042800 is in aquifer blacklist\n",
      "IGNORED: Site 11042900 is in aquifer blacklist\n",
      "Working on CA site 113/483 (11044000)\n",
      "IGNORED: Site 11044250 is in aquifer blacklist\n",
      "IGNORED: Site 11044300 is in aquifer blacklist\n",
      "IGNORED: Site 11044350 is in aquifer blacklist\n",
      "IGNORED: Site 11044800 is in aquifer blacklist\n",
      "IGNORED: Site 11045300 is in aquifer blacklist\n",
      "IGNORED: Site 11045600 is in aquifer blacklist\n",
      "IGNORED: Site 11045700 is in aquifer blacklist\n",
      "Working on CA site 121/483 (11046000)\n",
      "Working on CA site 122/483 (11046300)\n",
      "IGNORED: Site 11046360 is in aquifer blacklist\n",
      "IGNORED: Site 11046530 is in aquifer blacklist\n",
      "IGNORED: Site 11047300 is in aquifer blacklist\n",
      "IGNORED: Site 11048175 is in aquifer blacklist\n",
      "IGNORED: Site 11048600 is in aquifer blacklist\n",
      "IGNORED: Site 11051499 is in aquifer blacklist\n",
      "IGNORED: Site 11051502 is in aquifer blacklist\n",
      "Working on CA site 130/483 (11055500)\n",
      "Working on CA site 131/483 (11055800)\n",
      "Working on CA site 132/483 (11057500)\n",
      "Working on CA site 133/483 (11058500)\n",
      "Working on CA site 134/483 (11058600)\n",
      "Working on CA site 135/483 (11059300)\n",
      "Working on CA site 136/483 (11060400)\n",
      "Working on CA site 137/483 (11062000)\n",
      "IGNORED: Site 11062399 is in aquifer blacklist\n",
      "IGNORED: Site 11062400 is in aquifer blacklist\n",
      "IGNORED: Site 11062450 is in aquifer blacklist\n",
      "IGNORED: Site 11062800 is in aquifer blacklist\n",
      "IGNORED: Site 11063510 is in aquifer blacklist\n",
      "Working on CA site 143/483 (11063680)\n",
      "Working on CA site 144/483 (11065000)\n",
      "Working on CA site 145/483 (11069500)\n",
      "IGNORED: Site 11070210 is in aquifer blacklist\n",
      "Working on CA site 147/483 (11070270)\n",
      "IGNORED: Site 11070365 is in aquifer blacklist\n",
      "IGNORED: Site 11070465 is in aquifer blacklist\n",
      "Working on CA site 150/483 (11070500)\n",
      "IGNORED: Site 11071760 is in aquifer blacklist\n",
      "IGNORED: Site 11071900 is in aquifer blacklist\n",
      "IGNORED: Site 11072100 is in aquifer blacklist\n",
      "IGNORED: Site 11073300 is in aquifer blacklist\n",
      "Working on CA site 155/483 (11073360)\n",
      "Working on CA site 156/483 (11073495)\n",
      "Working on CA site 157/483 (11074000)\n",
      "Working on CA site 158/483 (11075720)\n",
      "Working on CA site 159/483 (11077500)\n",
      "Working on CA site 160/483 (11078000)\n",
      "Working on CA site 161/483 (11085000)\n",
      "Working on CA site 162/483 (11087020)\n",
      "Working on CA site 163/483 (11088500)\n",
      "Working on CA site 164/483 (11089500)\n",
      "Working on CA site 165/483 (11092450)\n",
      "Working on CA site 166/483 (11097000)\n",
      "Working on CA site 167/483 (11098000)\n",
      "Working on CA site 168/483 (11101250)\n",
      "Working on CA site 169/483 (11102300)\n",
      "Working on CA site 170/483 (11109000)\n",
      "Working on CA site 171/483 (11109600)\n",
      "Working on CA site 172/483 (11109800)\n",
      "Working on CA site 173/483 (11111500)\n",
      "Working on CA site 174/483 (11113000)\n",
      "Working on CA site 175/483 (11113500)\n",
      "IGNORED: Site 11114495 is in aquifer blacklist\n",
      "Working on CA site 177/483 (11118500)\n",
      "Working on CA site 178/483 (11119500)\n",
      "Working on CA site 179/483 (11119750)\n",
      "Working on CA site 180/483 (11119940)\n",
      "Working on CA site 181/483 (11120000)\n",
      "Working on CA site 182/483 (11120500)\n",
      "Working on CA site 183/483 (11120520)\n",
      "Working on CA site 184/483 (11121900)\n",
      "IGNORED: Site 11122010 is in aquifer blacklist\n",
      "Working on CA site 186/483 (11123000)\n",
      "Working on CA site 187/483 (11123500)\n",
      "Working on CA site 188/483 (11124500)\n",
      "IGNORED: Site 11125605 is in aquifer blacklist\n",
      "IGNORED: Site 11126400 is in aquifer blacklist\n",
      "Working on CA site 191/483 (11128250)\n",
      "Working on CA site 192/483 (11128500)\n",
      "Working on CA site 193/483 (11129800)\n",
      "Working on CA site 194/483 (11132500)\n",
      "Working on CA site 195/483 (11133000)\n",
      "Working on CA site 196/483 (11134000)\n",
      "Working on CA site 197/483 (11135800)\n",
      "IGNORED: Site 11136040 is in aquifer blacklist\n",
      "IGNORED: Site 11136045 is in aquifer blacklist\n",
      "Working on CA site 200/483 (11136100)\n",
      "Working on CA site 201/483 (11136500)\n",
      "IGNORED: Site 11136600 is in aquifer blacklist\n",
      "IGNORED: Site 11136710 is in aquifer blacklist\n",
      "Working on CA site 204/483 (11136800)\n",
      "Working on CA site 205/483 (11137900)\n",
      "Working on CA site 206/483 (11138500)\n",
      "Working on CA site 207/483 (11140000)\n",
      "IGNORED: Site 11140585 is in aquifer blacklist\n",
      "IGNORED: Site 11141050 is in aquifer blacklist\n",
      "Working on CA site 210/483 (11141280)\n",
      "Working on CA site 211/483 (11143000)\n",
      "Working on CA site 212/483 (11143200)\n",
      "Working on CA site 213/483 (11143250)\n",
      "IGNORED: Site 11147070 is in aquifer blacklist\n",
      "IGNORED: Site 11147098 is in aquifer blacklist\n",
      "Working on CA site 216/483 (11147500)\n",
      "IGNORED: Site 11148900 is in aquifer blacklist\n",
      "Working on CA site 218/483 (11149400)\n",
      "Working on CA site 219/483 (11149900)\n",
      "Working on CA site 220/483 (11150500)\n",
      "Working on CA site 221/483 (11151300)\n",
      "Working on CA site 222/483 (11151700)\n",
      "IGNORED: Site 11151870 is in aquifer blacklist\n",
      "Working on CA site 224/483 (11152000)\n",
      "IGNORED: Site 11152050 is in aquifer blacklist\n",
      "IGNORED: Site 11152300 is in aquifer blacklist\n",
      "Working on CA site 227/483 (11152500)\n",
      "Working on CA site 228/483 (11152650)\n",
      "Working on CA site 229/483 (11153000)\n",
      "IGNORED: Site 11153650 is in aquifer blacklist\n",
      "Working on CA site 231/483 (11156500)\n",
      "Working on CA site 232/483 (11157500)\n",
      "Working on CA site 233/483 (11158600)\n",
      "Working on CA site 234/483 (11159000)\n",
      "Working on CA site 235/483 (11159200)\n",
      "IGNORED: Site 11159490 is in aquifer blacklist\n",
      "IGNORED: Site 11159500 is in aquifer blacklist\n",
      "IGNORED: Site 11159690 is in aquifer blacklist\n",
      "Working on CA site 239/483 (11160000)\n",
      "Working on CA site 240/483 (11160500)\n",
      "Working on CA site 241/483 (11161000)\n",
      "Working on CA site 242/483 (11162500)\n",
      "Working on CA site 243/483 (11162570)\n",
      "IGNORED: Site 111626182 is in aquifer blacklist\n",
      "IGNORED: Site 11162619 is in aquifer blacklist\n",
      "IGNORED: Site 11162620 is in aquifer blacklist\n",
      "Working on CA site 247/483 (11162630)\n",
      "IGNORED: Site 11162735 is in aquifer blacklist\n",
      "IGNORED: Site 11162737 is in aquifer blacklist\n",
      "IGNORED: Site 11162753 is in aquifer blacklist\n",
      "Working on CA site 251/483 (11164500)\n",
      "IGNORED: Site 11169025 is in aquifer blacklist\n",
      "Working on CA site 253/483 (11169500)\n",
      "Working on CA site 254/483 (11169800)\n",
      "IGNORED: Site 11169860 is in aquifer blacklist\n",
      "IGNORED: Site 11170000 is in aquifer blacklist\n",
      "IGNORED: Site 11172175 is in aquifer blacklist\n",
      "IGNORED: Site 11172945 is in aquifer blacklist\n",
      "IGNORED: Site 11172955 is in aquifer blacklist\n",
      "Working on CA site 260/483 (11173200)\n",
      "Working on CA site 261/483 (11173500)\n",
      "IGNORED: Site 11173510 is in aquifer blacklist\n",
      "IGNORED: Site 11173575 is in aquifer blacklist\n",
      "IGNORED: Site 11173800 is in aquifer blacklist\n",
      "Working on CA site 265/483 (11174000)\n",
      "IGNORED: Site 11174160 is in aquifer blacklist\n",
      "IGNORED: Site 11174600 is in aquifer blacklist\n",
      "IGNORED: Site 11176340 is in aquifer blacklist\n",
      "Working on CA site 269/483 (11176400)\n",
      "Working on CA site 270/483 (11176500)\n",
      "Working on CA site 271/483 (11176900)\n",
      "Working on CA site 272/483 (11179000)\n",
      "IGNORED: Site 11179100 is in aquifer blacklist\n",
      "Working on CA site 274/483 (11180500)\n",
      "Working on CA site 275/483 (11180700)\n",
      "IGNORED: Site 11180825 is in aquifer blacklist\n",
      "IGNORED: Site 11180900 is in aquifer blacklist\n",
      "IGNORED: Site 11180960 is in aquifer blacklist\n",
      "Working on CA site 279/483 (11181000)\n",
      "IGNORED: Site 11181008 is in aquifer blacklist\n",
      "Working on CA site 281/483 (11181040)\n",
      "Working on CA site 282/483 (11182500)\n",
      "Working on CA site 283/483 (11189500)\n",
      "Working on CA site 284/483 (11200800)\n",
      "IGNORED: Site 11203580 is in aquifer blacklist\n",
      "IGNORED: Site 11204100 is in aquifer blacklist\n",
      "IGNORED: Site 11206820 is in aquifer blacklist\n",
      "IGNORED: Site 11224000 is in aquifer blacklist\n",
      "Working on CA site 289/483 (11224500)\n",
      "Working on CA site 290/483 (11251000)\n",
      "Working on CA site 291/483 (11253310)\n",
      "IGNORED: Site 11255575 is in aquifer blacklist\n",
      "IGNORED: Site 11261100 is in aquifer blacklist\n",
      "Working on CA site 294/483 (11261500)\n",
      "IGNORED: Site 11262900 is in aquifer blacklist\n",
      "Working on CA site 296/483 (11264500)\n",
      "Working on CA site 297/483 (11266500)\n",
      "IGNORED: Site 11273400 is in aquifer blacklist\n",
      "Working on CA site 299/483 (11274000)\n",
      "Working on CA site 300/483 (11274500)\n",
      "IGNORED: Site 11274550 is in aquifer blacklist\n",
      "Working on CA site 302/483 (11274630)\n",
      "IGNORED: Site 11274790 is in aquifer blacklist\n",
      "Working on CA site 304/483 (11276500)\n",
      "Working on CA site 305/483 (11276600)\n",
      "Working on CA site 306/483 (11276900)\n",
      "Working on CA site 307/483 (11277300)\n",
      "Working on CA site 308/483 (11278000)\n",
      "Working on CA site 309/483 (11278300)\n",
      "Working on CA site 310/483 (11278400)\n",
      "Working on CA site 311/483 (11284400)\n",
      "Working on CA site 312/483 (11289000)\n",
      "Working on CA site 313/483 (11289500)\n",
      "Working on CA site 314/483 (11289650)\n",
      "IGNORED: Site 11289850 is in aquifer blacklist\n",
      "Working on CA site 316/483 (11290000)\n",
      "IGNORED: Site 11299600 is in aquifer blacklist\n",
      "Working on CA site 318/483 (11303000)\n",
      "Working on CA site 319/483 (11303500)\n",
      "IGNORED: Site 11304810 is in aquifer blacklist\n",
      "IGNORED: Site 11311300 is in aquifer blacklist\n",
      "IGNORED: Site 11312672 is in aquifer blacklist\n",
      "IGNORED: Site 11312676 is in aquifer blacklist\n",
      "IGNORED: Site 11312685 is in aquifer blacklist\n",
      "IGNORED: Site 11312968 is in aquifer blacklist\n",
      "IGNORED: Site 11313240 is in aquifer blacklist\n",
      "IGNORED: Site 11313315 is in aquifer blacklist\n",
      "IGNORED: Site 11313405 is in aquifer blacklist\n",
      "IGNORED: Site 11313431 is in aquifer blacklist\n",
      "IGNORED: Site 11313433 is in aquifer blacklist\n",
      "IGNORED: Site 11313434 is in aquifer blacklist\n",
      "IGNORED: Site 11313440 is in aquifer blacklist\n",
      "IGNORED: Site 11313452 is in aquifer blacklist\n",
      "IGNORED: Site 11313460 is in aquifer blacklist\n",
      "Working on CA site 335/483 (11335000)\n",
      "IGNORED: Site 11336600 is in aquifer blacklist\n",
      "IGNORED: Site 11336685 is in aquifer blacklist\n",
      "IGNORED: Site 11336790 is in aquifer blacklist\n",
      "IGNORED: Site 11336930 is in aquifer blacklist\n",
      "IGNORED: Site 11336955 is in aquifer blacklist\n",
      "IGNORED: Site 11337080 is in aquifer blacklist\n",
      "IGNORED: Site 11337190 is in aquifer blacklist\n",
      "Working on CA site 343/483 (11342000)\n",
      "Working on CA site 344/483 (11345500)\n",
      "Working on CA site 345/483 (11348500)\n",
      "IGNORED: Site 11355010 is in aquifer blacklist\n",
      "Working on CA site 347/483 (11355500)\n",
      "Working on CA site 348/483 (11370500)\n",
      "IGNORED: Site 11370700 is in aquifer blacklist\n",
      "Working on CA site 350/483 (11372000)\n",
      "Working on CA site 351/483 (11374000)\n",
      "Working on CA site 352/483 (11376000)\n",
      "Working on CA site 353/483 (11376550)\n",
      "Working on CA site 354/483 (11377100)\n",
      "Working on CA site 355/483 (11379500)\n",
      "Working on CA site 356/483 (11381500)\n",
      "Working on CA site 357/483 (11383500)\n",
      "Working on CA site 358/483 (11389500)\n",
      "Working on CA site 359/483 (11390000)\n",
      "Working on CA site 360/483 (11390500)\n",
      "IGNORED: Site 11401920 is in aquifer blacklist\n",
      "Working on CA site 362/483 (11402000)\n",
      "Working on CA site 363/483 (11413000)\n",
      "Working on CA site 364/483 (11418500)\n",
      "Working on CA site 365/483 (11421000)\n",
      "Working on CA site 366/483 (11424000)\n",
      "Working on CA site 367/483 (11425500)\n",
      "Working on CA site 368/483 (11427000)\n",
      "Working on CA site 369/483 (11446500)\n",
      "Working on CA site 370/483 (11447360)\n",
      "Working on CA site 371/483 (11447650)\n",
      "IGNORED: Site 11447830 is in aquifer blacklist\n",
      "IGNORED: Site 11447850 is in aquifer blacklist\n",
      "IGNORED: Site 11447890 is in aquifer blacklist\n",
      "IGNORED: Site 11447903 is in aquifer blacklist\n",
      "IGNORED: Site 11447905 is in aquifer blacklist\n",
      "IGNORED: Site 11448750 is in aquifer blacklist\n",
      "IGNORED: Site 11448800 is in aquifer blacklist\n",
      "IGNORED: Site 11449255 is in aquifer blacklist\n",
      "Working on CA site 380/483 (11449500)\n",
      "Working on CA site 381/483 (11451000)\n",
      "IGNORED: Site 11451100 is in aquifer blacklist\n",
      "IGNORED: Site 11451300 is in aquifer blacklist\n",
      "IGNORED: Site 11451715 is in aquifer blacklist\n",
      "IGNORED: Site 11451800 is in aquifer blacklist\n",
      "Working on CA site 386/483 (11452500)\n",
      "IGNORED: Site 11452800 is in aquifer blacklist\n",
      "IGNORED: Site 11452900 is in aquifer blacklist\n",
      "Working on CA site 389/483 (11453000)\n",
      "Working on CA site 390/483 (11453500)\n",
      "IGNORED: Site 11453590 is in aquifer blacklist\n",
      "Working on CA site 392/483 (11454000)\n",
      "IGNORED: Site 11455095 is in aquifer blacklist\n",
      "IGNORED: Site 11455140 is in aquifer blacklist\n",
      "IGNORED: Site 11455280 is in aquifer blacklist\n",
      "IGNORED: Site 11455315 is in aquifer blacklist\n",
      "IGNORED: Site 11455338 is in aquifer blacklist\n",
      "IGNORED: Site 11455385 is in aquifer blacklist\n",
      "IGNORED: Site 11455420 is in aquifer blacklist\n",
      "Working on CA site 400/483 (11456000)\n",
      "Working on CA site 401/483 (11458000)\n",
      "IGNORED: Site 11458433 is in aquifer blacklist\n",
      "Working on CA site 403/483 (11458500)\n",
      "IGNORED: Site 11458600 is in aquifer blacklist\n",
      "Working on CA site 405/483 (11459500)\n",
      "Working on CA site 406/483 (11460000)\n",
      "IGNORED: Site 11460151 is in aquifer blacklist\n",
      "IGNORED: Site 11460400 is in aquifer blacklist\n",
      "IGNORED: Site 11460600 is in aquifer blacklist\n",
      "IGNORED: Site 11460605 is in aquifer blacklist\n",
      "IGNORED: Site 11460750 is in aquifer blacklist\n",
      "Working on CA site 412/483 (11461500)\n",
      "IGNORED: Site 11462080 is in aquifer blacklist\n",
      "Working on CA site 414/483 (11462500)\n",
      "Working on CA site 415/483 (11463000)\n",
      "IGNORED: Site 11463170 is in aquifer blacklist\n",
      "Working on CA site 417/483 (11463200)\n",
      "Working on CA site 418/483 (11463500)\n",
      "IGNORED: Site 11463682 is in aquifer blacklist\n",
      "Working on CA site 420/483 (11463900)\n",
      "IGNORED: Site 11463980 is in aquifer blacklist\n",
      "Working on CA site 422/483 (11464000)\n",
      "IGNORED: Site 11465240 is in aquifer blacklist\n",
      "IGNORED: Site 11465350 is in aquifer blacklist\n",
      "IGNORED: Site 11465390 is in aquifer blacklist\n",
      "IGNORED: Site 11465660 is in aquifer blacklist\n",
      "IGNORED: Site 11465680 is in aquifer blacklist\n",
      "IGNORED: Site 11465690 is in aquifer blacklist\n",
      "IGNORED: Site 11465700 is in aquifer blacklist\n",
      "IGNORED: Site 11465750 is in aquifer blacklist\n",
      "IGNORED: Site 11466170 is in aquifer blacklist\n",
      "Working on CA site 432/483 (11466200)\n",
      "IGNORED: Site 11466320 is in aquifer blacklist\n",
      "IGNORED: Site 11466800 is in aquifer blacklist\n",
      "Working on CA site 435/483 (11467000)\n",
      "Working on CA site 436/483 (11467200)\n",
      "IGNORED: Site 11467510 is in aquifer blacklist\n",
      "IGNORED: Site 11467553 is in aquifer blacklist\n",
      "Working on CA site 439/483 (11468000)\n",
      "Working on CA site 440/483 (11468500)\n",
      "IGNORED: Site 11468900 is in aquifer blacklist\n",
      "Working on CA site 442/483 (11469000)\n",
      "IGNORED: Site 11472180 is in aquifer blacklist\n",
      "Working on CA site 444/483 (11473900)\n",
      "Working on CA site 445/483 (11475000)\n",
      "Working on CA site 446/483 (11475560)\n",
      "Working on CA site 447/483 (11475800)\n",
      "Working on CA site 448/483 (11476500)\n",
      "Working on CA site 449/483 (11476600)\n",
      "Working on CA site 450/483 (11477000)\n",
      "Working on CA site 451/483 (11478500)\n",
      "IGNORED: Site 11480390 is in aquifer blacklist\n",
      "Working on CA site 453/483 (11481000)\n",
      "Working on CA site 454/483 (11481200)\n",
      "Working on CA site 455/483 (11481500)\n",
      "Working on CA site 456/483 (11482500)\n",
      "Working on CA site 457/483 (11516530)\n",
      "Working on CA site 458/483 (11517000)\n",
      "Working on CA site 459/483 (11517500)\n",
      "Working on CA site 460/483 (11519500)\n",
      "Working on CA site 461/483 (11520500)\n",
      "Working on CA site 462/483 (11521500)\n",
      "Working on CA site 463/483 (11522500)\n",
      "Working on CA site 464/483 (11523000)\n",
      "Working on CA site 465/483 (11523200)\n",
      "Working on CA site 466/483 (11525500)\n",
      "IGNORED: Site 11525530 is in aquifer blacklist\n",
      "IGNORED: Site 11525655 is in aquifer blacklist\n",
      "IGNORED: Site 11525670 is in aquifer blacklist\n",
      "IGNORED: Site 11525854 is in aquifer blacklist\n",
      "IGNORED: Site 11526250 is in aquifer blacklist\n",
      "IGNORED: Site 11526400 is in aquifer blacklist\n",
      "Working on CA site 473/483 (11527000)\n",
      "Working on CA site 474/483 (11528700)\n",
      "Working on CA site 475/483 (11530000)\n",
      "Working on CA site 476/483 (11530500)\n",
      "Working on CA site 477/483 (11532500)\n",
      "IGNORED: Site 351813119150601 is in aquifer blacklist\n",
      "IGNORED: Site 360013118575201 is in aquifer blacklist\n",
      "IGNORED: Site 373507121472101 is in aquifer blacklist\n",
      "IGNORED: Site 375450121331701 is in aquifer blacklist\n",
      "Working on CA site 482/483 (10336698)\n",
      "Working on CA site 483/483 (10336700)\n"
     ]
    }
   ],
   "source": [
    "df_natl_metrics = pd.DataFrame()\n",
    "df_natl_mk_mag = pd.DataFrame()\n",
    "df_natl_mk_dur = pd.DataFrame()\n",
    "df_natl_mk_intra = pd.DataFrame()\n",
    "\n",
    "natl_blacklist = []\n",
    "\n",
    "for state_index, state in enumerate(fn.STATE_LIST):\n",
    "    if state_index >= state_limit: break\n",
    "    \n",
    "    print(f'[---Working on {state}---]')\n",
    "    state_uri = fn.create_state_uri(state, fn.PARAM_CODE)\n",
    "    df_state_sites = filter_state_site(shapefile_path, state_uri)\n",
    "    df_state_sites = df_state_sites.reset_index()\n",
    "    print(f'Total Sites: {len(df_state_sites)} in the state of {state}')\n",
    "    \n",
    "    # Modified version of the create_multi_site_data() function\n",
    "    for site_index, row in df_state_sites.iterrows():\n",
    "        \n",
    "        if allow_blacklist and str(row['site_no']) in curr_blacklist:\n",
    "            print(f'IGNORED: Site {row[\"site_no\"]} is in aquifer blacklist')\n",
    "            continue\n",
    "        \n",
    "        df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "        df = df.reset_index()\n",
    "        print(f'Working on {state} site {site_index + 1}/{len(df_state_sites)} ({row[\"site_no\"]})')\n",
    "        \n",
    "        if df.empty:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: No data for site {row[\"site_no\"]}')\n",
    "            continue\n",
    "        \n",
    "        if '00060_Mean' not in df.columns:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: No 00060_Mean data for site {row[\"site_no\"]}')\n",
    "            continue\n",
    "        \n",
    "        start = df['datetime'].min().date()\n",
    "        end = df['datetime'].max().date()\n",
    "        range = round((end - start).days / 365.25, 1)\n",
    "        \n",
    "        if range < fn.MIN_DATA_PERIOD:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: Not enough data for site {row[\"site_no\"]}')\n",
    "            continue\n",
    "        \n",
    "        point = Point(row['dec_long_va'], row['dec_lat_va'])\n",
    "        \n",
    "        huc2 = huc4 = aquifer = 'NA'\n",
    "        # HUC2 assignment\n",
    "        for i, geo_row in huc2_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                huc2 = geo_row['huc2_code']\n",
    "                continue\n",
    "            \n",
    "        # HUC4 assignment\n",
    "        for i, geo_row in huc4_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                huc4 = geo_row['huc4_code']\n",
    "                continue\n",
    "            \n",
    "        # Aquifer assignment    \n",
    "        for i, geo_row in aq_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                aquifer = geo_row['aq_name']\n",
    "                continue        \n",
    "        \n",
    "        # A few very broken sites with almost no data can have 0 hmf years and cause errors (i.e. '03592000')\n",
    "        # so we'll catch these and add them to the site blacklist\n",
    "        try:\n",
    "            df_single_site_metric, df_mk_mag, df_mk_dur, df_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "            add_data = {'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va'], 'data_start': start, 'data_end': end, 'total_record': range, \n",
    "                        'state': row['STUSPS'], 'huc2_code': huc2, 'huc4_code': huc4, 'within_aq': aquifer}\t\t\t\n",
    "            add_data = pd.DataFrame(add_data, index=['0'])\n",
    "        except Exception as e:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f\"ERROR: Single site data failure for site {row['site_no']}:\\n{e}\")\n",
    "            continue\n",
    "        \n",
    "        add_data = pd.DataFrame(np.tile(add_data.values, (len(df_single_site_metric), 1)), columns=add_data.columns)\n",
    "        df_single_site_metric = pd.concat([df_single_site_metric.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_mag = pd.concat([df_mk_mag.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_dur = pd.concat([df_mk_dur.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_intra = pd.concat([df_mk_intra.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "        # Append single site data to multi-site dataframes\n",
    "        df_natl_metrics = pd.concat([df_natl_metrics, df_single_site_metric], ignore_index=True)\n",
    "        df_natl_mk_mag = pd.concat([df_natl_mk_mag, df_mk_mag], ignore_index=True)\n",
    "        df_natl_mk_dur = pd.concat([df_natl_mk_dur, df_mk_dur], ignore_index=True)\n",
    "        df_natl_mk_intra = pd.concat([df_natl_mk_intra, df_mk_intra], ignore_index=True)\n",
    "        \n",
    "df_natl_metrics = fn.gages_2_filtering(df_natl_metrics)\n",
    "\n",
    "# Create national site blacklist\n",
    "try:\n",
    "    with open(f'Prelim_Data/National_Sites_Blacklist.txt', 'w') as f:\n",
    "        natl_blacklist = [\"'\" + str(x) + \"'\" for x in natl_blacklist]        \n",
    "        f.write(', '.join(natl_blacklist))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:    \n",
    "    fn.save_data(df_natl_metrics, df_natl_mk_mag, df_natl_mk_dur, df_natl_mk_intra, dataset_name)\n",
    "except Exception as e:\n",
    "    print(f'Error saving data: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: Nationwide Validity Dataset\n",
    "Generates a dataset for every water gauge with 00060_Mean data in the contiguous US indicating gauge validity for this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = 30\n",
    "shapefile_path = 'ShapeFiles/Lower48/lower48.shp'\n",
    "test_limit = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on AL---]\n",
      "Total Sites: 173 in the state of AL\n",
      "Ignored 35 sites (20.23%)\n",
      "[---Working on AZ---]\n",
      "Total Sites: 218 in the state of AZ\n",
      "Ignored 11 sites (5.05%)\n",
      "[---Working on AR---]\n",
      "Total Sites: 139 in the state of AR\n",
      "Ignored 16 sites (11.51%)\n",
      "[---Working on CA---]\n",
      "Total Sites: 481 in the state of CA\n",
      "Ignored 54 sites (11.23%)\n",
      "[---Working on CO---]\n",
      "Total Sites: 349 in the state of CO\n",
      "Ignored 14 sites (4.01%)\n",
      "[---Working on CT---]\n",
      "Total Sites: 74 in the state of CT\n",
      "Ignored 4 sites (5.41%)\n",
      "[---Working on DE---]\n",
      "Total Sites: 36 in the state of DE\n",
      "Ignored 3 sites (8.33%)\n",
      "[---Working on FL---]\n",
      "Total Sites: 434 in the state of FL\n",
      "Ignored 39 sites (8.99%)\n",
      "[---Working on GA---]\n",
      "Total Sites: 308 in the state of GA\n",
      "Ignored 17 sites (5.52%)\n",
      "[---Working on ID---]\n",
      "Total Sites: 229 in the state of ID\n",
      "Ignored 9 sites (3.93%)\n",
      "[---Working on IL---]\n",
      "Total Sites: 193 in the state of IL\n",
      "Ignored 7 sites (3.63%)\n",
      "[---Working on IN---]\n",
      "Total Sites: 205 in the state of IN\n",
      "Ignored 20 sites (9.76%)\n",
      "[---Working on IA---]\n",
      "Total Sites: 143 in the state of IA\n",
      "Ignored 7 sites (4.9%)\n",
      "[---Working on KS---]\n",
      "Total Sites: 195 in the state of KS\n",
      "Ignored 5 sites (2.56%)\n",
      "[---Working on KY---]\n",
      "Total Sites: 169 in the state of KY\n",
      "Ignored 8 sites (4.73%)\n",
      "[---Working on LA---]\n",
      "Total Sites: 98 in the state of LA\n",
      "Ignored 27 sites (27.55%)\n",
      "[---Working on ME---]\n",
      "Total Sites: 68 in the state of ME\n",
      "Ignored 0 sites (0.0%)\n",
      "[---Working on MD---]\n",
      "Total Sites: 190 in the state of MD\n",
      "Ignored 8 sites (4.21%)\n",
      "[---Working on MA---]\n",
      "Total Sites: 146 in the state of MA\n",
      "Ignored 15 sites (10.27%)\n",
      "[---Working on MI---]\n",
      "Total Sites: 197 in the state of MI\n",
      "Ignored 16 sites (8.12%)\n",
      "[---Working on MN---]\n",
      "Total Sites: 130 in the state of MN\n",
      "Ignored 7 sites (5.38%)\n",
      "[---Working on MS---]\n",
      "Total Sites: 99 in the state of MS\n",
      "Ignored 16 sites (16.16%)\n",
      "[---Working on MO---]\n",
      "Total Sites: 244 in the state of MO\n",
      "Ignored 2 sites (0.82%)\n",
      "[---Working on MT---]\n",
      "Total Sites: 226 in the state of MT\n",
      "Ignored 7 sites (3.1%)\n",
      "[---Working on NE---]\n",
      "Total Sites: 120 in the state of NE\n",
      "Ignored 1 sites (0.83%)\n",
      "[---Working on NV---]\n",
      "Total Sites: 185 in the state of NV\n",
      "Ignored 9 sites (4.86%)\n",
      "[---Working on NH---]\n",
      "Total Sites: 101 in the state of NH\n",
      "Ignored 2 sites (1.98%)\n",
      "[---Working on NJ---]\n",
      "Total Sites: 141 in the state of NJ\n",
      "Ignored 10 sites (7.09%)\n",
      "[---Working on NM---]\n",
      "Total Sites: 174 in the state of NM\n",
      "Ignored 10 sites (5.75%)\n",
      "[---Working on NY---]\n",
      "Total Sites: 321 in the state of NY\n",
      "Ignored 16 sites (4.98%)\n",
      "[---Working on NC---]\n",
      "Total Sites: 240 in the state of NC\n",
      "Ignored 10 sites (4.17%)\n",
      "[---Working on ND---]\n",
      "Total Sites: 123 in the state of ND\n",
      "Ignored 9 sites (7.32%)\n",
      "[---Working on OH---]\n",
      "Total Sites: 222 in the state of OH\n",
      "Ignored 16 sites (7.21%)\n",
      "[---Working on OK---]\n",
      "Total Sites: 178 in the state of OK\n",
      "Ignored 9 sites (5.06%)\n",
      "[---Working on OR---]\n",
      "Total Sites: 213 in the state of OR\n",
      "Ignored 7 sites (3.29%)\n",
      "[---Working on PA---]\n",
      "Total Sites: 311 in the state of PA\n",
      "Ignored 6 sites (1.93%)\n",
      "[---Working on RI---]\n",
      "Total Sites: 34 in the state of RI\n",
      "Ignored 2 sites (5.88%)\n",
      "[---Working on SC---]\n",
      "Total Sites: 131 in the state of SC\n",
      "Ignored 12 sites (9.16%)\n",
      "[---Working on SD---]\n",
      "Total Sites: 122 in the state of SD\n",
      "Ignored 4 sites (3.28%)\n",
      "[---Working on TN---]\n",
      "Total Sites: 136 in the state of TN\n",
      "Ignored 15 sites (11.03%)\n",
      "[---Working on TX---]\n",
      "Total Sites: 597 in the state of TX\n",
      "Ignored 64 sites (10.72%)\n",
      "[---Working on UT---]\n",
      "Total Sites: 153 in the state of UT\n",
      "Ignored 4 sites (2.61%)\n",
      "[---Working on VT---]\n",
      "Total Sites: 98 in the state of VT\n",
      "Ignored 2 sites (2.04%)\n",
      "[---Working on VA---]\n",
      "Total Sites: 200 in the state of VA\n",
      "Ignored 8 sites (4.0%)\n",
      "[---Working on WA---]\n",
      "Total Sites: 265 in the state of WA\n",
      "Ignored 11 sites (4.15%)\n",
      "[---Working on WV---]\n",
      "Total Sites: 110 in the state of WV\n",
      "Ignored 2 sites (1.82%)\n",
      "[---Working on WI---]\n",
      "Total Sites: 220 in the state of WI\n",
      "Ignored 31 sites (14.09%)\n",
      "[---Working on WY---]\n",
      "Total Sites: 116 in the state of WY\n",
      "Ignored 1 sites (0.86%)\n"
     ]
    }
   ],
   "source": [
    "df_natl_validity = pd.DataFrame()\n",
    "\n",
    "for i, state in enumerate(fn.STATE_LIST):\n",
    "    if i >= test_limit: break\n",
    "    print(f'[---Working on {state}---]')\n",
    "    state_uri = fn.create_state_uri(state, fn.PARAM_CODE)\n",
    "    df_state_sites = filter_state_site(shapefile_path, state_uri)\n",
    "    print(f'Total Sites: {len(df_state_sites)} in the state of {state}')\n",
    "    \n",
    "    ignored_count = 0\n",
    "    for loc, row in df_state_sites.iterrows():\n",
    "        df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        if '00060_Mean' not in df.columns:\n",
    "            ignored_count += 1\n",
    "            continue\n",
    "        \n",
    "        start = df['datetime'].min().date()\n",
    "        end = df['datetime'].max().date()\n",
    "        range = round((end - start).days / 365.25, 1)\n",
    "        \n",
    "        valid_range = range > date_range\n",
    "        \n",
    "        date_threshold = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * date_range)\n",
    "        df = df[df['datetime'].dt.date >= date_threshold]\n",
    "        missing = fn.validate(df, date_threshold, fn.DEFAULT_END)\n",
    "        valid = missing < fn.MAX_MISSING_THRESHOLD\n",
    "        \n",
    "        data = {'site_no': row['site_no'], 'state': state, 'data_range': valid_range, 'data_cont': valid, 'start': start, 'end': end, \n",
    "                'total_record': range, 'missing': missing, 'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va']}\n",
    "        \n",
    "        df_natl_validity = pd.concat([df_natl_validity, pd.DataFrame(data, index=['0'])], axis=0, ignore_index=True)\n",
    "        \n",
    "    print(f'Ignored {ignored_count} sites ({round((ignored_count / len(df_state_sites)) * 100, 2)}%)')        \n",
    "        \n",
    "try:\n",
    "    with pd.ExcelWriter('Prelim_Data/National_Validity_30.xlsx') as writer:\n",
    "        df_natl_validity.to_excel(writer, sheet_name='national_validity', index=False) \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: HUC2/HUC4/Aquifer Master Shapefile Creation\n",
    "The following code combines HUC2, HUC4, and Aquifer shapefiles into master shapefiles, respectively. It's not necessary to run if these files already exist within the ShapeFiles directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc2_path = 'ShapeFiles/HUC2'\n",
    "huc4_path = 'ShapeFiles/HUC4'\n",
    "aquifer_path = 'ShapeFiles/Aquifers'\n",
    "aq_ext = '.shp'\n",
    "huc2 = 'WBDHU2.shp'\n",
    "huc4 = 'WBDHU4.shp'\n",
    "\n",
    "create_huc2 = False\n",
    "create_huc4 = False\n",
    "create_aquifer = True\n",
    "\n",
    "huc2_gdf = gpd.GeoDataFrame()\n",
    "huc4_gdf = gpd.GeoDataFrame()\n",
    "aquifer_df = gpd.GeoDataFrame()\n",
    "\n",
    "# HUC2's\n",
    "if create_huc2:\n",
    "    for root, dirs, files in os.walk(huc2_path):\n",
    "        if os.path.basename(root).startswith('WBD_'):\n",
    "            code = root[-2:]\n",
    "            if huc2 in files:\n",
    "                shape = gpd.read_file(os.path.join(root, huc2))\n",
    "                shape['huc2_code'] = code\n",
    "                huc2_gdf = pd.concat([huc2_gdf, shape], ignore_index=True)\n",
    "\n",
    "    huc2_gdf = huc2_gdf.to_crs(4269)\n",
    "    huc2_gdf.to_file('ShapeFiles/HUC2/_Master_HUC2/master_huc2.shp')\n",
    "\n",
    "# HUC4's    \n",
    "if create_huc4:\n",
    "    for root, dirs, files in os.walk(huc4_path):\n",
    "        if os.path.basename(root).startswith('NHD_H_'):\n",
    "            code = root[-4:]\n",
    "            if huc4 in files:\n",
    "                shape = gpd.read_file(os.path.join(root, huc4))\n",
    "                shape['huc4_code'] = code\n",
    "                huc4_gdf = pd.concat([huc4_gdf, shape], ignore_index=True)\n",
    "\n",
    "    huc4_gdf = huc4_gdf.to_crs(4269)\n",
    "    huc4_gdf.to_file('ShapeFiles/HUC4/_Master_HUC4/master_huc4.shp')\n",
    "    \n",
    "# Aquifers\n",
    "if create_aquifer:\n",
    "    for root, dirs, files in os.walk(aquifer_path):\n",
    "        for file in files:\n",
    "            if file.endswith(aq_ext) and dirs:\n",
    "                if file.startswith('Penn'):\n",
    "                    shape = gpd.read_file(os.path.join(root, file))\n",
    "                    shape = shape.iloc[[17]]                    \n",
    "                else:\n",
    "                    shape = gpd.read_file(os.path.join(root, file))\n",
    "                    \n",
    "                shape = shape.to_crs(4269)\n",
    "                shape['aq_name'] = file[:-4]\n",
    "                aquifer_df = pd.concat([aquifer_df, shape], ignore_index=True)\n",
    "                \n",
    "    aquifer_df = aquifer_df.to_crs(4269)\n",
    "    aquifer_df.to_file('ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: HUC2/4/Aquifer Sorting\n",
    "Takes a national validity or national metrics dataset (any dataset with dec_lat_va and dec_long_va data), and adds corresponding HUC2, HUC4, and Aquifer columns indicating a water guages presence or lack thereof in each of these boundaries.\n",
    "\n",
    "<strong>NOTE:</strong> This feature has already been implemented directly into the National Metrics dataset creation. Its only real use is on National Validity datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Prelim_Data/_National_Metrics/'\n",
    "datasets = ['National_Metrics_30_90.xlsx']\n",
    "huc2_gdf = gpd.read_file('ShapeFiles/HUC2/_Master_HUC2/master_huc2.shp')\n",
    "huc4_gdf = gpd.read_file('ShapeFiles/HUC4/_Master_HUC4/master_huc4.shp')\n",
    "aq_gdf = gpd.read_file('ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data = pd.DataFrame({'site_no': 'NA', 'huc2_code': 'NA', 'huc4_code': 'NA', 'aquifer': 'NA'}, index=['0'])\n",
    "temp = pd.DataFrame()\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.read_excel(f'{path}{dataset}', dtype={'site_no': str})\n",
    "    sheets = pd.ExcelFile(f'{path}{dataset}')\n",
    "    sheet = sheets.sheet_names[0]          \n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        point = Point(row['dec_long_va'], row['dec_lat_va'])\n",
    "        add_data['site_no'] = row['site_no']\n",
    "        \n",
    "        # HUC2 assignment\n",
    "        for j, geo_row in huc2_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                add_data[\"huc2_code\"] = geo_row['huc2_code']\n",
    "                continue\n",
    "            \n",
    "        # HUC4 assignment\n",
    "        for j, geo_row in huc4_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                add_data[\"huc4_code\"] = geo_row['huc4_code']\n",
    "                continue\n",
    "            \n",
    "        for j, geo_row in aq_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                add_data[\"aquifer\"] = geo_row['aq_name']\n",
    "                continue\n",
    "            \n",
    "        temp = pd.concat([temp, add_data], axis=0, ignore_index=True)\n",
    "    \n",
    "    df = pd.merge(df, temp, on='site_no')\n",
    "    \n",
    "    with pd.ExcelWriter(f'{path}{dataset}', mode='a', if_sheet_exists='replace') as writer:\n",
    "        df.to_excel(writer, sheet_name=sheet, index=False)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: National Dataset Splitting by Aquifer\n",
    "This script splits a national metric dataset or datasets into smaller per-aquifer datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "natl_path = 'Prelim_Data/_National_Metrics'\n",
    "\n",
    "# National datasets to split\n",
    "datasets = ['National_Metrics_30_90.xlsx', 'National_Metrics_30_95.xlsx', 'National_Metrics_50_90.xlsx', 'National_Metrics_50_95.xlsx']\n",
    "# The datasets to generate from the national dataset\n",
    "target_aquifers = cl.ALL_AQUIFERS\n",
    "sheet_names = ['site_metrics', 'mk_magnitude', 'mk_duration', 'mk_intra_annual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over national datasets\n",
    "for dataset in datasets:\n",
    "    df_list = []\n",
    "    for sheet in sheet_names:\n",
    "        df = pd.read_excel(f'{natl_path}/{dataset}', sheet_name=sheet, dtype=fn.DATASET_DTYPES)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Iterate over target aquifers\n",
    "    for aquifer in target_aquifers:\n",
    "        save_path = f\"{aquifer.datasets_dir}/{aquifer.name}_{dataset[-10:-8]}_{dataset[-7:-5]}.xlsx\"\n",
    "        for i, df in enumerate(df_list):\n",
    "            df = df[df['huc4_code'].isin(aquifer.huc4s)]\n",
    "            \n",
    "            # Append new sheets to existing file\n",
    "            if os.path.exists(save_path):\n",
    "                with pd.ExcelWriter(save_path, mode='a', if_sheet_exists='replace') as writer:\n",
    "                    df.to_excel(writer, sheet_name=sheet_names[i], index=False)\n",
    "            # Create file if it doesn't exist (first iteration)                        \n",
    "            else: \n",
    "                with pd.ExcelWriter(save_path, mode='w') as writer:\n",
    "                    df.to_excel(writer, sheet_name=sheet_names[i], index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
