{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES:\n",
    "<ul>\n",
    "<li>Sections labeled <strong>CORE</strong> must be run in order to begin Aquifer Analysis</li>\n",
    "<li>Cells labeled <strong>Control</strong> contain inputs for the immeadiately proceeding section(s)</li>\n",
    "<li>Sections labeled <strong>EXTRA</strong> contain additional plotting or analysis tools but are not necessary for Aquifer Analysis</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python3.10\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import contextily as cx\n",
    "import requests\n",
    "import calendar\n",
    "from importlib import reload\n",
    "from typing import IO\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import Point\n",
    "from io import StringIO\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# USGS Data retreival tool\n",
    "from dataretrieval import nwis, utils, codes\n",
    "\n",
    "# Custom modules are imported in multiple locations to faciliate easy reloading when edits are made to their respective files\n",
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "reload(fn)\n",
    "\n",
    "# TODO: Look into the warning that this is disabling. It doesn't appear to be significant for the purposes of this code but should be understood\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#pd.options.mode.chained_assignment = 'warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Single Site Data<br>\n",
    "This function produces the streamflow analysis (using the functions above) for a single site given a list of thresholds and time windows to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "\n",
    "# df2 holds all-time data, df is analyzed range\n",
    "curr_guage = cl.SRB_Guage\n",
    "df = nwis.get_record(sites='11447650', service=fn.SERVICE, parameterCD=[fn.PARAM_CODE, fn.TIDAL_CODE], start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "df = fn.merge_tidal(df)\n",
    "#df2 = nwis.get_record(sites=curr_guage.id, service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2014-09-30')\n",
    "\n",
    "\n",
    "# Set to true if running indepedently to verify data with Kocis paper (or other source)\n",
    "# Set to false if running Aquifer Analysis\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Site Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site No: 0    11447650\n",
      "1    11447650\n",
      "2    11447650\n",
      "0    11447650\n",
      "Name: site_no, dtype: object\n",
      "Analyzed Range: 30\n",
      "30\n",
      "50\n",
      "50\n",
      "Quantile: 0.90\n",
      "0.95\n",
      "0.90\n",
      "0.95\n",
      "Valid: True\n",
      "True\n",
      "True\n",
      "True\n",
      "% Missing: 0\n",
      "0\n",
      "0\n",
      "0\n",
      "90%: 48800.0\n",
      "69000.0\n",
      "50600.0\n",
      "69500.0\n",
      "HMF Years: 20\n",
      "15\n",
      "33\n",
      "27\n",
      "Annual Duration: 54.600000\n",
      "36.133333\n",
      "55.090909\n",
      "33.814815\n",
      "Event Duration: 20.760833\n",
      "14.422222\n",
      "21.529293\n",
      "12.409171\n",
      "Event HMF: 1.033732\n",
      "0.290248\n",
      "1.045558\n",
      "0.260939\n",
      "Inter-annual Frequency: 66.667\n",
      "50.000\n",
      "66.000\n",
      "54.000%\n",
      "Intra-annual Frequency: 2.700000\n",
      "2.600000\n",
      "2.666667\n",
      "2.777778\n",
      "Total HMF in km^3/year: 2.632442\n",
      "0.751311\n",
      "2.585712\n",
      "0.719230\n",
      "Center of Mass: 144.600000\n",
      "159.000000\n",
      "140.636364\n",
      "147.370370\n",
      "6 Month HMF in km^3/year: 2.440887\n",
      "0.717613\n",
      "2.449815\n",
      "0.699585\n",
      "3 Month HMF in km^3/year: 1.373055\n",
      "0.484194\n",
      "1.399463\n",
      "0.465339\n"
     ]
    }
   ],
   "source": [
    "# If looking at a post impairment period, but calculating threshold based on the full record of data, pass a second dataframe with a different \n",
    "# start/end date as the final parameter. This method was used in Kocis 2017 and is needed for some data verification, but is not the methodology\n",
    "# used for the Aquifer Analysis and so *args will most often be empty.\n",
    "import Src.func as fn\n",
    "reload(fn)\n",
    "\n",
    "def single_site_data(df: pd.DataFrame, quantiles_list: list, data_ranges_list: list, *args):\n",
    "    df = df.reset_index()    \n",
    "    threshold = None\n",
    "    df_complete_site_data = pd.DataFrame()    \n",
    "    df_complete_mk_mag = pd.DataFrame()\n",
    "    df_complete_mk_dur = pd.DataFrame()\n",
    "    df_complete_mk_intra = pd.DataFrame()\n",
    "    df_complete_mk_event_mag = pd.DataFrame()\n",
    "    df_complete_mk_event_dur = pd.DataFrame()\n",
    "    df_complete_mk_timing = pd.DataFrame()\n",
    "    \n",
    "    for data_range in data_ranges_list:\n",
    "        for quantile in quantiles_list:\n",
    "            # Copying original dataframe to avoid any conflicts\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Validate that site is not missing > 10% of data\n",
    "            date_threshold = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * data_range)              \n",
    "            \n",
    "            # Filter dataframe based on current data_range (don't do this during testing if testing unique dataset/range)\n",
    "            if not testing:\n",
    "                df_copy = df_copy[df_copy['datetime'].dt.date >= date_threshold]\n",
    "            \n",
    "            # Validate <10% missing data based on filtered dataframe\n",
    "            missing = fn.validate(df_copy, date_threshold, fn.DEFAULT_END)\n",
    "            valid = missing < fn.MAX_MISSING_THRESHOLD\n",
    "            missing = round(missing, 5) * 100                              \n",
    "    \n",
    "            # Check for optional second dataframe containing full record\n",
    "            if args and isinstance(args[0], pd.DataFrame) and not args[0].empty:\n",
    "                #print('Threshold calculation across the full record')\n",
    "                df2 = args[0].reset_index()\n",
    "                threshold = fn.calc_threshold(df2, quantile)\n",
    "                # threshold = 52350 # SRB_Guage post-impairment threshold for verification with Kocis_2017 \n",
    "            else:\n",
    "                #print('Threshold calculation across a limited record')\n",
    "                threshold = fn.calc_threshold(df_copy, quantile)                             \n",
    "\n",
    "            # Create a dataframe with only days over HMF threshold as well as continuous dataframe for MK trend test\n",
    "            hmf_series_defl, hmf_series_cont = fn.filter_hmf(df_copy, threshold)\n",
    "            #print(zero_deflated_hmf)\n",
    "            \n",
    "            # Aggregate data before performing MK magnitude test\n",
    "            df_agg_cont = hmf_series_cont.copy()\n",
    "            df_agg_cont = fn.convert_hmf(df_agg_cont, threshold)\n",
    "            df_agg_cont['00060_Mean'] = df_agg_cont['00060_Mean'].apply(lambda x: x * fn.CUBIC_FT_KM_FACTOR if x >= 0 else 0)\n",
    "            df_agg_cont.set_index('datetime', inplace=True)\n",
    "            df_agg_cont = df_agg_cont.resample(fn.HYDRO_YEAR).agg({'00060_Mean': ['sum', 'count']})\n",
    "            df_agg_cont.columns = ['Sum', 'Count']\n",
    "            df_agg_cont_defl = df_agg_cont[df_agg_cont['Sum'] > 0]          \n",
    "            \n",
    "            # MK Magnitude\n",
    "            df_mk_mag = fn.mann_kendall(df_agg_cont_defl['Sum'], df_agg_cont['Sum'], fn.MK_TREND_ALPHA)\n",
    "\n",
    "            # Find number of years with HMF\n",
    "            hmf_years = fn.num_hmf_years(hmf_series_defl)            \n",
    "\n",
    "            # Mask out months that don't fall within 3 and 6 month Winter range\n",
    "            df_six_month, df_three_month = fn.three_six_range(hmf_series_defl, 12, 2, 11, 4)\n",
    "\n",
    "            # Convert to daily average flow in cfpd, and take only flow above the threshold\n",
    "            hmf_series_defl = fn.convert_hmf(hmf_series_defl, threshold)\n",
    "            total_hmf_flow = hmf_series_defl[\"00060_Mean\"].sum()\n",
    "            hmf_per_month = fn.monthly_hmf(hmf_series_defl, data_range, quantile)\n",
    "\n",
    "            # Calculate 3 and 6 month HMF\n",
    "            df_six_month = fn.convert_hmf(df_six_month, threshold)\n",
    "            six_month_hmf = df_six_month[\"00060_Mean\"].sum()\n",
    "            df_three_month = fn.convert_hmf(df_three_month, threshold)\n",
    "            three_month_hmf = df_three_month[\"00060_Mean\"].sum()\n",
    "            \n",
    "            total_hmf_flow = (total_hmf_flow * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            six_month_hmf = (six_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            three_month_hmf = (three_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "\n",
    "            # Inter-annual\n",
    "            inter_annual, delta = fn.calc_inter_annual(hmf_series_cont, hmf_years)            \n",
    "\n",
    "            # Average Duration and Intra-annual Frequency\n",
    "            hmf_series_cont = fn.convert_hmf(hmf_series_cont, threshold)\n",
    "            event_duration, annual_duration, intra_annual, event_hmf, df_results = fn.calc_duration_intra_annual(hmf_series_cont, hmf_years)\n",
    "            \n",
    "            # Total duration mk test \n",
    "            dur_series_defl = df_results['total_days'][df_results['total_days'] > 0]\n",
    "            dur_series_cont = df_results['total_days']\n",
    "            df_mk_dur = fn.mann_kendall(dur_series_defl, dur_series_cont, fn.MK_TREND_ALPHA)\n",
    "                        \n",
    "            # Total events per year mk test            \n",
    "            intra_series_defl = df_results['total_events'][df_results['total_events'] > 0]\n",
    "            intra_series_cont = df_results['total_events']\n",
    "            df_mk_intra = fn.mann_kendall(intra_series_defl, intra_series_cont, fn.MK_TREND_ALPHA) \n",
    "            \n",
    "            # Average event HMF mk test\n",
    "            event_mag_series_defl = df_results['event_hmf'][df_results['event_hmf'] > 0]\n",
    "            event_mag_series_cont = df_results['event_hmf']\n",
    "            df_mk_event_mag = fn.mann_kendall(event_mag_series_defl, event_mag_series_cont, fn.MK_TREND_ALPHA)\n",
    "            \n",
    "            # Average event duration mk test\n",
    "            event_dur_series_defl = df_results['duration'][df_results['duration'] > 0]\n",
    "            event_dur_series_cont = df_results['duration']\n",
    "            df_mk_event_dur = fn.mann_kendall(event_dur_series_defl, event_dur_series_cont, fn.MK_TREND_ALPHA)                \n",
    "            \n",
    "            # Timing Calculation using DOHY\n",
    "            timing, com_series = fn.calc_timing(hmf_series_defl)\n",
    "            df_mk_timing = fn.mann_kendall(com_series, com_series, fn.MK_TREND_ALPHA)   \n",
    "                     \n",
    "            \n",
    "            # Merging site dataframe with Mann-Kendall dataframe. This start date is the beginning of the actual data, not necessarily \n",
    "            # the beginning of the analyzed range. Validation (above) starts from the start of the official range (1970/90-2020)\n",
    "            start = df_copy['datetime'].min().date()\n",
    "            end = df_copy['datetime'].max().date()\n",
    "            data = {'dataset_ID': (data_range * quantile), 'site_no': df_copy.iloc[0]['site_no'], 'analyze_start': start, 'analyze_end': end, 'analyze_range': delta, 'quantile': quantile, \n",
    "                    'valid': valid, 'missing_data%': missing, 'threshold': threshold, 'hmf_years': hmf_years, 'annual_hmf': total_hmf_flow, 'six_mo_hmf': six_month_hmf, 'three_mo_hmf': three_month_hmf, \n",
    "                    'annual_duration': annual_duration, 'event_duration': event_duration, 'event_hmf': event_hmf, 'inter_annual%': inter_annual, 'intra_annual': intra_annual, 'timing': timing}              \n",
    "            \n",
    "            # Merging MK results\n",
    "            df_complete_mk_mag = fn.merge_mk_results(df_complete_mk_mag, df_mk_mag, df_copy.iloc[0]['site_no'], data_range, quantile)\n",
    "            df_complete_mk_dur = fn.merge_mk_results(df_complete_mk_dur, df_mk_dur, df_copy.iloc[0]['site_no'], data_range, quantile)\n",
    "            df_complete_mk_intra = fn.merge_mk_results(df_complete_mk_intra, df_mk_intra, df_copy.iloc[0]['site_no'], data_range, quantile)\n",
    "            df_complete_mk_event_mag = fn.merge_mk_results(df_complete_mk_event_mag, df_mk_event_mag, df_copy.iloc[0]['site_no'], data_range, quantile) \n",
    "            df_complete_mk_event_dur = fn.merge_mk_results(df_complete_mk_event_dur, df_mk_event_dur, df_copy.iloc[0]['site_no'], data_range, quantile)\n",
    "            df_complete_mk_timing = fn.merge_mk_results(df_complete_mk_timing, df_mk_timing, df_copy.iloc[0]['site_no'], data_range, quantile)        \n",
    "            \n",
    "            # Merging metric results\n",
    "            df_single_iteration = pd.DataFrame(data, index=['0'])\n",
    "            df_single_iteration = pd.concat([df_single_iteration.reset_index(drop=True), hmf_per_month.reset_index(drop=True)], axis=1)\n",
    "            df_complete_site_data = pd.concat([df_complete_site_data.reset_index(drop=True), df_single_iteration.reset_index(drop=True)], axis=0)\n",
    "        \n",
    "    return df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra, df_complete_mk_event_mag, df_complete_mk_event_dur, df_complete_mk_timing\n",
    "\n",
    "# For testing purposes, to run this cell independently\n",
    "single_site_result = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "\n",
    "'''\n",
    "try:\n",
    "    with pd.ExcelWriter('df_single_site.xlsx') as writer:\n",
    "        df_complete_site_data.to_excel(writer, sheet_name='site_metrics', index=False)\n",
    "        df_complete_mk_mag.to_excel(writer, sheet_name='mk_magnitude', index=False)\n",
    "        df_complete_mk_intra.to_excel(writer, sheet_name='mk_intra', index=False)\n",
    "        df_complete_mk_dur.to_excel(writer, sheet_name='mk_duration', index=False)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "'''\n",
    "   \n",
    "fn.single_site_report(single_site_result[0])\n",
    "df_complete_site_data = fn.gages_2_filtering(single_site_result[0])\n",
    "#fn.save_data(df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra, 'TEST')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Multi-Site Filtering<br>\n",
    "This function creates the list of sites to analyze by filtering the complete list of state sites with 00060_Mean data down to those that lie within a specific watershed boundary using decimal long/lat positional data and a region shapefile (e.g. state or watershed boundary)<br><br>\n",
    "\n",
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed when running the following cell independently\n",
    "shapefile_path = 'ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site List Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sites: 248 in the state of CA in the given WB\n",
      "['09429000', '09429100', '09429155', '09429180', '09429200', '09429500', '09429600', '09521100', '09523000', '09523200', '09523400', '09523600', '09523800', '09524000', '09524700', '09525000', '09526200', '09527590', '09527594', '09527597', '09527630', '09527660', '09527700', '09530000', '09530500', '10251290', '10251300', '10251330', '10251335', '10254050', '10254730', '10254970', '10255550', '10257548', '10257549', '10258000', '10259000', '10259050', '10259100', '10259300', '10259540', '10261500', '10262000', '10262500', '10262700', '10263000', '10263500', '10291500', '10296750', '10336610', '10336660', '10336676', '10337500', '10339410', '10340500', '11345500', '11355010', '11370700', '11374000', '11376000', '11376550', '11517000', '11022480', '11027000', '11042000', '11042400', '11042800', '11042900', '11044000', '11046530', '11047300', '11048175', '11048600', '11057500', '11059300', '11060400', '11062800', '11065000', '11070210', '11070270', '11072100', '11073300', '11073360', '11073495', '11074000', '11077500', '11078000', '11085000', '11087020', '11088500', '11089500', '11092450', '11097000', '11101250', '11102300', '11109000', '11113000', '11118500', '11119500', '11119750', '11119940', '11120000', '11124500', '11128250', '11129800', '11133000', '11134000', '11135800', '11136040', '11136045', '11136710', '11140000', '11140585', '11141050', '11143200', '11143250', '11147500', '11149900', '11151300', '11151700', '11152000', '11152050', '11152300', '11152500', '11152650', '11153000', '11153650', '11157500', '11158600', '11159000', '11159200', '11159490', '11159500', '11159690', '11160000', '11161000', '11162630', '11169025', '11170000', '11172175', '11174000', '11176500', '11176900', '11179100', '11180500', '11180700', '11181008', '11181040', '11449255', '11451300', '11453500', '11456000', '11458000', '11458433', '11458500', '11458600', '11462080', '11462500', '11463500', '11463682', '11463900', '11463980', '11464000', '11465240', '11465350', '11465390', '11465660', '11465680', '11465690', '11465700', '11465750', '11466170', '11466200', '11466320', '11466800', '11467000', '11472180', '11481000', '11481200', '11530500', '373507121472101', '11251000', '11253310', '11255575', '11261100', '11261500', '11262900', '11273400', '11274000', '11274500', '11274550', '11274630', '11289850', '11290000', '11303000', '11303500', '11304810', '11311300', '11312672', '11312676', '11312685', '11312968', '11313240', '11313315', '11313405', '11313431', '11313433', '11313434', '11313440', '11336600', '11336685', '11336790', '11336930', '11337080', '11379500', '11389500', '11390500', '11421000', '11424000', '11425500', '11446500', '11447360', '11447650', '11447830', '11447850', '11447890', '11447903', '11447905', '11451800', '11452500', '11452800', '11452900', '11453000', '11455095', '11455140', '11455280', '11455315', '11455338', '11455385', '351813119150601', '352533118494601', '360013118575201', '375450121331701', '11355500', '11377100', '11381500', '11383500', '11390000']\n"
     ]
    }
   ],
   "source": [
    "def filter_state_site(shapefile_path: str, state_uri: str):\n",
    "    \"\"\"Creates a list of sites with over 50 years of 00060_Mean streamflow data for a given region\"\"\"\n",
    "    # Request page from USGS site, ignore all informational lines\n",
    "    response = requests.get(state_uri)\n",
    "    data = response.text\n",
    "    lines = data.splitlines()\n",
    "    lines = [line for line in lines if not line.startswith('#')]\n",
    "\n",
    "    # Create dataframe where site_no is a list of all sites in a state with 00060 data\n",
    "    tsd = \"\\n\".join(lines)\n",
    "    df = pd.read_csv(StringIO(tsd), sep='\\t')\n",
    "    df_state_sites = df.iloc[1:]\n",
    "        \n",
    "    # Filter out sites outside of HU boundary\n",
    "    if fn.SORT_BY_WB:\n",
    "        shapefile = gpd.read_file(shapefile_path)\n",
    "        df_state_sites['geometry'] = [Point(lon, lat) for lon, lat in zip(df_state_sites['dec_long_va'], df_state_sites['dec_lat_va'])]\n",
    "        gdf_data = gpd.GeoDataFrame(df_state_sites, crs=shapefile.crs)\n",
    "        df_state_sites = gpd.sjoin(gdf_data, shapefile, predicate='within')\n",
    "            \n",
    "    #print(df_state_sites.columns.to_list())\n",
    "    #print(df_state_sites)\n",
    "    \n",
    "    return df_state_sites\n",
    "\n",
    "df_state_sites = filter_state_site(shapefile_path, fn.SITES_URI)\n",
    "print(f'Total Sites: {len(df_state_sites)} in the state of {fn.STATE_CODE.upper()} in the given WB')\n",
    "site_list = df_state_sites['site_no'].to_list()\n",
    "print(site_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: National Metrics Dataset\n",
    "Generates a nationwide dataset for all sites meeting datarange criteria (lengthy runtime!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc2_gdf = gpd.read_file('ShapeFiles/HUC2/_Master_HUC2/master_huc2.shp')\n",
    "huc4_gdf = gpd.read_file('ShapeFiles/HUC4/_Master_HUC4/master_huc4.shp')\n",
    "aq_gdf = gpd.read_file('ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Prelim_Data/_National_Metrics/National_Sites_Blacklist.txt'\n"
     ]
    }
   ],
   "source": [
    "state_limit = 50\n",
    "dataset_name = 'National_Metrics'\n",
    "shapefile_path = 'ShapeFiles/Lower48/lower48.shp'\n",
    "\n",
    "# States with few sites for testing purposes\n",
    "test_state_list = ['ME', 'DE']\n",
    "\n",
    "# Set to False to ignore the blacklist and analyze all sites. This could be occasionally\n",
    "# worth doing as USGS could update site data making it valid in the future\n",
    "allow_blacklist = False\n",
    "\n",
    "curr_blacklist = []\n",
    "try:\n",
    "    with open('Prelim_Data/_National_Metrics/National_Sites_Blacklist.txt', 'r') as file:\n",
    "        curr_blacklist = file.read().split(', ')\n",
    "        curr_blacklist = [x.replace(\"'\", \"\") for x in curr_blacklist]\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on CA---]\n",
      "Total Sites: 484 in the state of CA\n",
      "Working on CA site 1/484 (09423350)\n",
      "Working on CA site 2/484 (09429000)\n",
      "Working on CA site 3/484 (09429100)\n",
      "Working on CA site 4/484 (09429155)\n",
      "Working on CA site 5/484 (09429180)\n",
      "Working on CA site 6/484 (09429200)\n",
      "Working on CA site 7/484 (09429500)\n",
      "Working on CA site 8/484 (09429600)\n",
      "Working on CA site 9/484 (09521100)\n",
      "Working on CA site 10/484 (09523000)\n",
      "Working on CA site 11/484 (09523200)\n",
      "Working on CA site 12/484 (09523400)\n",
      "Working on CA site 13/484 (09523600)\n",
      "Working on CA site 14/484 (09523800)\n",
      "Working on CA site 15/484 (09524000)\n",
      "Working on CA site 16/484 (09524700)\n",
      "Working on CA site 17/484 (09525000)\n",
      "Working on CA site 18/484 (09526200)\n",
      "Working on CA site 19/484 (09527590)\n",
      "Working on CA site 20/484 (09527594)\n",
      "Working on CA site 21/484 (09527597)\n",
      "Working on CA site 22/484 (09527630)\n",
      "Working on CA site 23/484 (09527660)\n",
      "Working on CA site 24/484 (09527700)\n",
      "Working on CA site 25/484 (09530000)\n",
      "Working on CA site 26/484 (09530500)\n",
      "Working on CA site 27/484 (10251290)\n",
      "Working on CA site 28/484 (10251300)\n",
      "Working on CA site 29/484 (10251330)\n",
      "Working on CA site 30/484 (10251335)\n",
      "Working on CA site 31/484 (10254050)\n",
      "Working on CA site 32/484 (10254730)\n",
      "Working on CA site 33/484 (10254970)\n",
      "Working on CA site 34/484 (10255550)\n",
      "Working on CA site 35/484 (10256500)\n",
      "Working on CA site 36/484 (10257500)\n",
      "Working on CA site 37/484 (10257548)\n",
      "Working on CA site 38/484 (10257549)\n",
      "Working on CA site 39/484 (10257600)\n",
      "Working on CA site 40/484 (10257720)\n",
      "Working on CA site 41/484 (10258000)\n",
      "Working on CA site 42/484 (10258500)\n",
      "Working on CA site 43/484 (10258700)\n",
      "Working on CA site 44/484 (10259000)\n",
      "Working on CA site 45/484 (10259050)\n",
      "Working on CA site 46/484 (10259100)\n",
      "Working on CA site 47/484 (10259200)\n",
      "Working on CA site 48/484 (10259300)\n",
      "Working on CA site 49/484 (10259540)\n",
      "Working on CA site 50/484 (10260500)\n",
      "Working on CA site 51/484 (10260855)\n",
      "Working on CA site 52/484 (10260950)\n",
      "Working on CA site 53/484 (10261500)\n",
      "Working on CA site 54/484 (10262000)\n",
      "ERROR: Single site data failure for site 10262000:\n",
      "float division by zero\n",
      "Working on CA site 55/484 (10262500)\n",
      "Working on CA site 56/484 (10262700)\n",
      "IGNORED: No data for site 10262700\n",
      "Working on CA site 57/484 (10263000)\n",
      "Working on CA site 58/484 (10263500)\n",
      "Working on CA site 59/484 (10289000)\n",
      "Working on CA site 60/484 (10289500)\n",
      "Working on CA site 61/484 (10290500)\n",
      "Working on CA site 62/484 (10291500)\n",
      "Working on CA site 63/484 (10293000)\n",
      "Working on CA site 64/484 (10296000)\n",
      "Working on CA site 65/484 (10296500)\n",
      "Working on CA site 66/484 (10296750)\n",
      "Working on CA site 67/484 (10308200)\n",
      "Working on CA site 68/484 (10308783)\n",
      "Working on CA site 69/484 (103087835)\n",
      "Working on CA site 70/484 (10308784)\n",
      "Working on CA site 71/484 (10308785)\n",
      "Working on CA site 72/484 (103087855)\n",
      "Working on CA site 73/484 (103087865)\n",
      "Working on CA site 74/484 (103087889)\n",
      "Working on CA site 75/484 (10308789)\n",
      "Working on CA site 76/484 (10308792)\n",
      "Working on CA site 77/484 (10308794)\n",
      "Working on CA site 78/484 (10310000)\n",
      "Working on CA site 79/484 (103366092)\n",
      "Working on CA site 80/484 (10336610)\n",
      "Working on CA site 81/484 (10336645)\n",
      "Working on CA site 82/484 (10336660)\n",
      "Working on CA site 83/484 (10336676)\n",
      "Working on CA site 84/484 (10336780)\n",
      "Working on CA site 85/484 (10337500)\n",
      "Working on CA site 86/484 (10338000)\n",
      "Working on CA site 87/484 (10338500)\n",
      "Working on CA site 88/484 (10338700)\n",
      "Working on CA site 89/484 (10339410)\n",
      "Working on CA site 90/484 (10340500)\n",
      "Working on CA site 91/484 (10343000)\n",
      "Working on CA site 92/484 (10343500)\n",
      "Working on CA site 93/484 (10344400)\n",
      "Working on CA site 94/484 (10344500)\n",
      "Working on CA site 95/484 (10344505)\n",
      "Working on CA site 96/484 (10346000)\n",
      "Working on CA site 97/484 (11012500)\n",
      "Working on CA site 98/484 (11014000)\n",
      "Working on CA site 99/484 (11015000)\n",
      "Working on CA site 100/484 (11016200)\n",
      "Working on CA site 101/484 (11022200)\n",
      "Working on CA site 102/484 (11022480)\n",
      "Working on CA site 103/484 (11023000)\n",
      "Working on CA site 104/484 (11023340)\n",
      "Working on CA site 105/484 (11025500)\n",
      "Working on CA site 106/484 (11027000)\n",
      "Working on CA site 107/484 (11028500)\n",
      "Working on CA site 108/484 (11042000)\n",
      "Working on CA site 109/484 (11042400)\n",
      "Working on CA site 110/484 (11042800)\n",
      "Working on CA site 111/484 (11042900)\n",
      "Working on CA site 112/484 (11044000)\n",
      "Working on CA site 113/484 (11044250)\n",
      "Working on CA site 114/484 (11044300)\n",
      "Working on CA site 115/484 (11044350)\n",
      "Working on CA site 116/484 (11044800)\n",
      "Working on CA site 117/484 (11045300)\n",
      "Working on CA site 118/484 (11045600)\n",
      "Working on CA site 119/484 (11045700)\n",
      "Working on CA site 120/484 (11046000)\n",
      "Working on CA site 121/484 (11046300)\n",
      "Working on CA site 122/484 (11046360)\n",
      "Working on CA site 123/484 (11046530)\n",
      "Working on CA site 124/484 (11047300)\n",
      "Working on CA site 125/484 (11048175)\n",
      "Working on CA site 126/484 (11048600)\n",
      "Working on CA site 127/484 (11051499)\n",
      "Working on CA site 128/484 (11051502)\n",
      "Working on CA site 129/484 (11055500)\n",
      "Working on CA site 130/484 (11055800)\n",
      "Working on CA site 131/484 (11057500)\n",
      "Working on CA site 132/484 (11058500)\n",
      "Working on CA site 133/484 (11058600)\n",
      "Working on CA site 134/484 (11059300)\n",
      "Working on CA site 135/484 (11060400)\n",
      "Working on CA site 136/484 (11062000)\n",
      "Working on CA site 137/484 (11062399)\n",
      "Working on CA site 138/484 (11062400)\n",
      "Working on CA site 139/484 (11062450)\n",
      "Working on CA site 140/484 (11062800)\n",
      "Working on CA site 141/484 (11063510)\n",
      "Working on CA site 142/484 (11063680)\n",
      "Working on CA site 143/484 (11065000)\n",
      "Working on CA site 144/484 (11069500)\n",
      "Working on CA site 145/484 (11070210)\n",
      "Working on CA site 146/484 (11070270)\n",
      "Working on CA site 147/484 (11070365)\n",
      "Working on CA site 148/484 (11070465)\n",
      "Working on CA site 149/484 (11070500)\n",
      "Working on CA site 150/484 (11071760)\n",
      "Working on CA site 151/484 (11071900)\n",
      "Working on CA site 152/484 (11072100)\n",
      "Working on CA site 153/484 (11073300)\n",
      "Working on CA site 154/484 (11073360)\n",
      "Working on CA site 155/484 (11073495)\n",
      "Working on CA site 156/484 (11074000)\n",
      "Working on CA site 157/484 (11075720)\n",
      "Working on CA site 158/484 (11077500)\n",
      "Working on CA site 159/484 (11078000)\n",
      "Working on CA site 160/484 (11085000)\n",
      "Working on CA site 161/484 (11087020)\n",
      "Working on CA site 162/484 (11088500)\n",
      "Working on CA site 163/484 (11089500)\n",
      "Working on CA site 164/484 (11092450)\n",
      "Working on CA site 165/484 (11097000)\n",
      "Working on CA site 166/484 (11098000)\n",
      "Working on CA site 167/484 (11101250)\n",
      "Working on CA site 168/484 (11102300)\n",
      "Working on CA site 169/484 (11109000)\n",
      "Working on CA site 170/484 (11109600)\n",
      "Working on CA site 171/484 (11109800)\n",
      "Working on CA site 172/484 (11111500)\n",
      "Working on CA site 173/484 (11113000)\n",
      "Working on CA site 174/484 (11113500)\n",
      "Working on CA site 175/484 (11114495)\n",
      "Working on CA site 176/484 (11118500)\n",
      "Working on CA site 177/484 (11119500)\n",
      "Working on CA site 178/484 (11119750)\n",
      "Working on CA site 179/484 (11119940)\n",
      "Working on CA site 180/484 (11120000)\n",
      "Working on CA site 181/484 (11120500)\n",
      "Working on CA site 182/484 (11120520)\n",
      "Working on CA site 183/484 (11121900)\n",
      "Working on CA site 184/484 (11122010)\n",
      "Working on CA site 185/484 (11123000)\n",
      "Working on CA site 186/484 (11123500)\n",
      "Working on CA site 187/484 (11124500)\n",
      "Working on CA site 188/484 (11125605)\n",
      "Working on CA site 189/484 (11126400)\n",
      "IGNORED: No data for site 11126400\n",
      "Working on CA site 190/484 (11128250)\n",
      "Working on CA site 191/484 (11128500)\n",
      "Working on CA site 192/484 (11129800)\n",
      "Working on CA site 193/484 (11132500)\n",
      "Working on CA site 194/484 (11133000)\n",
      "Working on CA site 195/484 (11134000)\n",
      "Working on CA site 196/484 (11135800)\n",
      "Working on CA site 197/484 (11136040)\n",
      "Working on CA site 198/484 (11136045)\n",
      "IGNORED: No data for site 11136045\n",
      "Working on CA site 199/484 (11136100)\n",
      "Working on CA site 200/484 (11136500)\n",
      "Working on CA site 201/484 (11136600)\n",
      "Working on CA site 202/484 (11136710)\n",
      "IGNORED: No data for site 11136710\n",
      "Working on CA site 203/484 (11136800)\n",
      "Working on CA site 204/484 (11137900)\n",
      "Working on CA site 205/484 (11138500)\n",
      "Working on CA site 206/484 (11140000)\n",
      "Working on CA site 207/484 (11140585)\n",
      "Working on CA site 208/484 (11141050)\n",
      "Working on CA site 209/484 (11141280)\n",
      "Working on CA site 210/484 (11143000)\n",
      "Working on CA site 211/484 (11143200)\n",
      "Working on CA site 212/484 (11143250)\n",
      "Working on CA site 213/484 (11147070)\n",
      "Working on CA site 214/484 (11147098)\n",
      "IGNORED: No data for site 11147098\n",
      "Working on CA site 215/484 (11147500)\n",
      "Working on CA site 216/484 (11148900)\n",
      "Working on CA site 217/484 (11149400)\n",
      "Working on CA site 218/484 (11149900)\n",
      "Working on CA site 219/484 (11150500)\n",
      "Working on CA site 220/484 (11151300)\n",
      "Working on CA site 221/484 (11151700)\n",
      "Working on CA site 222/484 (11151870)\n",
      "ERROR: Single site data failure for site 11151870:\n",
      "float division by zero\n",
      "Working on CA site 223/484 (11152000)\n",
      "Working on CA site 224/484 (11152050)\n",
      "Working on CA site 225/484 (11152300)\n",
      "Working on CA site 226/484 (11152500)\n",
      "Working on CA site 227/484 (11152650)\n",
      "Working on CA site 228/484 (11153000)\n",
      "Working on CA site 229/484 (11153650)\n",
      "Working on CA site 230/484 (11156500)\n",
      "Working on CA site 231/484 (11157500)\n",
      "Working on CA site 232/484 (11158600)\n",
      "Working on CA site 233/484 (11159000)\n",
      "Working on CA site 234/484 (11159200)\n",
      "Working on CA site 235/484 (11159490)\n",
      "IGNORED: No data for site 11159490\n",
      "Working on CA site 236/484 (11159500)\n",
      "ERROR: Single site data failure for site 11159500:\n",
      "float division by zero\n",
      "Working on CA site 237/484 (11159690)\n",
      "ERROR: Single site data failure for site 11159690:\n",
      "float division by zero\n",
      "Working on CA site 238/484 (11160000)\n",
      "Working on CA site 239/484 (11160500)\n",
      "Working on CA site 240/484 (11161000)\n",
      "Working on CA site 241/484 (11162500)\n",
      "Working on CA site 242/484 (11162540)\n",
      "ERROR: Single site data failure for site 11162540:\n",
      "float division by zero\n",
      "Working on CA site 243/484 (11162570)\n",
      "Working on CA site 244/484 (111626182)\n",
      "IGNORED: No data for site 111626182\n",
      "Working on CA site 245/484 (11162619)\n",
      "IGNORED: No data for site 11162619\n",
      "Working on CA site 246/484 (11162620)\n",
      "Working on CA site 247/484 (11162630)\n",
      "Working on CA site 248/484 (11162735)\n",
      "IGNORED: No data for site 11162735\n",
      "Working on CA site 249/484 (11162737)\n",
      "IGNORED: No data for site 11162737\n",
      "Working on CA site 250/484 (11162753)\n",
      "Working on CA site 251/484 (11164500)\n",
      "Working on CA site 252/484 (11169025)\n",
      "Working on CA site 253/484 (11169500)\n",
      "Working on CA site 254/484 (11169800)\n",
      "Working on CA site 255/484 (11169860)\n",
      "ERROR: Single site data failure for site 11169860:\n",
      "float division by zero\n",
      "Working on CA site 256/484 (11170000)\n",
      "ERROR: Single site data failure for site 11170000:\n",
      "'00060_Mean'\n",
      "Working on CA site 257/484 (11172175)\n",
      "Working on CA site 258/484 (11172945)\n",
      "Working on CA site 259/484 (11172955)\n",
      "Working on CA site 260/484 (11173200)\n",
      "Working on CA site 261/484 (11173500)\n",
      "Working on CA site 262/484 (11173510)\n",
      "Working on CA site 263/484 (11173575)\n",
      "Working on CA site 264/484 (11173800)\n",
      "Working on CA site 265/484 (11174000)\n",
      "Working on CA site 266/484 (11174160)\n",
      "IGNORED: No data for site 11174160\n",
      "Working on CA site 267/484 (11174600)\n",
      "Working on CA site 268/484 (11176340)\n",
      "IGNORED: No data for site 11176340\n",
      "Working on CA site 269/484 (11176400)\n",
      "Working on CA site 270/484 (11176500)\n",
      "Working on CA site 271/484 (11176900)\n",
      "Working on CA site 272/484 (11179000)\n",
      "Working on CA site 273/484 (11179100)\n",
      "Working on CA site 274/484 (11180500)\n",
      "Working on CA site 275/484 (11180700)\n",
      "Working on CA site 276/484 (11180825)\n",
      "Working on CA site 277/484 (11180900)\n",
      "Working on CA site 278/484 (11180960)\n",
      "Working on CA site 279/484 (11181000)\n",
      "Working on CA site 280/484 (11181008)\n",
      "Working on CA site 281/484 (11181040)\n",
      "Working on CA site 282/484 (11182500)\n",
      "Working on CA site 283/484 (11189500)\n",
      "Working on CA site 284/484 (11200800)\n",
      "Working on CA site 285/484 (11203580)\n",
      "Working on CA site 286/484 (11204100)\n",
      "Working on CA site 287/484 (11206820)\n",
      "Working on CA site 288/484 (11224000)\n",
      "Working on CA site 289/484 (11224500)\n",
      "Working on CA site 290/484 (11251000)\n",
      "Working on CA site 291/484 (11253310)\n",
      "Working on CA site 292/484 (11255575)\n",
      "Working on CA site 293/484 (11261100)\n",
      "Working on CA site 294/484 (11261500)\n",
      "Working on CA site 295/484 (11262900)\n",
      "Working on CA site 296/484 (11264500)\n",
      "Working on CA site 297/484 (11266500)\n",
      "Working on CA site 298/484 (11273400)\n",
      "Working on CA site 299/484 (11274000)\n",
      "Working on CA site 300/484 (11274500)\n",
      "Working on CA site 301/484 (11274550)\n",
      "Working on CA site 302/484 (11274630)\n",
      "Working on CA site 303/484 (11274790)\n",
      "IGNORED: No data for site 11274790\n",
      "Working on CA site 304/484 (11276500)\n",
      "Working on CA site 305/484 (11276600)\n",
      "Working on CA site 306/484 (11276900)\n",
      "Working on CA site 307/484 (11277300)\n",
      "Working on CA site 308/484 (11278000)\n",
      "Working on CA site 309/484 (11278300)\n",
      "Working on CA site 310/484 (11278400)\n",
      "Working on CA site 311/484 (11284400)\n",
      "Working on CA site 312/484 (11289000)\n",
      "Working on CA site 313/484 (11289500)\n",
      "Working on CA site 314/484 (11289650)\n",
      "Working on CA site 315/484 (11289850)\n",
      "ERROR: Single site data failure for site 11289850:\n",
      "float division by zero\n",
      "Working on CA site 316/484 (11290000)\n",
      "Working on CA site 317/484 (11299600)\n",
      "Working on CA site 318/484 (11303000)\n",
      "Working on CA site 319/484 (11303500)\n",
      "Working on CA site 320/484 (11304810)\n",
      "Working on CA site 321/484 (11311300)\n",
      "Working on CA site 322/484 (11312672)\n",
      "Working on CA site 323/484 (11312676)\n",
      "Working on CA site 324/484 (11312685)\n",
      "Working on CA site 325/484 (11312968)\n",
      "Working on CA site 326/484 (11313240)\n",
      "Working on CA site 327/484 (11313315)\n",
      "Working on CA site 328/484 (11313405)\n",
      "Working on CA site 329/484 (11313431)\n",
      "Working on CA site 330/484 (11313433)\n",
      "Working on CA site 331/484 (11313434)\n",
      "Working on CA site 332/484 (11313440)\n",
      "Working on CA site 333/484 (11313452)\n",
      "Working on CA site 334/484 (11313460)\n",
      "Working on CA site 335/484 (11335000)\n",
      "Working on CA site 336/484 (11336600)\n",
      "Working on CA site 337/484 (11336685)\n",
      "Working on CA site 338/484 (11336790)\n",
      "Working on CA site 339/484 (11336930)\n",
      "Working on CA site 340/484 (11336955)\n",
      "IGNORED: No data for site 11336955\n",
      "Working on CA site 341/484 (11337080)\n",
      "Working on CA site 342/484 (11337190)\n",
      "Working on CA site 343/484 (11342000)\n",
      "Working on CA site 344/484 (11345500)\n",
      "Working on CA site 345/484 (11348500)\n",
      "Working on CA site 346/484 (11355010)\n",
      "Working on CA site 347/484 (11355500)\n",
      "Working on CA site 348/484 (11370500)\n",
      "Working on CA site 349/484 (11370700)\n",
      "Working on CA site 350/484 (11372000)\n",
      "Working on CA site 351/484 (11374000)\n",
      "Working on CA site 352/484 (11376000)\n",
      "Working on CA site 353/484 (11376550)\n",
      "Working on CA site 354/484 (11377100)\n",
      "Working on CA site 355/484 (11379500)\n",
      "Working on CA site 356/484 (11381500)\n",
      "Working on CA site 357/484 (11383500)\n",
      "Working on CA site 358/484 (11389500)\n",
      "Working on CA site 359/484 (11390000)\n",
      "Working on CA site 360/484 (11390500)\n",
      "Working on CA site 361/484 (11401920)\n",
      "Working on CA site 362/484 (11402000)\n",
      "Working on CA site 363/484 (11413000)\n",
      "Working on CA site 364/484 (11418500)\n",
      "Working on CA site 365/484 (11421000)\n",
      "Working on CA site 366/484 (11424000)\n",
      "Working on CA site 367/484 (11425500)\n",
      "Working on CA site 368/484 (11427000)\n",
      "Working on CA site 369/484 (11446500)\n",
      "Working on CA site 370/484 (11447360)\n",
      "Working on CA site 371/484 (11447650)\n",
      "Working on CA site 372/484 (11447830)\n",
      "Working on CA site 373/484 (11447850)\n",
      "Working on CA site 374/484 (11447890)\n",
      "Working on CA site 375/484 (11447903)\n",
      "Working on CA site 376/484 (11447905)\n",
      "Working on CA site 377/484 (11448750)\n",
      "IGNORED: No data for site 11448750\n",
      "Working on CA site 378/484 (11448800)\n",
      "IGNORED: No data for site 11448800\n",
      "Working on CA site 379/484 (11449255)\n",
      "IGNORED: No data for site 11449255\n",
      "Working on CA site 380/484 (11449500)\n",
      "Working on CA site 381/484 (11451000)\n",
      "Working on CA site 382/484 (11451100)\n",
      "Working on CA site 383/484 (11451300)\n",
      "Working on CA site 384/484 (11451715)\n",
      "Working on CA site 385/484 (11451800)\n",
      "Working on CA site 386/484 (11452500)\n",
      "Working on CA site 387/484 (11452800)\n",
      "Working on CA site 388/484 (11452900)\n",
      "Working on CA site 389/484 (11453000)\n",
      "Working on CA site 390/484 (11453500)\n",
      "Working on CA site 391/484 (11453590)\n",
      "IGNORED: No data for site 11453590\n",
      "Working on CA site 392/484 (11454000)\n",
      "Working on CA site 393/484 (11455095)\n",
      "IGNORED: No data for site 11455095\n",
      "Working on CA site 394/484 (11455140)\n",
      "Working on CA site 395/484 (11455280)\n",
      "IGNORED: No data for site 11455280\n",
      "Working on CA site 396/484 (11455315)\n",
      "ERROR: Single site data failure for site 11455315:\n",
      "float division by zero\n",
      "Working on CA site 397/484 (11455338)\n",
      "IGNORED: No data for site 11455338\n",
      "Working on CA site 398/484 (11455385)\n",
      "ERROR: Single site data failure for site 11455385:\n",
      "float division by zero\n",
      "Working on CA site 399/484 (11455420)\n",
      "Working on CA site 400/484 (11456000)\n",
      "Working on CA site 401/484 (11458000)\n",
      "Working on CA site 402/484 (11458433)\n",
      "Working on CA site 403/484 (11458500)\n",
      "Working on CA site 404/484 (11458600)\n",
      "Working on CA site 405/484 (11459500)\n",
      "Working on CA site 406/484 (11460000)\n",
      "Working on CA site 407/484 (11460151)\n",
      "Working on CA site 408/484 (11460400)\n",
      "Working on CA site 409/484 (11460600)\n",
      "Working on CA site 410/484 (11460605)\n",
      "Working on CA site 411/484 (11460750)\n",
      "Working on CA site 412/484 (11461500)\n",
      "Working on CA site 413/484 (11462080)\n",
      "Working on CA site 414/484 (11462500)\n",
      "Working on CA site 415/484 (11463000)\n",
      "Working on CA site 416/484 (11463170)\n",
      "Working on CA site 417/484 (11463200)\n",
      "Working on CA site 418/484 (11463500)\n",
      "Working on CA site 419/484 (11463682)\n",
      "Working on CA site 420/484 (11463900)\n",
      "Working on CA site 421/484 (11463980)\n",
      "Working on CA site 422/484 (11464000)\n",
      "Working on CA site 423/484 (11465240)\n",
      "Working on CA site 424/484 (11465350)\n",
      "Working on CA site 425/484 (11465390)\n",
      "Working on CA site 426/484 (11465660)\n",
      "Working on CA site 427/484 (11465680)\n",
      "Working on CA site 428/484 (11465690)\n",
      "Working on CA site 429/484 (11465700)\n",
      "Working on CA site 430/484 (11465750)\n",
      "Working on CA site 431/484 (11466170)\n",
      "Working on CA site 432/484 (11466200)\n",
      "Working on CA site 433/484 (11466320)\n",
      "Working on CA site 434/484 (11466800)\n",
      "Working on CA site 435/484 (11467000)\n",
      "Working on CA site 436/484 (11467200)\n",
      "Working on CA site 437/484 (11467510)\n",
      "Working on CA site 438/484 (11467553)\n",
      "Working on CA site 439/484 (11468000)\n",
      "Working on CA site 440/484 (11468500)\n",
      "Working on CA site 441/484 (11468900)\n",
      "Working on CA site 442/484 (11469000)\n",
      "Working on CA site 443/484 (11472180)\n",
      "Working on CA site 444/484 (11473900)\n",
      "Working on CA site 445/484 (11475000)\n",
      "Working on CA site 446/484 (11475560)\n",
      "Working on CA site 447/484 (11475800)\n",
      "Working on CA site 448/484 (11476500)\n",
      "Working on CA site 449/484 (11476600)\n",
      "Working on CA site 450/484 (11477000)\n",
      "Working on CA site 451/484 (11478500)\n",
      "Working on CA site 452/484 (11480390)\n",
      "Working on CA site 453/484 (11481000)\n",
      "Working on CA site 454/484 (11481200)\n",
      "Working on CA site 455/484 (11481500)\n",
      "Working on CA site 456/484 (11482500)\n",
      "Working on CA site 457/484 (11516530)\n",
      "Working on CA site 458/484 (11517000)\n",
      "Working on CA site 459/484 (11517500)\n",
      "Working on CA site 460/484 (11519500)\n",
      "Working on CA site 461/484 (11520500)\n",
      "Working on CA site 462/484 (11521500)\n",
      "Working on CA site 463/484 (11522500)\n",
      "Working on CA site 464/484 (11523000)\n",
      "Working on CA site 465/484 (11523200)\n",
      "Working on CA site 466/484 (11525500)\n",
      "Working on CA site 467/484 (11525530)\n",
      "Working on CA site 468/484 (11525655)\n",
      "Working on CA site 469/484 (11525670)\n",
      "Working on CA site 470/484 (11525854)\n",
      "Working on CA site 471/484 (11526250)\n",
      "Working on CA site 472/484 (11526400)\n",
      "Working on CA site 473/484 (11527000)\n",
      "Working on CA site 474/484 (11528700)\n",
      "Working on CA site 475/484 (11530000)\n",
      "Working on CA site 476/484 (11530500)\n",
      "Working on CA site 477/484 (11532500)\n",
      "Working on CA site 478/484 (351813119150601)\n",
      "IGNORED: No data for site 351813119150601\n",
      "Working on CA site 479/484 (352533118494601)\n",
      "IGNORED: No data for site 352533118494601\n",
      "Working on CA site 480/484 (360013118575201)\n",
      "IGNORED: No data for site 360013118575201\n",
      "Working on CA site 481/484 (373507121472101)\n",
      "Working on CA site 482/484 (375450121331701)\n",
      "IGNORED: No data for site 375450121331701\n",
      "Working on CA site 483/484 (10336698)\n",
      "Working on CA site 484/484 (10336700)\n"
     ]
    }
   ],
   "source": [
    "df_natl_metrics = pd.DataFrame()\n",
    "df_natl_mk_mag = pd.DataFrame()\n",
    "df_natl_mk_dur = pd.DataFrame()\n",
    "df_natl_mk_intra = pd.DataFrame()\n",
    "df_natl_mk_event_mag = pd.DataFrame()\n",
    "df_natl_mk_event_dur = pd.DataFrame()\n",
    "df_natl_mk_timing = pd.DataFrame()\n",
    "\n",
    "#natl_blacklist = []\n",
    "\n",
    "# Use fn.STATE_LIST for full dataset generation\n",
    "for state_index, state in enumerate(fn.STATE_LIST):\n",
    "    if state_index >= state_limit: break\n",
    "    \n",
    "    print(f'[---Working on {state}---]')\n",
    "    state_uri = fn.create_state_uri(state, fn.PARAM_CODE)\n",
    "    df_state_sites = filter_state_site(shapefile_path, state_uri)\n",
    "    df_state_sites = df_state_sites.reset_index()\n",
    "    print(f'Total Sites: {len(df_state_sites)} in the state of {state}')\n",
    "    \n",
    "    # Modified version of the create_multi_site_data() function\n",
    "    for site_index, row in df_state_sites.iterrows():\n",
    "        \n",
    "        #if allow_blacklist and str(row['site_no']) in curr_blacklist:\n",
    "            #print(f'IGNORED: Site {row[\"site_no\"]} is in aquifer blacklist')\n",
    "            #continue\n",
    "        \n",
    "        df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=[fn.PARAM_CODE, fn.TIDAL_CODE], start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "        df = df.reset_index()\n",
    "        print(f'Working on {state} site {site_index + 1}/{len(df_state_sites)} ({row[\"site_no\"]})')\n",
    "        \n",
    "        # No data at all\n",
    "        if df.empty:\n",
    "            #natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: No data for site {row[\"site_no\"]}')\n",
    "            continue\n",
    "        \n",
    "        # Important for one outlet gauge in TGC aquifer (and maybe others)\n",
    "        if '00060_radar sensor_Mean' in df.columns and '00060_Mean' not in df.columns:\n",
    "            df.rename(columns={'00060_radar sensor_Mean': '00060_Mean'}, inplace=True)\n",
    "        \n",
    "        # Merge tidal data before beginning analysis\n",
    "        df = fn.merge_tidal(df)\n",
    "        \n",
    "        start = df['datetime'].min().date()\n",
    "        end = df['datetime'].max().date()\n",
    "        range = round((end - start).days / 365.25, 1)\n",
    "        \n",
    "        # Removed to create datasets with all 00060_Mean data regardless of available range\n",
    "        '''if range < fn.MIN_DATA_PERIOD and allow_blacklist:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: Not enough data for site {row[\"site_no\"]}')\n",
    "            continue'''\n",
    "        \n",
    "        point = Point(row['dec_long_va'], row['dec_lat_va'])\n",
    "        #state_code = row['station_nm'].strip()[-2:]\n",
    "        \n",
    "        huc2 = huc4 = aquifer = 'NA'\n",
    "        # HUC2 assignment\n",
    "        for i, geo_row in huc2_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                huc2 = geo_row['huc2_code']\n",
    "                continue\n",
    "            \n",
    "        # HUC4 assignment\n",
    "        for i, geo_row in huc4_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                huc4 = geo_row['huc4_code']\n",
    "                continue\n",
    "            \n",
    "        # Aquifer assignment    \n",
    "        for i, geo_row in aq_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                aquifer = geo_row['aq_name']\n",
    "                continue        \n",
    "        \n",
    "        # A few very broken sites with almost no data can have 0 hmf years and cause errors (i.e. '03592000')\n",
    "        # so we'll catch these and add them to the site blacklist\n",
    "        try:\n",
    "            df_single_site_metric, df_mk_mag, df_mk_dur, df_mk_intra, df_mk_event_mag, df_mk_event_dur, df_mk_timing = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "            add_data = {'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va'], 'data_start': start, 'data_end': end, 'total_record': range, \n",
    "                        'state': state, 'huc2_code': huc2, 'huc4_code': huc4, 'within_aq': aquifer}\t\t\t\n",
    "            add_data = pd.DataFrame(add_data, index=['0'])\n",
    "        except Exception as e:\n",
    "            #natl_blacklist.append(row['site_no'])\n",
    "            print(f\"ERROR: Single site data failure for site {row['site_no']}:\\n{e}\")\n",
    "            continue\n",
    "        \n",
    "        add_data = pd.DataFrame(np.tile(add_data.values, (len(df_single_site_metric), 1)), columns=add_data.columns)\n",
    "        df_single_site_metric = pd.concat([df_single_site_metric.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_mag = pd.concat([df_mk_mag.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_dur = pd.concat([df_mk_dur.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_intra = pd.concat([df_mk_intra.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_event_mag = pd.concat([df_mk_event_mag.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_event_dur = pd.concat([df_mk_event_dur.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_timing = pd.concat([df_mk_timing.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "        # Append single site data to multi-site dataframes\n",
    "        df_natl_metrics = pd.concat([df_natl_metrics, df_single_site_metric], ignore_index=True)\n",
    "        df_natl_mk_mag = pd.concat([df_natl_mk_mag, df_mk_mag], ignore_index=True)\n",
    "        df_natl_mk_dur = pd.concat([df_natl_mk_dur, df_mk_dur], ignore_index=True)\n",
    "        df_natl_mk_intra = pd.concat([df_natl_mk_intra, df_mk_intra], ignore_index=True)\n",
    "        df_natl_mk_event_mag = pd.concat([df_natl_mk_event_mag, df_mk_event_mag], ignore_index=True)\n",
    "        df_natl_mk_event_dur = pd.concat([df_natl_mk_event_dur, df_mk_event_dur], ignore_index=True)\n",
    "        df_natl_mk_timing = pd.concat([df_natl_mk_timing, df_mk_timing], ignore_index=True)\n",
    "        \n",
    "df_natl_metrics = fn.gages_2_filtering(df_natl_metrics)\n",
    "\n",
    "# Create national site blacklist\n",
    "'''try:\n",
    "    with open(f'Prelim_Data/National_Sites_Blacklist.txt', 'w') as f:\n",
    "        natl_blacklist = [\"'\" + str(x) + \"'\" for x in natl_blacklist]        \n",
    "        f.write(', '.join(natl_blacklist))\n",
    "except Exception as e:\n",
    "    print(e)'''\n",
    "\n",
    "try:    \n",
    "    fn.save_data(df_natl_metrics, df_natl_mk_mag, df_natl_mk_dur, df_natl_mk_intra, df_natl_mk_event_mag, df_natl_mk_event_dur, df_natl_mk_timing, dataset_name)\n",
    "except Exception as e:\n",
    "    print(f'Error saving data: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: Outlet Gauges Dataset\n",
    "Uses <code>outlet_gauges.xlsx</code> to generate a custom dataset with only outlet gauges present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlets = pd.read_excel('Prelim_Data/_Outlet_Gauges/outlet_gauges.xlsx', dtype={'site_no': str})\n",
    "test_limit = 999\n",
    "#print(df_outlets.duplicated(subset=['site_no']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on site 09519800---]\n",
      "[---Working on site 09521100---]\n",
      "[---Working on site 11303500---]\n",
      "[---Working on site 11447650---]\n",
      "[---Working on site 14105700---]\n",
      "[---Working on site 07316000---]\n",
      "[---Working on site 02330000---]\n",
      "[---Working on site 02320500---]\n",
      "[---Working on site 02236000---]\n",
      "[---Working on site 02296750---]\n",
      "[---Working on site 02292900---]\n",
      "[---Working on site 02198500---]\n",
      "[---Working on site 02169500---]\n",
      "[---Working on site 02129000---]\n",
      "[---Working on site 13269000---]\n",
      "[---Working on site 08114000---]\n",
      "[---Working on site 08033500---]\n",
      "[---Working on site 08066500---]\n",
      "[---Working on site 08068000---]\n",
      "[---Working on site 08116650---]\n",
      "[---Working on site 08162000---]\n",
      "[---Working on site 08176500---]\n",
      "[---Working on site 08211000---]\n",
      "[---Working on site 08164000---]\n",
      "[---Working on site 08188500---]\n",
      "[---Working on site 08030500---]\n",
      "[---Working on site 08013500---]\n",
      "[---Working on site 07378500---]\n",
      "[---Working on site 07375000---]\n",
      "[---Working on site 02489500---]\n",
      "[---Working on site 02479000---]\n",
      "[---Working on site 02469761---]\n",
      "[---Working on site 07029500---]\n",
      "[---Working on site 07077000---]\n",
      "[---Working on site 07268000---]\n",
      "[---Working on site 07362000---]\n",
      "[---Working on site 07369000---]\n",
      "[---Working on site 07290000---]\n",
      "[---Working on site 07074500---]\n",
      "[---Working on site 07337000---]\n",
      "[---Working on site 09180500---]\n",
      "[---Working on site 09315000---]\n",
      "[---Working on site 09355500---]\n"
     ]
    }
   ],
   "source": [
    "df_outlet_metrics = pd.DataFrame()\n",
    "df_outlet_mk_mag = pd.DataFrame()\n",
    "df_outlet_mk_dur = pd.DataFrame()\n",
    "df_outlet_mk_intra = pd.DataFrame()\n",
    "\n",
    "for i, row in df_outlets.iterrows():\n",
    "    if i >= test_limit: break\n",
    "    print(f'[---Working on site {row[\"site_no\"]}---]')\n",
    "    df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    if '00060_radar sensor_Mean' in df.columns and '00060_Mean' not in df.columns:\n",
    "        df.rename(columns={'00060_radar sensor_Mean': '00060_Mean'}, inplace=True)\n",
    "    \n",
    "    start = df['datetime'].min().date()\n",
    "    end = df['datetime'].max().date()\n",
    "    range = round((end - start).days / 365.25, 1)\n",
    "    \n",
    "    aquifer = row['aquifer']\n",
    "    \n",
    "    try:\n",
    "        df_single_site_metric, df_mk_mag, df_mk_dur, df_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "        add_data = {'data_start': start, 'data_end': end, 'total_record': range, 'aquifer': aquifer}\t\t\t\n",
    "        add_data = pd.DataFrame(add_data, index=['0'])\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Single site data failure for site {row['site_no']}:\\n{e}\")\n",
    "        continue\n",
    "    \n",
    "    add_data = pd.DataFrame(np.tile(add_data.values, (len(df_single_site_metric), 1)), columns=add_data.columns)\n",
    "    df_single_site_metric = pd.concat([df_single_site_metric.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "    df_mk_mag = pd.concat([df_mk_mag.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "    df_mk_dur = pd.concat([df_mk_dur.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "    df_mk_intra = pd.concat([df_mk_intra.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    df_outlet_metrics = pd.concat([df_outlet_metrics, df_single_site_metric], ignore_index=True)\n",
    "    df_outlet_mk_mag = pd.concat([df_outlet_mk_mag, df_mk_mag], ignore_index=True)\n",
    "    df_outlet_mk_dur = pd.concat([df_outlet_mk_dur, df_mk_dur], ignore_index=True)\n",
    "    df_outlet_mk_intra = pd.concat([df_outlet_mk_intra, df_mk_intra], ignore_index=True)\n",
    "    \n",
    "df_outlet_metrics = fn.gages_2_filtering(df_outlet_metrics)\n",
    "\n",
    "try:\n",
    "    fn.save_data(df_outlet_metrics, df_outlet_mk_mag, df_outlet_mk_dur, df_outlet_mk_intra, 'Outlet_Metrics')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: Nationwide Validity Dataset\n",
    "Generates a dataset for every water gauge with 00060_Mean data in the contiguous US indicating gauge validity for this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = 'ShapeFiles/Lower48/lower48.shp'\n",
    "test_state_list = ['ME', 'DE']\n",
    "test_limit = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on AL---]\n",
      "Total Sites: 177 in the state of AL\n",
      "Ignored 39 sites (22.03%)\n",
      "[---Working on AZ---]\n",
      "Total Sites: 218 in the state of AZ\n",
      "Ignored 11 sites (5.05%)\n",
      "[---Working on AR---]\n",
      "Total Sites: 139 in the state of AR\n",
      "Ignored 14 sites (10.07%)\n",
      "[---Working on CA---]\n",
      "Total Sites: 484 in the state of CA\n",
      "Ignored 56 sites (11.57%)\n",
      "[---Working on CO---]\n",
      "Total Sites: 348 in the state of CO\n",
      "Ignored 14 sites (4.02%)\n",
      "[---Working on CT---]\n",
      "Total Sites: 75 in the state of CT\n",
      "Ignored 5 sites (6.67%)\n",
      "[---Working on DE---]\n",
      "Total Sites: 36 in the state of DE\n",
      "Ignored 3 sites (8.33%)\n",
      "[---Working on FL---]\n",
      "Total Sites: 436 in the state of FL\n",
      "Ignored 36 sites (8.26%)\n",
      "[---Working on GA---]\n",
      "Total Sites: 308 in the state of GA\n",
      "Ignored 17 sites (5.52%)\n",
      "[---Working on ID---]\n",
      "Total Sites: 230 in the state of ID\n",
      "Ignored 10 sites (4.35%)\n",
      "[---Working on IL---]\n",
      "Total Sites: 193 in the state of IL\n",
      "Ignored 7 sites (3.63%)\n",
      "[---Working on IN---]\n",
      "Total Sites: 207 in the state of IN\n",
      "Ignored 22 sites (10.63%)\n",
      "[---Working on IA---]\n",
      "Total Sites: 143 in the state of IA\n",
      "Ignored 7 sites (4.9%)\n",
      "[---Working on KS---]\n",
      "Total Sites: 195 in the state of KS\n",
      "Ignored 5 sites (2.56%)\n",
      "[---Working on KY---]\n",
      "Total Sites: 170 in the state of KY\n",
      "Ignored 9 sites (5.29%)\n",
      "[---Working on LA---]\n",
      "Total Sites: 101 in the state of LA\n",
      "Ignored 29 sites (28.71%)\n",
      "[---Working on ME---]\n",
      "Total Sites: 68 in the state of ME\n",
      "Ignored 0 sites (0.0%)\n",
      "[---Working on MD---]\n",
      "Total Sites: 189 in the state of MD\n",
      "Ignored 8 sites (4.23%)\n",
      "[---Working on MA---]\n",
      "Total Sites: 144 in the state of MA\n",
      "Ignored 13 sites (9.03%)\n",
      "[---Working on MI---]\n",
      "Total Sites: 203 in the state of MI\n",
      "Ignored 20 sites (9.85%)\n",
      "[---Working on MN---]\n",
      "Total Sites: 130 in the state of MN\n",
      "Ignored 6 sites (4.62%)\n",
      "[---Working on MS---]\n",
      "Total Sites: 99 in the state of MS\n",
      "Ignored 16 sites (16.16%)\n",
      "[---Working on MO---]\n",
      "Total Sites: 244 in the state of MO\n",
      "Ignored 2 sites (0.82%)\n",
      "[---Working on MT---]\n",
      "Total Sites: 226 in the state of MT\n",
      "Ignored 7 sites (3.1%)\n",
      "[---Working on NE---]\n",
      "Total Sites: 120 in the state of NE\n",
      "Ignored 1 sites (0.83%)\n",
      "[---Working on NV---]\n",
      "Total Sites: 188 in the state of NV\n",
      "Ignored 10 sites (5.32%)\n",
      "[---Working on NH---]\n",
      "Total Sites: 101 in the state of NH\n",
      "Ignored 1 sites (0.99%)\n",
      "[---Working on NJ---]\n",
      "Total Sites: 141 in the state of NJ\n",
      "Ignored 10 sites (7.09%)\n",
      "[---Working on NM---]\n",
      "Total Sites: 174 in the state of NM\n",
      "Ignored 10 sites (5.75%)\n",
      "[---Working on NY---]\n",
      "Total Sites: 320 in the state of NY\n",
      "Ignored 15 sites (4.69%)\n",
      "[---Working on NC---]\n",
      "Total Sites: 241 in the state of NC\n",
      "Ignored 10 sites (4.15%)\n",
      "[---Working on ND---]\n",
      "Total Sites: 122 in the state of ND\n",
      "Ignored 8 sites (6.56%)\n",
      "[---Working on OH---]\n",
      "Total Sites: 223 in the state of OH\n",
      "Ignored 17 sites (7.62%)\n",
      "[---Working on OK---]\n",
      "Total Sites: 178 in the state of OK\n",
      "Ignored 8 sites (4.49%)\n",
      "[---Working on OR---]\n",
      "Total Sites: 213 in the state of OR\n",
      "Ignored 7 sites (3.29%)\n",
      "[---Working on PA---]\n",
      "Total Sites: 311 in the state of PA\n",
      "Ignored 6 sites (1.93%)\n",
      "[---Working on RI---]\n",
      "Total Sites: 34 in the state of RI\n",
      "Ignored 2 sites (5.88%)\n",
      "[---Working on SC---]\n",
      "Total Sites: 131 in the state of SC\n",
      "Ignored 13 sites (9.92%)\n",
      "[---Working on SD---]\n",
      "Total Sites: 122 in the state of SD\n",
      "Ignored 4 sites (3.28%)\n",
      "[---Working on TN---]\n",
      "Total Sites: 136 in the state of TN\n",
      "Ignored 15 sites (11.03%)\n",
      "[---Working on TX---]\n",
      "Total Sites: 598 in the state of TX\n",
      "Ignored 63 sites (10.54%)\n",
      "[---Working on UT---]\n",
      "Total Sites: 154 in the state of UT\n",
      "Ignored 5 sites (3.25%)\n",
      "[---Working on VT---]\n",
      "Total Sites: 98 in the state of VT\n",
      "Ignored 1 sites (1.02%)\n",
      "[---Working on VA---]\n",
      "Total Sites: 200 in the state of VA\n",
      "Ignored 8 sites (4.0%)\n",
      "[---Working on WA---]\n",
      "Total Sites: 266 in the state of WA\n",
      "Ignored 12 sites (4.51%)\n",
      "[---Working on WV---]\n",
      "Total Sites: 110 in the state of WV\n",
      "Ignored 2 sites (1.82%)\n",
      "[---Working on WI---]\n",
      "Total Sites: 220 in the state of WI\n",
      "Ignored 31 sites (14.09%)\n",
      "[---Working on WY---]\n",
      "Total Sites: 120 in the state of WY\n",
      "Ignored 4 sites (3.33%)\n"
     ]
    }
   ],
   "source": [
    "df_natl_validity = pd.DataFrame()\n",
    "\n",
    "for i, state in enumerate(fn.STATE_LIST):\n",
    "    if i >= test_limit: break\n",
    "    print(f'[---Working on {state}---]')\n",
    "    state_uri = fn.create_state_uri(state, fn.PARAM_CODE)\n",
    "    df_state_sites = filter_state_site(shapefile_path, state_uri)\n",
    "    print(f'Total Sites: {len(df_state_sites)} in the state of {state}')\n",
    "    \n",
    "    ignored_count = 0\n",
    "    for loc, row in df_state_sites.iterrows():\n",
    "        df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        if df.empty:\n",
    "            ignored_count += 1\n",
    "            continue\n",
    "        \n",
    "        if '00060_radar sensor_Mean' in df.columns and '00060_Mean' not in df.columns:\n",
    "            df.rename(columns={'00060_radar sensor_Mean': '00060_Mean'}, inplace=True)\n",
    "        \n",
    "        if '00060_Mean' not in df.columns:\n",
    "            ignored_count += 1\n",
    "            continue\n",
    "        \n",
    "        start = df['datetime'].min().date()\n",
    "        end = df['datetime'].max().date()\n",
    "        total_record = round((end - start).days / 365.25, 1)\n",
    "        \n",
    "        valid_range_30 = total_record > 30\n",
    "        valid_range_50 = total_record > 50\n",
    "        \n",
    "        date_threshold_30 = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * 30)\n",
    "        df_30 = df[df['datetime'].dt.date >= date_threshold_30]\n",
    "        missing_30 = fn.validate(df_30, date_threshold_30, fn.DEFAULT_END)\n",
    "        valid_30 = missing_30 < fn.MAX_MISSING_THRESHOLD\n",
    "        \n",
    "        date_threshold_50 = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * 50)\n",
    "        df_50 = df[df['datetime'].dt.date >= date_threshold_50]\n",
    "        missing_50 = fn.validate(df_50, date_threshold_50, fn.DEFAULT_END)\n",
    "        valid_50 = missing_50 < fn.MAX_MISSING_THRESHOLD\n",
    "        \n",
    "        data = {'site_no': str(row['site_no']), 'state': state, 'date_range_30': valid_range_30, 'data_cont_30': valid_30, 'missing_30': missing_30,\n",
    "                'date_range_50': valid_range_50, 'data_cont_50': valid_50, 'missing_50': missing_50, 'start': start, 'end': end, \n",
    "                'total_record': total_record, 'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va']}\n",
    "        \n",
    "        df_natl_validity = pd.concat([df_natl_validity, pd.DataFrame(data, index=['0'])], axis=0, ignore_index=True)\n",
    "        \n",
    "    print(f'Ignored {ignored_count} sites ({round((ignored_count / len(df_state_sites)) * 100, 2)}%)')        \n",
    "        \n",
    "try:\n",
    "    with pd.ExcelWriter('Prelim_Data/National_Validity.xlsx') as writer:\n",
    "        df_natl_validity.to_excel(writer, sheet_name='national_validity', index=False) \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: HUC2/HUC4/Aquifer Master Shapefile Creation\n",
    "The following code combines HUC2, HUC4, and Aquifer shapefiles into master shapefiles, respectively. It's not necessary to run if these files already exist within the ShapeFiles directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc2_path = 'ShapeFiles/HUC2'\n",
    "huc4_path = 'ShapeFiles/HUC4'\n",
    "aquifer_path = 'ShapeFiles/Aquifers'\n",
    "aq_ext = '.shp'\n",
    "huc2 = 'WBDHU2.shp'\n",
    "huc4 = 'WBDHU4.shp'\n",
    "\n",
    "create_huc2 = False\n",
    "create_huc4 = False\n",
    "create_aquifer = True\n",
    "\n",
    "huc2_gdf = gpd.GeoDataFrame()\n",
    "huc4_gdf = gpd.GeoDataFrame()\n",
    "aquifer_df = gpd.GeoDataFrame()\n",
    "\n",
    "# HUC2's\n",
    "if create_huc2:\n",
    "    for root, dirs, files in os.walk(huc2_path):\n",
    "        if os.path.basename(root).startswith('WBD_'):\n",
    "            code = root[-2:]\n",
    "            if huc2 in files:\n",
    "                shape = gpd.read_file(os.path.join(root, huc2))\n",
    "                shape['huc2_code'] = code\n",
    "                huc2_gdf = pd.concat([huc2_gdf, shape], ignore_index=True)\n",
    "\n",
    "    huc2_gdf = huc2_gdf.to_crs(4269)\n",
    "    huc2_gdf.to_file('ShapeFiles/HUC2/_Master_HUC2/master_huc2.shp')\n",
    "\n",
    "# HUC4's    \n",
    "if create_huc4:\n",
    "    for root, dirs, files in os.walk(huc4_path):\n",
    "        if os.path.basename(root).startswith('NHD_H_'):\n",
    "            code = root[-4:]\n",
    "            if huc4 in files:\n",
    "                shape = gpd.read_file(os.path.join(root, huc4))\n",
    "                shape['huc4_code'] = code\n",
    "                huc4_gdf = pd.concat([huc4_gdf, shape], ignore_index=True)\n",
    "\n",
    "    huc4_gdf = huc4_gdf.to_crs(4269)\n",
    "    huc4_gdf.to_file('ShapeFiles/HUC4/_Master_HUC4/master_huc4.shp')\n",
    "    \n",
    "# Aquifers\n",
    "if create_aquifer:\n",
    "    for root, dirs, files in os.walk(aquifer_path):\n",
    "        for file in files:\n",
    "            if file.endswith(aq_ext) and dirs:\n",
    "                if file.startswith('Penn'):\n",
    "                    shape = gpd.read_file(os.path.join(root, file))\n",
    "                    shape = shape.iloc[[17]]                    \n",
    "                else:\n",
    "                    shape = gpd.read_file(os.path.join(root, file))\n",
    "                    \n",
    "                shape = shape.to_crs(4269)\n",
    "                shape['aq_name'] = file[:-4]\n",
    "                aquifer_df = pd.concat([aquifer_df, shape], ignore_index=True)\n",
    "                \n",
    "    aquifer_df = aquifer_df.to_crs(4269)\n",
    "    aquifer_df.to_file('ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: HUC2/4/Aquifer Sorting\n",
    "Takes a national validity or national metrics dataset (any dataset with dec_lat_va and dec_long_va data), and adds corresponding HUC2, HUC4, and Aquifer columns indicating a water guages presence or lack thereof in each of these boundaries.\n",
    "\n",
    "<strong>NOTE:</strong> This feature has already been implemented directly into the National Metrics dataset creation. Its only real use is on National Validity datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Prelim_Data/_National_Metrics/'\n",
    "datasets = ['National_Metrics_30_90_TEST.xlsx']\n",
    "huc2_gdf = gpd.read_file('ShapeFiles/HUC2/_Master_HUC2/master_huc2.shp')\n",
    "huc4_gdf = gpd.read_file('ShapeFiles/HUC4/_Master_HUC4/master_huc4.shp')\n",
    "aq_gdf = gpd.read_file('ShapeFiles/Aquifers/Conterm_US/master_aquifer.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    temp = pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_excel(f'{path}{dataset}', dtype={'site_no': str})\n",
    "    sheets = pd.ExcelFile(f'{path}{dataset}')\n",
    "    sheet = sheets.sheet_names[0]          \n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        add_data = pd.DataFrame({'site_no': str(row['site_no']), 'huc2_code': 'NA', 'huc4_code': 'NA', 'aquifer': 'NA'}, index=['0'])\n",
    "        point = Point(row['dec_long_va'], row['dec_lat_va'])\n",
    "        \n",
    "        # HUC2 assignment\n",
    "        for j, geo_row in huc2_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                add_data[\"huc2_code\"] = geo_row['huc2_code']\n",
    "                continue\n",
    "            \n",
    "        # HUC4 assignment\n",
    "        for j, geo_row in huc4_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                add_data[\"huc4_code\"] = geo_row['huc4_code']\n",
    "                continue\n",
    "            \n",
    "        # Aquifer Assignment\n",
    "        for j, geo_row in aq_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                add_data[\"aquifer\"] = geo_row['aq_name']\n",
    "                continue\n",
    "            \n",
    "        temp = pd.concat([temp, add_data], axis=0, ignore_index=True)\n",
    "    \n",
    "    df = pd.merge(df, temp, on='site_no', validate='1:1')\n",
    "    \n",
    "    with pd.ExcelWriter(f'{path}{dataset}', mode='a', if_sheet_exists='replace') as writer:\n",
    "        df.to_excel(writer, sheet_name=sheet, index=False)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: National Dataset Splitting by Aquifer\n",
    "This script splits a national metric dataset or datasets into smaller per-aquifer datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "natl_path = 'Prelim_Data/_National_Metrics'\n",
    "\n",
    "# National datasets to split\n",
    "datasets = ['National_Metrics_30_90.xlsx', 'National_Metrics_30_95.xlsx', 'National_Metrics_50_90.xlsx', 'National_Metrics_50_95.xlsx']\n",
    "# The datasets to generate from the national dataset\n",
    "target_aquifers = cl.ALL_AQUIFERS\n",
    "sheet_names = ['site_metrics', 'mk_magnitude', 'mk_duration', 'mk_intra_annual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over national datasets\n",
    "for dataset in datasets:\n",
    "    df_list = []\n",
    "    for sheet in sheet_names:\n",
    "        df = pd.read_excel(f'{natl_path}/{dataset}', sheet_name=sheet, dtype=fn.DATASET_DTYPES)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Iterate over target aquifers\n",
    "    for aquifer in target_aquifers:\n",
    "        save_path = f\"{aquifer.datasets_dir}/{aquifer.name}_{dataset[-10:-8]}_{dataset[-7:-5]}.xlsx\"\n",
    "        for i, df in enumerate(df_list):\n",
    "            df = df[df['huc4_code'].isin(aquifer.huc4s)]\n",
    "            \n",
    "            # Append new sheets to existing file\n",
    "            if os.path.exists(save_path):\n",
    "                with pd.ExcelWriter(save_path, mode='a', if_sheet_exists='replace') as writer:\n",
    "                    df.to_excel(writer, sheet_name=sheet_names[i], index=False)\n",
    "            # Create file if it doesn't exist (first iteration)                        \n",
    "            else: \n",
    "                with pd.ExcelWriter(save_path, mode='w') as writer:\n",
    "                    df.to_excel(writer, sheet_name=sheet_names[i], index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
