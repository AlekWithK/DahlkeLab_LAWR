{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES:\n",
    "<ul>\n",
    "<li>Sections labeled <strong>CORE</strong> must be run in order to begin Aquifer Analysis</li>\n",
    "<li>Cells labeled <strong>Control</strong> contain inputs for the immeadiately proceeding section(s)</li>\n",
    "<li>Sections labeled <strong>EXTRA</strong> contain additional plotting or analysis tools but are not necessary for Aquifer Analysis</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python3.10\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import contextily as cx\n",
    "import requests\n",
    "import calendar\n",
    "from importlib import reload\n",
    "from typing import IO\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import Point\n",
    "from io import StringIO\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# USGS Data retreival tool\n",
    "from dataretrieval import nwis, utils, codes\n",
    "\n",
    "# Custom modules are imported in multiple locations to faciliate easy reloading when edits are made to their respective files\n",
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "reload(fn)\n",
    "\n",
    "# TODO: Look into the warning that this is disabling. It doesn't appear to be significant for the purposes of this code but should be understood\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#pd.options.mode.chained_assignment = 'warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Single Site Data<br>\n",
    "This function produces the streamflow analysis (using the functions above) for a single site given a list of thresholds and time windows to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "\n",
    "# df2 holds all-time data, df is analyzed range\n",
    "curr_guage = cl.SRB_Guage\n",
    "df = nwis.get_record(sites='13335050', service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2020-09-30')\n",
    "df2 = nwis.get_record(sites=curr_guage.id, service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2014-09-30')\n",
    "\n",
    "# Set to true if running indepedently to verify data with Kocis paper (or other source)\n",
    "# Set to false if running Aquifer Analysis\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Site Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site No: 0    01495000\n",
      "1    01495000\n",
      "2    01495000\n",
      "0    01495000\n",
      "Name: site_no, dtype: object\n",
      "Analyzed Range: 30\n",
      "30\n",
      "50\n",
      "50\n",
      "Quantile: 0.90\n",
      "0.95\n",
      "0.90\n",
      "0.95\n",
      "Valid: True\n",
      "True\n",
      "True\n",
      "True\n",
      "% Missing: 0\n",
      "0\n",
      "0\n",
      "0\n",
      "90%: 127.0\n",
      "189.0\n",
      "126.0\n",
      "190.0\n",
      "HMF Years: 30\n",
      "29\n",
      "50\n",
      "49\n",
      "Annual Duration: 36.200000\n",
      "18.827586\n",
      "36.200000\n",
      "18.469388\n",
      "Event Duration: 1.996894\n",
      "1.443214\n",
      "1.908477\n",
      "1.415568\n",
      "Event HMF: 0.000807\n",
      "0.000863\n",
      "0.000767\n",
      "0.000816\n",
      "Inter-annual Frequency: 100.000\n",
      " 96.667\n",
      "100.000\n",
      " 98.000%\n",
      "Intra-annual Frequency: 17.033333\n",
      "12.689655\n",
      "18.080000\n",
      "12.714286\n",
      "Total HMF in km^3/year: 0.014802\n",
      "0.011335\n",
      "0.014540\n",
      "0.010781\n",
      "Center of Mass: 176.500000\n",
      "174.275862\n",
      "170.680000\n",
      "172.204082\n",
      "6 Month HMF in km^3/year: 0.009492\n",
      "0.007050\n",
      "0.009139\n",
      "0.006633\n",
      "3 Month HMF in km^3/year: 0.005017\n",
      "0.003819\n",
      "0.005216\n",
      "0.003950\n"
     ]
    }
   ],
   "source": [
    "# If looking at a post impairment period, but calculating threshold based on the full record of data, pass a second dataframe with a different \n",
    "# start/end date as the final parameter. This method was used in Kocis 2017 and is needed for some data verification, but is not the methodology\n",
    "# used for the Aquifer Analysis and so *args will most often be empty.\n",
    "import Src.func as fn\n",
    "reload(fn)\n",
    "\n",
    "def single_site_data(df: pd.DataFrame, quantiles_list: list, data_ranges_list: list, *args):\n",
    "    df = df.reset_index()    \n",
    "    threshold = None\n",
    "    df_complete_site_data = pd.DataFrame()    \n",
    "    df_complete_mk_mag = pd.DataFrame()\n",
    "    df_complete_mk_dur = pd.DataFrame()\n",
    "    df_complete_mk_intra = pd.DataFrame()\n",
    "    df_complete_mk_event_mag = pd.DataFrame()\n",
    "    df_complete_mk_event_dur = pd.DataFrame()\n",
    "    df_complete_mk_timing = pd.DataFrame()\n",
    "    \n",
    "    for data_range in data_ranges_list:\n",
    "        for quantile in quantiles_list:\n",
    "            # Copying original dataframe to avoid any conflicts\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Validate that site is not missing > 10% of data\n",
    "            date_threshold = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * data_range)              \n",
    "            \n",
    "            # Filter dataframe based on current data_range (don't do this during testing if testing unique dataset/range)\n",
    "            if not testing:\n",
    "                df_copy = df_copy[df_copy['datetime'].dt.date >= date_threshold]\n",
    "            \n",
    "            # Validate <10% missing data based on filtered dataframe\n",
    "            missing = fn.validate(df_copy, date_threshold, fn.DEFAULT_END)\n",
    "            valid = missing < fn.MAX_MISSING_THRESHOLD\n",
    "            missing = round(missing, 5) * 100                              \n",
    "    \n",
    "            # Check for optional second dataframe containing full record\n",
    "            if args and isinstance(args[0], pd.DataFrame) and not args[0].empty:\n",
    "                #print('Threshold calculation across the full record')\n",
    "                df2 = args[0].reset_index()\n",
    "                threshold = fn.calc_threshold(df2, quantile)\n",
    "                # threshold = 52350 # SRB_Guage post-impairment threshold for verification with Kocis_2017 \n",
    "            else:\n",
    "                #print('Threshold calculation across a limited record')\n",
    "                threshold = fn.calc_threshold(df_copy, quantile)                             \n",
    "\n",
    "            # Create a dataframe with only days over HMF threshold as well as continuous dataframe for MK trend test\n",
    "            hmf_series_defl, hmf_series_cont = fn.filter_hmf(df_copy, threshold)\n",
    "            #print(zero_deflated_hmf)\n",
    "            \n",
    "            # Aggregate data before performing MK magnitude test\n",
    "            df_agg_cont = hmf_series_cont.copy()\n",
    "            df_agg_cont = fn.convert_hmf(df_agg_cont, threshold)\n",
    "            df_agg_cont['00060_Mean'] = df_agg_cont['00060_Mean'].apply(lambda x: x * fn.CUBIC_FT_KM_FACTOR if x >= 0 else 0)\n",
    "            df_agg_cont.set_index('datetime', inplace=True)\n",
    "            df_agg_cont = df_agg_cont.resample(fn.HYDRO_YEAR).agg({'00060_Mean': ['sum', 'count']})\n",
    "            df_agg_cont.columns = ['Sum', 'Count']\n",
    "            df_agg_cont_defl = df_agg_cont[df_agg_cont['Sum'] > 0]          \n",
    "            \n",
    "            # MK Magnitude\n",
    "            df_mk_mag = fn.mann_kendall(df_agg_cont_defl['Sum'], df_agg_cont['Sum'], fn.MK_TREND_ALPHA)\n",
    "\n",
    "            # Find number of years with HMF\n",
    "            hmf_years = fn.num_hmf_years(hmf_series_defl)            \n",
    "\n",
    "            # Mask out months that don't fall within 3 and 6 month Winter range\n",
    "            df_six_month, df_three_month = fn.three_six_range(hmf_series_defl, 12, 2, 11, 4)\n",
    "\n",
    "            # Convert to daily average flow in cfpd, and take only flow above the threshold\n",
    "            hmf_series_defl = fn.convert_hmf(hmf_series_defl, threshold)\n",
    "            total_hmf_flow = hmf_series_defl[\"00060_Mean\"].sum()\n",
    "            hmf_per_month = fn.monthly_hmf(hmf_series_defl, data_range, quantile)\n",
    "\n",
    "            # Calculate 3 and 6 month HMF\n",
    "            df_six_month = fn.convert_hmf(df_six_month, threshold)\n",
    "            six_month_hmf = df_six_month[\"00060_Mean\"].sum()\n",
    "            df_three_month = fn.convert_hmf(df_three_month, threshold)\n",
    "            three_month_hmf = df_three_month[\"00060_Mean\"].sum()\n",
    "            \n",
    "            total_hmf_flow = (total_hmf_flow * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            six_month_hmf = (six_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            three_month_hmf = (three_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "\n",
    "            # Inter-annual\n",
    "            inter_annual, delta = fn.calc_inter_annual(hmf_series_cont, hmf_years)            \n",
    "\n",
    "            # Average Duration and Intra-annual Frequency\n",
    "            hmf_series_cont = fn.convert_hmf(hmf_series_cont, threshold)\n",
    "            event_duration, annual_duration, intra_annual, event_hmf, df_results = fn.calc_duration_intra_annual(hmf_series_cont, hmf_years)\n",
    "            \n",
    "            # Total duration mk test \n",
    "            dur_series_defl = df_results['total_days'][df_results['total_days'] > 0]\n",
    "            dur_series_cont = df_results['total_days']\n",
    "            df_mk_dur = fn.mann_kendall(dur_series_defl, dur_series_cont, fn.MK_TREND_ALPHA)\n",
    "                        \n",
    "            # Total events per year mk test            \n",
    "            intra_series_defl = df_results['total_events'][df_results['total_events'] > 0]\n",
    "            intra_series_cont = df_results['total_events']\n",
    "            df_mk_intra = fn.mann_kendall(intra_series_defl, intra_series_cont, fn.MK_TREND_ALPHA) \n",
    "            \n",
    "            # Average event HMF mk test\n",
    "            event_mag_series_defl = df_results['event_hmf'][df_results['event_hmf'] > 0]\n",
    "            event_mag_series_cont = df_results['event_hmf']\n",
    "            df_mk_event_mag = fn.mann_kendall(event_mag_series_defl, event_mag_series_cont, fn.MK_TREND_ALPHA)\n",
    "            \n",
    "            # Average event duration mk test\n",
    "            event_dur_series_defl = df_results['duration'][df_results['duration'] > 0]\n",
    "            event_dur_series_cont = df_results['duration']\n",
    "            df_mk_event_dur = fn.mann_kendall(event_dur_series_defl, event_dur_series_cont, fn.MK_TREND_ALPHA)                \n",
    "            \n",
    "            # Timing Calculation using DOHY\n",
    "            timing, com_series = fn.calc_timing(hmf_series_defl)\n",
    "            df_mk_timing = fn.mann_kendall(com_series, com_series, fn.MK_TREND_ALPHA)   \n",
    "                     \n",
    "            \n",
    "            # Merging site dataframe with Mann-Kendall dataframe. This start date is the beginning of the actual data, not necessarily \n",
    "            # the beginning of the analyzed range. Validation (above) starts from the start of the official range (1970/90-2020)\n",
    "            start = df_copy['datetime'].min().date()\n",
    "            end = df_copy['datetime'].max().date()\n",
    "            data = {'dataset_ID': (data_range * quantile), 'site_no': df_copy.iloc[0]['site_no'], 'analyze_start': start, 'analyze_end': end, 'analyze_range': delta, 'quantile': quantile, \n",
    "                    'valid': valid, 'missing_data%': missing, 'threshold': threshold, 'hmf_years': hmf_years, 'annual_hmf': total_hmf_flow, 'six_mo_hmf': six_month_hmf, 'three_mo_hmf': three_month_hmf, \n",
    "                    'annual_duration': annual_duration, 'event_duration': event_duration, 'event_hmf': event_hmf, 'inter_annual%': inter_annual, 'intra_annual': intra_annual, 'timing': timing}              \n",
    "            \n",
    "            # Merging MK results\n",
    "            df_complete_mk_mag = fn.merge_mk_results(df_complete_mk_mag, df_mk_mag, df_copy.iloc[0]['site_no'], data_range, quantile)\n",
    "            df_complete_mk_dur = fn.merge_mk_results(df_complete_mk_dur, df_mk_dur, df_copy.iloc[0]['site_no'], data_range, quantile)\n",
    "            df_complete_mk_intra = fn.merge_mk_results(df_complete_mk_intra, df_mk_intra, df_copy.iloc[0]['site_no'], data_range, quantile)\n",
    "            df_complete_mk_event_mag = fn.merge_mk_results(df_complete_mk_event_mag, df_mk_event_mag, df_copy.iloc[0]['site_no'], data_range, quantile) \n",
    "            df_complete_mk_event_dur = fn.merge_mk_results(df_complete_mk_event_dur, df_mk_event_dur, df_copy.iloc[0]['site_no'], data_range, quantile)\n",
    "            df_complete_mk_timing = fn.merge_mk_results(df_complete_mk_timing, df_mk_timing, df_copy.iloc[0]['site_no'], data_range, quantile)        \n",
    "            \n",
    "            # Merging metric results\n",
    "            df_single_iteration = pd.DataFrame(data, index=['0'])\n",
    "            df_single_iteration = pd.concat([df_single_iteration.reset_index(drop=True), hmf_per_month.reset_index(drop=True)], axis=1)\n",
    "            df_complete_site_data = pd.concat([df_complete_site_data.reset_index(drop=True), df_single_iteration.reset_index(drop=True)], axis=0)\n",
    "        \n",
    "    return df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra, df_complete_mk_event_mag, df_complete_mk_event_dur, df_complete_mk_timing\n",
    "\n",
    "# For testing purposes, to run this cell independently\n",
    "single_site_result = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "\n",
    "'''\n",
    "try:\n",
    "    with pd.ExcelWriter('df_single_site.xlsx') as writer:\n",
    "        df_complete_site_data.to_excel(writer, sheet_name='site_metrics', index=False)\n",
    "        df_complete_mk_mag.to_excel(writer, sheet_name='mk_magnitude', index=False)\n",
    "        df_complete_mk_intra.to_excel(writer, sheet_name='mk_intra', index=False)\n",
    "        df_complete_mk_dur.to_excel(writer, sheet_name='mk_duration', index=False)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "'''\n",
    "   \n",
    "fn.single_site_report(single_site_result[0])\n",
    "df_complete_site_data = fn.gages_2_filtering(single_site_result[0])\n",
    "#fn.save_data(df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra, 'TEST')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Multi-Site Filtering<br>\n",
    "This function creates the list of sites to analyze by filtering the complete list of state sites with 00060_Mean data down to those that lie within a specific watershed boundary using decimal long/lat positional data and a region shapefile (e.g. state or watershed boundary)<br><br>\n",
    "\n",
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed when running the following cell independently\n",
    "shapefile_path = 'ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site List Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sites: 87 in the state of CA in the given WB\n",
      "['09429000', '09429100', '09429155', '09429180', '09429200', '09429500', '09429600', '09521100', '09523000', '09523200', '09523400', '09523600', '09523800', '09524000', '09524700', '09525000', '09526200', '09530000', '09530500', '11251000', '11253310', '11255575', '11261100', '11261500', '11262900', '11273400', '11274000', '11274500', '11274550', '11274630', '11289850', '11290000', '11303000', '11303500', '11304810', '11311300', '11312672', '11312676', '11312685', '11312968', '11313240', '11313315', '11313405', '11313431', '11313433', '11313434', '11313440', '11313460', '11336600', '11336685', '11336790', '11336930', '11337080', '11370500', '11370700', '11374000', '11376000', '11376550', '11379500', '11389500', '11390500', '11421000', '11424000', '11425500', '11446500', '11447360', '11447650', '11447830', '11447850', '11447890', '11447903', '11447905', '11451800', '11452500', '11452800', '11452900', '11453000', '11455095', '11455140', '11455280', '11455315', '11455338', '11455385', '351813119150601', '352533118494601', '360013118575201', '375450121331701']\n"
     ]
    }
   ],
   "source": [
    "def filter_state_site(shapefile_path: str, state_uri: str):\n",
    "    \"\"\"Creates a list of sites with over 50 years of 00060_Mean streamflow data for a given region\"\"\"\n",
    "    # Request page from USGS site, ignore all informational lines\n",
    "    response = requests.get(state_uri)\n",
    "    data = response.text\n",
    "    lines = data.splitlines()\n",
    "    lines = [line for line in lines if not line.startswith('#')]\n",
    "\n",
    "    # Create dataframe where site_no is a list of all sites in a state with 00060 data\n",
    "    tsd = \"\\n\".join(lines)\n",
    "    df = pd.read_csv(StringIO(tsd), sep='\\t')\n",
    "    df_state_sites = df.iloc[1:]\n",
    "        \n",
    "    # Filter out sites outside of HU boundary\n",
    "    if fn.SORT_BY_WB:\n",
    "        shapefile = gpd.read_file(shapefile_path)\n",
    "        df_state_sites['geometry'] = [Point(lon, lat) for lon, lat in zip(df_state_sites['dec_long_va'], df_state_sites['dec_lat_va'])]\n",
    "        gdf_data = gpd.GeoDataFrame(df_state_sites, crs=shapefile.crs)\n",
    "        df_state_sites = gpd.sjoin(gdf_data, shapefile, predicate='within')\n",
    "            \n",
    "    #print(df_state_sites.columns.to_list())\n",
    "    #print(df_state_sites)\n",
    "    \n",
    "    return df_state_sites\n",
    "\n",
    "df_state_sites = filter_state_site(shapefile_path, fn.SITES_URI)\n",
    "print(f'Total Sites: {len(df_state_sites)} in the state of {fn.STATE_CODE.upper()} in the given WB')\n",
    "site_list = df_state_sites['site_no'].to_list()\n",
    "print(site_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: National Metrics Dataset\n",
    "Generates a nationwide dataset for all sites meeting datarange criteria (lengthy runtime!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc2_gdf = gpd.read_file('ShapeFiles/HUC2/_Master_HUC2/master_huc2.shp')\n",
    "huc4_gdf = gpd.read_file('ShapeFiles/HUC4/_Master_HUC4/master_huc4.shp')\n",
    "aq_gdf = gpd.read_file('ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aq_name', 'OBJECTID_1', 'ROCK_NAME', 'ROCK_TYPE', 'AQ_CODE', 'Shape_Leng', 'Shape_Area', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "print(aq_gdf['aq_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_limit = 50\n",
    "dataset_name = 'National_Metrics'\n",
    "shapefile_path = 'ShapeFiles/Lower48/lower48.shp'\n",
    "\n",
    "# States with few sites for testing purposes\n",
    "test_state_list = ['ME', 'DE']\n",
    "\n",
    "# Set to False to ignore the blacklist and analyze all sites. This could be occasionally\n",
    "# worth doing as USGS could update site data making it valid in the future\n",
    "allow_blacklist = False\n",
    "\n",
    "curr_blacklist = []\n",
    "try:\n",
    "    with open('Prelim_Data/_National_Metrics/National_Sites_Blacklist.txt', 'r') as file:\n",
    "        curr_blacklist = file.read().split(', ')\n",
    "        curr_blacklist = [x.replace(\"'\", \"\") for x in curr_blacklist]\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on DE---]\n",
      "Total Sites: 36 in the state of DE\n",
      "Working on DE site 1/36 (01463500)\n",
      "Working on DE site 2/36 (01477800)\n",
      "Working on DE site 3/36 (01478000)\n",
      "Working on DE site 4/36 (01478650)\n",
      "Working on DE site 5/36 (01478950)\n",
      "ERROR: Single site data failure for site 01478950:\n",
      "float division by zero\n",
      "Working on DE site 6/36 (01479000)\n",
      "Working on DE site 7/36 (01480000)\n",
      "Working on DE site 8/36 (01480015)\n",
      "Working on DE site 9/36 (01480017)\n",
      "Working on DE site 10/36 (01481500)\n",
      "Working on DE site 11/36 (01482100)\n",
      "IGNORED: No data for site 01482100\n",
      "Working on DE site 12/36 (01482695)\n",
      "IGNORED: No data for site 01482695\n",
      "Working on DE site 13/36 (01483155)\n",
      "Working on DE site 14/36 (01483200)\n",
      "Working on DE site 15/36 (01483700)\n",
      "Working on DE site 16/36 (01484080)\n",
      "Working on DE site 17/36 (01484100)\n",
      "Working on DE site 18/36 (01484525)\n",
      "Working on DE site 19/36 (01484695)\n",
      "Working on DE site 20/36 (01487000)\n",
      "Working on DE site 21/36 (01488500)\n",
      "Working on DE site 22/36 (01478245)\n",
      "Working on DE site 23/36 (01479820)\n",
      "Working on DE site 24/36 (01481000)\n",
      "Working on DE site 25/36 (0148471320)\n",
      "Working on DE site 26/36 (01485000)\n",
      "Working on DE site 27/36 (01485500)\n",
      "Working on DE site 28/36 (01486000)\n",
      "Working on DE site 29/36 (01490000)\n",
      "Working on DE site 30/36 (01491000)\n",
      "Working on DE site 31/36 (01491500)\n",
      "Working on DE site 32/36 (01492500)\n",
      "Working on DE site 33/36 (01493000)\n",
      "Working on DE site 34/36 (01493112)\n",
      "Working on DE site 35/36 (01493500)\n",
      "Working on DE site 36/36 (01495000)\n"
     ]
    }
   ],
   "source": [
    "df_natl_metrics = pd.DataFrame()\n",
    "df_natl_mk_mag = pd.DataFrame()\n",
    "df_natl_mk_dur = pd.DataFrame()\n",
    "df_natl_mk_intra = pd.DataFrame()\n",
    "df_natl_mk_event_mag = pd.DataFrame()\n",
    "df_natl_mk_event_dur = pd.DataFrame()\n",
    "df_natl_mk_timing = pd.DataFrame()\n",
    "\n",
    "natl_blacklist = []\n",
    "\n",
    "for state_index, state in enumerate(['DE']):\n",
    "    if state_index >= state_limit: break\n",
    "    \n",
    "    print(f'[---Working on {state}---]')\n",
    "    state_uri = fn.create_state_uri(state, fn.PARAM_CODE)\n",
    "    df_state_sites = filter_state_site(shapefile_path, state_uri)\n",
    "    df_state_sites = df_state_sites.reset_index()\n",
    "    print(f'Total Sites: {len(df_state_sites)} in the state of {state}')\n",
    "    \n",
    "    # Modified version of the create_multi_site_data() function\n",
    "    for site_index, row in df_state_sites.iterrows():\n",
    "        \n",
    "        #if allow_blacklist and str(row['site_no']) in curr_blacklist:\n",
    "            #print(f'IGNORED: Site {row[\"site_no\"]} is in aquifer blacklist')\n",
    "            #continue\n",
    "        \n",
    "        df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=[fn.PARAM_CODE, fn.TIDAL_CODE], start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "        df = df.reset_index()\n",
    "        print(f'Working on {state} site {site_index + 1}/{len(df_state_sites)} ({row[\"site_no\"]})')\n",
    "        \n",
    "        # No data at all\n",
    "        if df.empty:\n",
    "            #natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: No data for site {row[\"site_no\"]}')\n",
    "            continue\n",
    "        \n",
    "        # Important for one outlet gauge in TGC aquifer (and maybe others)\n",
    "        if '00060_radar sensor_Mean' in df.columns and '00060_Mean' not in df.columns:\n",
    "            df.rename(columns={'00060_radar sensor_Mean': '00060_Mean'}, inplace=True)\n",
    "        \n",
    "        # Merge tidal data before beginning analysis\n",
    "        df = fn.merge_tidal(df)\n",
    "        \n",
    "        start = df['datetime'].min().date()\n",
    "        end = df['datetime'].max().date()\n",
    "        range = round((end - start).days / 365.25, 1)\n",
    "        \n",
    "        # Removed to create datasets with all 00060_Mean data regardless of available range\n",
    "        '''if range < fn.MIN_DATA_PERIOD and allow_blacklist:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: Not enough data for site {row[\"site_no\"]}')\n",
    "            continue'''\n",
    "        \n",
    "        point = Point(row['dec_long_va'], row['dec_lat_va'])\n",
    "        #state_code = row['station_nm'].strip()[-2:]\n",
    "        \n",
    "        huc2 = huc4 = aquifer = 'NA'\n",
    "        # HUC2 assignment\n",
    "        for i, geo_row in huc2_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                huc2 = geo_row['huc2_code']\n",
    "                continue\n",
    "            \n",
    "        # HUC4 assignment\n",
    "        for i, geo_row in huc4_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                huc4 = geo_row['huc4_code']\n",
    "                continue\n",
    "            \n",
    "        # Aquifer assignment    \n",
    "        for i, geo_row in aq_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                aquifer = geo_row['aq_name']\n",
    "                continue        \n",
    "        \n",
    "        # A few very broken sites with almost no data can have 0 hmf years and cause errors (i.e. '03592000')\n",
    "        # so we'll catch these and add them to the site blacklist\n",
    "        try:\n",
    "            df_single_site_metric, df_mk_mag, df_mk_dur, df_mk_intra, df_mk_event_mag, df_mk_event_dur, df_mk_timing = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "            add_data = {'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va'], 'data_start': start, 'data_end': end, 'total_record': range, \n",
    "                        'state': state, 'huc2_code': huc2, 'huc4_code': huc4, 'within_aq': aquifer}\t\t\t\n",
    "            add_data = pd.DataFrame(add_data, index=['0'])\n",
    "        except Exception as e:\n",
    "            #natl_blacklist.append(row['site_no'])\n",
    "            print(f\"ERROR: Single site data failure for site {row['site_no']}:\\n{e}\")\n",
    "            continue\n",
    "        \n",
    "        add_data = pd.DataFrame(np.tile(add_data.values, (len(df_single_site_metric), 1)), columns=add_data.columns)\n",
    "        df_single_site_metric = pd.concat([df_single_site_metric.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_mag = pd.concat([df_mk_mag.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_dur = pd.concat([df_mk_dur.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_intra = pd.concat([df_mk_intra.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_event_mag = pd.concat([df_mk_event_mag.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_event_dur = pd.concat([df_mk_event_dur.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        df_mk_timing = pd.concat([df_mk_timing.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "        # Append single site data to multi-site dataframes\n",
    "        df_natl_metrics = pd.concat([df_natl_metrics, df_single_site_metric], ignore_index=True)\n",
    "        df_natl_mk_mag = pd.concat([df_natl_mk_mag, df_mk_mag], ignore_index=True)\n",
    "        df_natl_mk_dur = pd.concat([df_natl_mk_dur, df_mk_dur], ignore_index=True)\n",
    "        df_natl_mk_intra = pd.concat([df_natl_mk_intra, df_mk_intra], ignore_index=True)\n",
    "        df_natl_mk_event_mag = pd.concat([df_natl_mk_event_mag, df_mk_event_mag], ignore_index=True)\n",
    "        df_natl_mk_event_dur = pd.concat([df_natl_mk_event_dur, df_mk_event_dur], ignore_index=True)\n",
    "        df_natl_mk_timing = pd.concat([df_natl_mk_timing, df_mk_timing], ignore_index=True)\n",
    "        \n",
    "df_natl_metrics = fn.gages_2_filtering(df_natl_metrics)\n",
    "\n",
    "# Create national site blacklist\n",
    "'''try:\n",
    "    with open(f'Prelim_Data/National_Sites_Blacklist.txt', 'w') as f:\n",
    "        natl_blacklist = [\"'\" + str(x) + \"'\" for x in natl_blacklist]        \n",
    "        f.write(', '.join(natl_blacklist))\n",
    "except Exception as e:\n",
    "    print(e)'''\n",
    "\n",
    "try:    \n",
    "    fn.save_data(df_natl_metrics, df_natl_mk_mag, df_natl_mk_dur, df_natl_mk_intra, df_natl_mk_event_mag, df_natl_mk_event_dur, df_natl_mk_timing, dataset_name)\n",
    "except Exception as e:\n",
    "    print(f'Error saving data: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: Outlet Gauges Dataset\n",
    "Uses <code>outlet_gauges.xlsx</code> to generate a custom dataset with only outlet gauges present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlets = pd.read_excel('Prelim_Data/_Outlet_Gauges/outlet_gauges.xlsx', dtype={'site_no': str})\n",
    "test_limit = 999\n",
    "#print(df_outlets.duplicated(subset=['site_no']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on site 09519800---]\n",
      "[---Working on site 09521100---]\n",
      "[---Working on site 11303500---]\n",
      "[---Working on site 11447650---]\n",
      "[---Working on site 14105700---]\n",
      "[---Working on site 07316000---]\n",
      "[---Working on site 02330000---]\n",
      "[---Working on site 02320500---]\n",
      "[---Working on site 02236000---]\n",
      "[---Working on site 02296750---]\n",
      "[---Working on site 02292900---]\n",
      "[---Working on site 02198500---]\n",
      "[---Working on site 02169500---]\n",
      "[---Working on site 02129000---]\n",
      "[---Working on site 13269000---]\n",
      "[---Working on site 08114000---]\n",
      "[---Working on site 08033500---]\n",
      "[---Working on site 08066500---]\n",
      "[---Working on site 08068000---]\n",
      "[---Working on site 08116650---]\n",
      "[---Working on site 08162000---]\n",
      "[---Working on site 08176500---]\n",
      "[---Working on site 08211000---]\n",
      "[---Working on site 08164000---]\n",
      "[---Working on site 08188500---]\n",
      "[---Working on site 08030500---]\n",
      "[---Working on site 08013500---]\n",
      "[---Working on site 07378500---]\n",
      "[---Working on site 07375000---]\n",
      "[---Working on site 02489500---]\n",
      "[---Working on site 02479000---]\n",
      "[---Working on site 02469761---]\n",
      "[---Working on site 07029500---]\n",
      "[---Working on site 07077000---]\n",
      "[---Working on site 07268000---]\n",
      "[---Working on site 07362000---]\n",
      "[---Working on site 07369000---]\n",
      "[---Working on site 07290000---]\n",
      "[---Working on site 07074500---]\n",
      "[---Working on site 07337000---]\n",
      "[---Working on site 09180500---]\n",
      "[---Working on site 09315000---]\n",
      "[---Working on site 09355500---]\n"
     ]
    }
   ],
   "source": [
    "df_outlet_metrics = pd.DataFrame()\n",
    "df_outlet_mk_mag = pd.DataFrame()\n",
    "df_outlet_mk_dur = pd.DataFrame()\n",
    "df_outlet_mk_intra = pd.DataFrame()\n",
    "\n",
    "for i, row in df_outlets.iterrows():\n",
    "    if i >= test_limit: break\n",
    "    print(f'[---Working on site {row[\"site_no\"]}---]')\n",
    "    df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    if '00060_radar sensor_Mean' in df.columns and '00060_Mean' not in df.columns:\n",
    "        df.rename(columns={'00060_radar sensor_Mean': '00060_Mean'}, inplace=True)\n",
    "    \n",
    "    start = df['datetime'].min().date()\n",
    "    end = df['datetime'].max().date()\n",
    "    range = round((end - start).days / 365.25, 1)\n",
    "    \n",
    "    aquifer = row['aquifer']\n",
    "    \n",
    "    try:\n",
    "        df_single_site_metric, df_mk_mag, df_mk_dur, df_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "        add_data = {'data_start': start, 'data_end': end, 'total_record': range, 'aquifer': aquifer}\t\t\t\n",
    "        add_data = pd.DataFrame(add_data, index=['0'])\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Single site data failure for site {row['site_no']}:\\n{e}\")\n",
    "        continue\n",
    "    \n",
    "    add_data = pd.DataFrame(np.tile(add_data.values, (len(df_single_site_metric), 1)), columns=add_data.columns)\n",
    "    df_single_site_metric = pd.concat([df_single_site_metric.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "    df_mk_mag = pd.concat([df_mk_mag.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "    df_mk_dur = pd.concat([df_mk_dur.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "    df_mk_intra = pd.concat([df_mk_intra.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    df_outlet_metrics = pd.concat([df_outlet_metrics, df_single_site_metric], ignore_index=True)\n",
    "    df_outlet_mk_mag = pd.concat([df_outlet_mk_mag, df_mk_mag], ignore_index=True)\n",
    "    df_outlet_mk_dur = pd.concat([df_outlet_mk_dur, df_mk_dur], ignore_index=True)\n",
    "    df_outlet_mk_intra = pd.concat([df_outlet_mk_intra, df_mk_intra], ignore_index=True)\n",
    "    \n",
    "df_outlet_metrics = fn.gages_2_filtering(df_outlet_metrics)\n",
    "\n",
    "try:\n",
    "    fn.save_data(df_outlet_metrics, df_outlet_mk_mag, df_outlet_mk_dur, df_outlet_mk_intra, 'Outlet_Metrics')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: Nationwide Validity Dataset\n",
    "Generates a dataset for every water gauge with 00060_Mean data in the contiguous US indicating gauge validity for this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = 'ShapeFiles/Lower48/lower48.shp'\n",
    "test_state_list = ['ME', 'DE']\n",
    "test_limit = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on AL---]\n",
      "Total Sites: 177 in the state of AL\n",
      "Ignored 39 sites (22.03%)\n",
      "[---Working on AZ---]\n",
      "Total Sites: 218 in the state of AZ\n",
      "Ignored 11 sites (5.05%)\n",
      "[---Working on AR---]\n",
      "Total Sites: 139 in the state of AR\n",
      "Ignored 14 sites (10.07%)\n",
      "[---Working on CA---]\n",
      "Total Sites: 484 in the state of CA\n",
      "Ignored 56 sites (11.57%)\n",
      "[---Working on CO---]\n",
      "Total Sites: 348 in the state of CO\n",
      "Ignored 14 sites (4.02%)\n",
      "[---Working on CT---]\n",
      "Total Sites: 75 in the state of CT\n",
      "Ignored 5 sites (6.67%)\n",
      "[---Working on DE---]\n",
      "Total Sites: 36 in the state of DE\n",
      "Ignored 3 sites (8.33%)\n",
      "[---Working on FL---]\n",
      "Total Sites: 436 in the state of FL\n",
      "Ignored 36 sites (8.26%)\n",
      "[---Working on GA---]\n",
      "Total Sites: 308 in the state of GA\n",
      "Ignored 17 sites (5.52%)\n",
      "[---Working on ID---]\n",
      "Total Sites: 230 in the state of ID\n",
      "Ignored 10 sites (4.35%)\n",
      "[---Working on IL---]\n",
      "Total Sites: 193 in the state of IL\n",
      "Ignored 7 sites (3.63%)\n",
      "[---Working on IN---]\n",
      "Total Sites: 207 in the state of IN\n",
      "Ignored 22 sites (10.63%)\n",
      "[---Working on IA---]\n",
      "Total Sites: 143 in the state of IA\n",
      "Ignored 7 sites (4.9%)\n",
      "[---Working on KS---]\n",
      "Total Sites: 195 in the state of KS\n",
      "Ignored 5 sites (2.56%)\n",
      "[---Working on KY---]\n",
      "Total Sites: 170 in the state of KY\n",
      "Ignored 9 sites (5.29%)\n",
      "[---Working on LA---]\n",
      "Total Sites: 101 in the state of LA\n",
      "Ignored 29 sites (28.71%)\n",
      "[---Working on ME---]\n",
      "Total Sites: 68 in the state of ME\n",
      "Ignored 0 sites (0.0%)\n",
      "[---Working on MD---]\n",
      "Total Sites: 189 in the state of MD\n",
      "Ignored 8 sites (4.23%)\n",
      "[---Working on MA---]\n",
      "Total Sites: 144 in the state of MA\n",
      "Ignored 13 sites (9.03%)\n",
      "[---Working on MI---]\n",
      "Total Sites: 203 in the state of MI\n",
      "Ignored 20 sites (9.85%)\n",
      "[---Working on MN---]\n",
      "Total Sites: 130 in the state of MN\n",
      "Ignored 6 sites (4.62%)\n",
      "[---Working on MS---]\n",
      "Total Sites: 99 in the state of MS\n",
      "Ignored 16 sites (16.16%)\n",
      "[---Working on MO---]\n",
      "Total Sites: 244 in the state of MO\n",
      "Ignored 2 sites (0.82%)\n",
      "[---Working on MT---]\n",
      "Total Sites: 226 in the state of MT\n",
      "Ignored 7 sites (3.1%)\n",
      "[---Working on NE---]\n",
      "Total Sites: 120 in the state of NE\n",
      "Ignored 1 sites (0.83%)\n",
      "[---Working on NV---]\n",
      "Total Sites: 188 in the state of NV\n",
      "Ignored 10 sites (5.32%)\n",
      "[---Working on NH---]\n",
      "Total Sites: 101 in the state of NH\n",
      "Ignored 1 sites (0.99%)\n",
      "[---Working on NJ---]\n",
      "Total Sites: 141 in the state of NJ\n",
      "Ignored 10 sites (7.09%)\n",
      "[---Working on NM---]\n",
      "Total Sites: 174 in the state of NM\n",
      "Ignored 10 sites (5.75%)\n",
      "[---Working on NY---]\n",
      "Total Sites: 320 in the state of NY\n",
      "Ignored 15 sites (4.69%)\n",
      "[---Working on NC---]\n",
      "Total Sites: 241 in the state of NC\n",
      "Ignored 10 sites (4.15%)\n",
      "[---Working on ND---]\n",
      "Total Sites: 122 in the state of ND\n",
      "Ignored 8 sites (6.56%)\n",
      "[---Working on OH---]\n",
      "Total Sites: 223 in the state of OH\n",
      "Ignored 17 sites (7.62%)\n",
      "[---Working on OK---]\n",
      "Total Sites: 178 in the state of OK\n",
      "Ignored 8 sites (4.49%)\n",
      "[---Working on OR---]\n",
      "Total Sites: 213 in the state of OR\n",
      "Ignored 7 sites (3.29%)\n",
      "[---Working on PA---]\n",
      "Total Sites: 311 in the state of PA\n",
      "Ignored 6 sites (1.93%)\n",
      "[---Working on RI---]\n",
      "Total Sites: 34 in the state of RI\n",
      "Ignored 2 sites (5.88%)\n",
      "[---Working on SC---]\n",
      "Total Sites: 131 in the state of SC\n",
      "Ignored 13 sites (9.92%)\n",
      "[---Working on SD---]\n",
      "Total Sites: 122 in the state of SD\n",
      "Ignored 4 sites (3.28%)\n",
      "[---Working on TN---]\n",
      "Total Sites: 136 in the state of TN\n",
      "Ignored 15 sites (11.03%)\n",
      "[---Working on TX---]\n",
      "Total Sites: 598 in the state of TX\n",
      "Ignored 63 sites (10.54%)\n",
      "[---Working on UT---]\n",
      "Total Sites: 154 in the state of UT\n",
      "Ignored 5 sites (3.25%)\n",
      "[---Working on VT---]\n",
      "Total Sites: 98 in the state of VT\n",
      "Ignored 1 sites (1.02%)\n",
      "[---Working on VA---]\n",
      "Total Sites: 200 in the state of VA\n",
      "Ignored 8 sites (4.0%)\n",
      "[---Working on WA---]\n",
      "Total Sites: 266 in the state of WA\n",
      "Ignored 12 sites (4.51%)\n",
      "[---Working on WV---]\n",
      "Total Sites: 110 in the state of WV\n",
      "Ignored 2 sites (1.82%)\n",
      "[---Working on WI---]\n",
      "Total Sites: 220 in the state of WI\n",
      "Ignored 31 sites (14.09%)\n",
      "[---Working on WY---]\n",
      "Total Sites: 120 in the state of WY\n",
      "Ignored 4 sites (3.33%)\n"
     ]
    }
   ],
   "source": [
    "df_natl_validity = pd.DataFrame()\n",
    "\n",
    "for i, state in enumerate(fn.STATE_LIST):\n",
    "    if i >= test_limit: break\n",
    "    print(f'[---Working on {state}---]')\n",
    "    state_uri = fn.create_state_uri(state, fn.PARAM_CODE)\n",
    "    df_state_sites = filter_state_site(shapefile_path, state_uri)\n",
    "    print(f'Total Sites: {len(df_state_sites)} in the state of {state}')\n",
    "    \n",
    "    ignored_count = 0\n",
    "    for loc, row in df_state_sites.iterrows():\n",
    "        df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        if df.empty:\n",
    "            ignored_count += 1\n",
    "            continue\n",
    "        \n",
    "        if '00060_radar sensor_Mean' in df.columns and '00060_Mean' not in df.columns:\n",
    "            df.rename(columns={'00060_radar sensor_Mean': '00060_Mean'}, inplace=True)\n",
    "        \n",
    "        if '00060_Mean' not in df.columns:\n",
    "            ignored_count += 1\n",
    "            continue\n",
    "        \n",
    "        start = df['datetime'].min().date()\n",
    "        end = df['datetime'].max().date()\n",
    "        total_record = round((end - start).days / 365.25, 1)\n",
    "        \n",
    "        valid_range_30 = total_record > 30\n",
    "        valid_range_50 = total_record > 50\n",
    "        \n",
    "        date_threshold_30 = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * 30)\n",
    "        df_30 = df[df['datetime'].dt.date >= date_threshold_30]\n",
    "        missing_30 = fn.validate(df_30, date_threshold_30, fn.DEFAULT_END)\n",
    "        valid_30 = missing_30 < fn.MAX_MISSING_THRESHOLD\n",
    "        \n",
    "        date_threshold_50 = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * 50)\n",
    "        df_50 = df[df['datetime'].dt.date >= date_threshold_50]\n",
    "        missing_50 = fn.validate(df_50, date_threshold_50, fn.DEFAULT_END)\n",
    "        valid_50 = missing_50 < fn.MAX_MISSING_THRESHOLD\n",
    "        \n",
    "        data = {'site_no': str(row['site_no']), 'state': state, 'date_range_30': valid_range_30, 'data_cont_30': valid_30, 'missing_30': missing_30,\n",
    "                'date_range_50': valid_range_50, 'data_cont_50': valid_50, 'missing_50': missing_50, 'start': start, 'end': end, \n",
    "                'total_record': total_record, 'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va']}\n",
    "        \n",
    "        df_natl_validity = pd.concat([df_natl_validity, pd.DataFrame(data, index=['0'])], axis=0, ignore_index=True)\n",
    "        \n",
    "    print(f'Ignored {ignored_count} sites ({round((ignored_count / len(df_state_sites)) * 100, 2)}%)')        \n",
    "        \n",
    "try:\n",
    "    with pd.ExcelWriter('Prelim_Data/National_Validity.xlsx') as writer:\n",
    "        df_natl_validity.to_excel(writer, sheet_name='national_validity', index=False) \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: HUC2/HUC4/Aquifer Master Shapefile Creation\n",
    "The following code combines HUC2, HUC4, and Aquifer shapefiles into master shapefiles, respectively. It's not necessary to run if these files already exist within the ShapeFiles directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc2_path = 'ShapeFiles/HUC2'\n",
    "huc4_path = 'ShapeFiles/HUC4'\n",
    "aquifer_path = 'ShapeFiles/Aquifers'\n",
    "aq_ext = '.shp'\n",
    "huc2 = 'WBDHU2.shp'\n",
    "huc4 = 'WBDHU4.shp'\n",
    "\n",
    "create_huc2 = False\n",
    "create_huc4 = False\n",
    "create_aquifer = True\n",
    "\n",
    "huc2_gdf = gpd.GeoDataFrame()\n",
    "huc4_gdf = gpd.GeoDataFrame()\n",
    "aquifer_df = gpd.GeoDataFrame()\n",
    "\n",
    "# HUC2's\n",
    "if create_huc2:\n",
    "    for root, dirs, files in os.walk(huc2_path):\n",
    "        if os.path.basename(root).startswith('WBD_'):\n",
    "            code = root[-2:]\n",
    "            if huc2 in files:\n",
    "                shape = gpd.read_file(os.path.join(root, huc2))\n",
    "                shape['huc2_code'] = code\n",
    "                huc2_gdf = pd.concat([huc2_gdf, shape], ignore_index=True)\n",
    "\n",
    "    huc2_gdf = huc2_gdf.to_crs(4269)\n",
    "    huc2_gdf.to_file('ShapeFiles/HUC2/_Master_HUC2/master_huc2.shp')\n",
    "\n",
    "# HUC4's    \n",
    "if create_huc4:\n",
    "    for root, dirs, files in os.walk(huc4_path):\n",
    "        if os.path.basename(root).startswith('NHD_H_'):\n",
    "            code = root[-4:]\n",
    "            if huc4 in files:\n",
    "                shape = gpd.read_file(os.path.join(root, huc4))\n",
    "                shape['huc4_code'] = code\n",
    "                huc4_gdf = pd.concat([huc4_gdf, shape], ignore_index=True)\n",
    "\n",
    "    huc4_gdf = huc4_gdf.to_crs(4269)\n",
    "    huc4_gdf.to_file('ShapeFiles/HUC4/_Master_HUC4/master_huc4.shp')\n",
    "    \n",
    "# Aquifers\n",
    "if create_aquifer:\n",
    "    for root, dirs, files in os.walk(aquifer_path):\n",
    "        for file in files:\n",
    "            if file.endswith(aq_ext) and dirs:\n",
    "                if file.startswith('Penn'):\n",
    "                    shape = gpd.read_file(os.path.join(root, file))\n",
    "                    shape = shape.iloc[[17]]                    \n",
    "                else:\n",
    "                    shape = gpd.read_file(os.path.join(root, file))\n",
    "                    \n",
    "                shape = shape.to_crs(4269)\n",
    "                shape['aq_name'] = file[:-4]\n",
    "                aquifer_df = pd.concat([aquifer_df, shape], ignore_index=True)\n",
    "                \n",
    "    aquifer_df = aquifer_df.to_crs(4269)\n",
    "    aquifer_df.to_file('ShapeFiles/Aquifers/_Master_Aquifer/master_aquifer.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: HUC2/4/Aquifer Sorting\n",
    "Takes a national validity or national metrics dataset (any dataset with dec_lat_va and dec_long_va data), and adds corresponding HUC2, HUC4, and Aquifer columns indicating a water guages presence or lack thereof in each of these boundaries.\n",
    "\n",
    "<strong>NOTE:</strong> This feature has already been implemented directly into the National Metrics dataset creation. Its only real use is on National Validity datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Prelim_Data/_National_Metrics/'\n",
    "datasets = ['National_Metrics_30_90_TEST.xlsx']\n",
    "huc2_gdf = gpd.read_file('ShapeFiles/HUC2/_Master_HUC2/master_huc2.shp')\n",
    "huc4_gdf = gpd.read_file('ShapeFiles/HUC4/_Master_HUC4/master_huc4.shp')\n",
    "aq_gdf = gpd.read_file('ShapeFiles/Aquifers/Conterm_US/master_aquifer.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    temp = pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_excel(f'{path}{dataset}', dtype={'site_no': str})\n",
    "    sheets = pd.ExcelFile(f'{path}{dataset}')\n",
    "    sheet = sheets.sheet_names[0]          \n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        add_data = pd.DataFrame({'site_no': str(row['site_no']), 'huc2_code': 'NA', 'huc4_code': 'NA', 'aquifer': 'NA'}, index=['0'])\n",
    "        point = Point(row['dec_long_va'], row['dec_lat_va'])\n",
    "        \n",
    "        # HUC2 assignment\n",
    "        for j, geo_row in huc2_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                add_data[\"huc2_code\"] = geo_row['huc2_code']\n",
    "                continue\n",
    "            \n",
    "        # HUC4 assignment\n",
    "        for j, geo_row in huc4_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                add_data[\"huc4_code\"] = geo_row['huc4_code']\n",
    "                continue\n",
    "            \n",
    "        # Aquifer Assignment\n",
    "        for j, geo_row in aq_gdf.iterrows():\n",
    "            if geo_row['geometry'].contains(point):\n",
    "                add_data[\"aquifer\"] = geo_row['aq_name']\n",
    "                continue\n",
    "            \n",
    "        temp = pd.concat([temp, add_data], axis=0, ignore_index=True)\n",
    "    \n",
    "    df = pd.merge(df, temp, on='site_no', validate='1:1')\n",
    "    \n",
    "    with pd.ExcelWriter(f'{path}{dataset}', mode='a', if_sheet_exists='replace') as writer:\n",
    "        df.to_excel(writer, sheet_name=sheet, index=False)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: National Dataset Splitting by Aquifer\n",
    "This script splits a national metric dataset or datasets into smaller per-aquifer datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "natl_path = 'Prelim_Data/_National_Metrics'\n",
    "\n",
    "# National datasets to split\n",
    "datasets = ['National_Metrics_30_90.xlsx', 'National_Metrics_30_95.xlsx', 'National_Metrics_50_90.xlsx', 'National_Metrics_50_95.xlsx']\n",
    "# The datasets to generate from the national dataset\n",
    "target_aquifers = cl.ALL_AQUIFERS\n",
    "sheet_names = ['site_metrics', 'mk_magnitude', 'mk_duration', 'mk_intra_annual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over national datasets\n",
    "for dataset in datasets:\n",
    "    df_list = []\n",
    "    for sheet in sheet_names:\n",
    "        df = pd.read_excel(f'{natl_path}/{dataset}', sheet_name=sheet, dtype=fn.DATASET_DTYPES)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Iterate over target aquifers\n",
    "    for aquifer in target_aquifers:\n",
    "        save_path = f\"{aquifer.datasets_dir}/{aquifer.name}_{dataset[-10:-8]}_{dataset[-7:-5]}.xlsx\"\n",
    "        for i, df in enumerate(df_list):\n",
    "            df = df[df['huc4_code'].isin(aquifer.huc4s)]\n",
    "            \n",
    "            # Append new sheets to existing file\n",
    "            if os.path.exists(save_path):\n",
    "                with pd.ExcelWriter(save_path, mode='a', if_sheet_exists='replace') as writer:\n",
    "                    df.to_excel(writer, sheet_name=sheet_names[i], index=False)\n",
    "            # Create file if it doesn't exist (first iteration)                        \n",
    "            else: \n",
    "                with pd.ExcelWriter(save_path, mode='w') as writer:\n",
    "                    df.to_excel(writer, sheet_name=sheet_names[i], index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
