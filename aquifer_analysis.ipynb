{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES:\n",
    "<ul>\n",
    "<li>Sections labeled <strong>CORE</strong> must be run in order to begin Aquifer Analysis</li>\n",
    "<li>Cells labeled <strong>Control</strong> contain inputs for the immeadiately proceeding section(s)</li>\n",
    "<li>Sections labeled <strong>EXTRA</strong> contain additional plotting or analysis tools but are not necessary for Aquifer Analysis</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nadp.py:44: UserWarning: GDAL not installed. Some functions will not work.\n",
      "  warnings.warn('GDAL not installed. Some functions will not work.')\n"
     ]
    }
   ],
   "source": [
    "#Python3.10\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import contextily as cx\n",
    "import requests\n",
    "import calendar\n",
    "from importlib import reload\n",
    "from typing import IO\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import Point\n",
    "from io import StringIO\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# USGS Data retreival tool\n",
    "from dataretrieval import nwis, utils, codes\n",
    "\n",
    "# Custom modules are imported in multiple locations to faciliate easy reloading when edits are made to their respective files\n",
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "reload(fn)\n",
    "\n",
    "# TODO: Look into the warning that this is disabling. It doesn't appear to be significant for the purposes of this code but should be understood\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#pd.options.mode.chained_assignment = 'warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Single Site Data<br>\n",
    "This function produces the streamflow analysis (using the functions above) for a single site given a list of thresholds and time windows to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "\n",
    "# df2 holds all-time data, df is analyzed range\n",
    "curr_guage = cl.SRB_Guage\n",
    "df = nwis.get_record(sites='09525000', service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2020-09-30')\n",
    "df2 = nwis.get_record(sites=curr_guage.id, service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2014-09-30')\n",
    "\n",
    "# Set to true if running indepedently to verify data with Kocis paper (or other source)\n",
    "# Set to false if running Aquifer Analysis\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Site Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site No: 0    09525000\n",
      "1    09525000\n",
      "2    09525000\n",
      "0    09525000\n",
      "Name: site_no, dtype: object\n",
      "Analyzed Range: 29.998631\n",
      "29.998631\n",
      "49.998631\n",
      "49.998631\n",
      "Quantile: 0.90\n",
      "0.95\n",
      "0.90\n",
      "0.95\n",
      "Valid: True\n",
      "True\n",
      "True\n",
      "True\n",
      "% Missing: 0\n",
      "0\n",
      "0\n",
      "0\n",
      "90%: 653.00\n",
      "770.15\n",
      "642.00\n",
      "754.00\n",
      "HMF Years: 30\n",
      "28\n",
      "45\n",
      "43\n",
      "Annual Duration: 36.433333\n",
      "19.571429\n",
      "40.466667\n",
      "21.116279\n",
      "Event Duration: 4.329896\n",
      "3.530450\n",
      "4.513132\n",
      "3.705451\n",
      "Event HMF: 0.001668\n",
      "0.001083\n",
      "0.001617\n",
      "0.001051\n",
      "Inter-annual Frequency: 1.000000\n",
      "0.933333\n",
      "0.900000\n",
      "0.860000%\n",
      "Intra-annual Frequency: 8.100000\n",
      "5.285714\n",
      "8.933333\n",
      "5.930233\n",
      "Total HMF in km^3/year: 0.013801\n",
      "0.006657\n",
      "0.014599\n",
      "0.006730\n",
      "Center of Mass: 184.700000\n",
      "169.000000\n",
      "177.844444\n",
      "169.604651\n",
      "6 Month HMF in km^3/year: 0.009458\n",
      "0.004924\n",
      "0.009234\n",
      "0.004595\n",
      "3 Month HMF in km^3/year: 0.007401\n",
      "0.004150\n",
      "0.007141\n",
      "0.003742\n"
     ]
    }
   ],
   "source": [
    "# If looking at a post impairment period, but calculating threshold based on the full record of data, pass a second dataframe with a different \n",
    "# start/end date as the final parameter. This method was used in Kocis 2017 and is needed for some data verification, but is not the methodology\n",
    "# used for the Aquifer Analysis and so *args will most often be empty.\n",
    "import Src.func as fn\n",
    "reload(fn)\n",
    "\n",
    "def single_site_data(df: pd.DataFrame, quantiles_list: list, data_ranges_list: list, *args):\n",
    "    df = df.reset_index()    \n",
    "    threshold = None\n",
    "    df_complete_site_data = pd.DataFrame()    \n",
    "    df_complete_mk_mag = pd.DataFrame()\n",
    "    df_complete_mk_dur = pd.DataFrame()\n",
    "    df_complete_mk_intra = pd.DataFrame()\n",
    "    \n",
    "    for data_range in data_ranges_list:\n",
    "        for quantile in quantiles_list:\n",
    "            # Copying original dataframe to avoid any conflicts\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Validate that site is not missing > 10% of data\n",
    "            date_threshold = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * data_range)              \n",
    "            \n",
    "            # Filter dataframe based on current data_range (don't do this during testing if testing unique dataset/range)\n",
    "            if not testing:\n",
    "                df_copy = df_copy[df_copy['datetime'].dt.date >= date_threshold]\n",
    "            \n",
    "            # Validate <10% missing data based on filtered dataframe\n",
    "            missing = fn.validate(df_copy, date_threshold, fn.DEFAULT_END)\n",
    "            valid = missing < fn.MAX_MISSING_THRESHOLD                               \n",
    "    \n",
    "            # Check for optional second dataframe containing full record\n",
    "            if args and isinstance(args[0], pd.DataFrame) and not args[0].empty:\n",
    "                #print('Threshold calculation across the full record')\n",
    "                df2 = args[0].reset_index()\n",
    "                threshold = fn.calc_threshold(df2, quantile)\n",
    "                # threshold = 52350 # SRB_Guage post-impairment threshold for verification with Kocis_2017 \n",
    "            else:\n",
    "                #print('Threshold calculation across a limited record')\n",
    "                threshold = fn.calc_threshold(df_copy, quantile)                             \n",
    "\n",
    "            # Create a dataframe with only days over HMF threshold as well as continuous dataframe for MK trend test\n",
    "            hmf_series_defl, hmf_series_cont = fn.filter_hmf(df_copy, threshold)\n",
    "            #print(zero_deflated_hmf)\n",
    "            \n",
    "            # Aggregate data before performing MK magnitude test\n",
    "            df_agg_cont = hmf_series_cont.copy()\n",
    "            df_agg_cont = fn.convert_hmf(df_agg_cont, threshold)\n",
    "            df_agg_cont['00060_Mean'] = df_agg_cont['00060_Mean'].apply(lambda x: x * fn.CUBIC_FT_KM_FACTOR if x >= 0 else 0)\n",
    "            df_agg_cont.set_index('datetime', inplace=True)\n",
    "            df_agg_cont = df_agg_cont.resample(fn.HYDRO_YEAR).agg({'00060_Mean': ['sum', 'count']})\n",
    "            df_agg_cont.columns = ['Sum', 'Count']\n",
    "            df_agg_cont_defl = df_agg_cont[df_agg_cont['Sum'] > 0]          \n",
    "            \n",
    "            # MK Magnitude\n",
    "            df_mk_mag = fn.mann_kendall(df_agg_cont_defl['Sum'], df_agg_cont['Sum'], fn.MK_TREND_ALPHA)\n",
    "\n",
    "            # Find number of years with HMF\n",
    "            hmf_years = fn.num_hmf_years(hmf_series_defl)            \n",
    "\n",
    "            # Mask out months that don't fall within 3 and 6 month Winter range\n",
    "            df_six_month, df_three_month = fn.three_six_range(hmf_series_defl, 12, 2, 11, 4)\n",
    "\n",
    "            # Convert to daily average flow in cfpd, and take only flow above the threshold\n",
    "            hmf_series_defl = fn.convert_hmf(hmf_series_defl, threshold)\n",
    "            total_hmf_flow = hmf_series_defl[\"00060_Mean\"].sum()\n",
    "            hmf_per_month = fn.monthly_hmf(hmf_series_defl, data_range, quantile)\n",
    "\n",
    "            # Calculate 3 and 6 month HMF\n",
    "            df_six_month = fn.convert_hmf(df_six_month, threshold)\n",
    "            six_month_hmf = df_six_month[\"00060_Mean\"].sum()\n",
    "            df_three_month = fn.convert_hmf(df_three_month, threshold)\n",
    "            three_month_hmf = df_three_month[\"00060_Mean\"].sum()\n",
    "            \n",
    "            total_hmf_flow = (total_hmf_flow * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            six_month_hmf = (six_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            three_month_hmf = (three_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "\n",
    "            # Inter-annual\n",
    "            inter_annual, delta = fn.calc_inter_annual(hmf_series_cont, hmf_years)            \n",
    "\n",
    "            # Average Duration and Intra-annual Frequency\n",
    "            hmf_series_cont = fn.convert_hmf(hmf_series_cont, threshold)\n",
    "            event_duration, annual_duration, intra_annual, event_hmf, df_results = fn.calc_duration_intra_annual(hmf_series_cont, hmf_years)\n",
    "            dur_series_defl = df_results['duration'][df_results['duration'] > 0]\n",
    "            dur_series_cont = df_results['duration']\n",
    "            df_mk_dur = fn.mann_kendall(dur_series_defl, dur_series_cont, fn.MK_TREND_ALPHA)\n",
    "                        \n",
    "            intra_series_defl = df_results['total_events'][df_results['total_events'] > 0]\n",
    "            intra_series_cont = df_results['total_events']\n",
    "            df_mk_intra = fn.mann_kendall(intra_series_defl, intra_series_cont, fn.MK_TREND_ALPHA) \n",
    "            \n",
    "            # Timing Calculation using DOHY\n",
    "            timing = fn.calc_timing(hmf_series_defl)           \n",
    "\n",
    "            # TODO: One-day peaks (avg. # of times hmf occurs on one day only)          \n",
    "            \n",
    "            # Merging site dataframe with Mann-Kendall dataframe. This start date is the beginning of the actual data, not necessarily \n",
    "            # the beginning of the analyzed range. Validation (above) starts from the start of the official range (1970/90-2020)\n",
    "            start = df_copy['datetime'].min().date()\n",
    "            end = df_copy['datetime'].max().date()\n",
    "            data = {'dataset_ID': (data_range * quantile), 'site_no': df_copy.iloc[0]['site_no'], 'analyze_start': start, 'analyze_end': end, 'analyze_range': delta, 'quantile': quantile, \n",
    "                    'valid': valid, 'missing': missing, 'threshold': threshold, 'hmf_years': hmf_years, 'annual_hmf': total_hmf_flow, 'six_mo_hmf': six_month_hmf, 'three_mo_hmf': three_month_hmf, \n",
    "                    'annual_duration': annual_duration, 'event_duration': event_duration, 'event_hmf': event_hmf, 'inter_annual': inter_annual, 'intra_annual': intra_annual, 'timing': timing}              \n",
    "            \n",
    "            # Merging MK magnitiude\n",
    "            df_mk_mag.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_mag.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_mag = pd.concat([df_complete_mk_mag.reset_index(drop=True), df_mk_mag.reset_index(drop=True)], axis=0) \n",
    "            \n",
    "            # Merging MK duration\n",
    "            df_mk_dur.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_dur.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_dur = pd.concat([df_complete_mk_dur.reset_index(drop=True), df_mk_dur.reset_index(drop=True)], axis=0)   \n",
    "            \n",
    "            # Merging MK intra-annual\n",
    "            df_mk_intra.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_intra.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_intra = pd.concat([df_complete_mk_intra.reset_index(drop=True), df_mk_intra.reset_index(drop=True)], axis=0)        \n",
    "            \n",
    "            # Merging metric results\n",
    "            df_single_iteration = pd.DataFrame(data, index=['0'])\n",
    "            df_single_iteration = pd.concat([df_single_iteration.reset_index(drop=True), hmf_per_month.reset_index(drop=True)], axis=1)\n",
    "            df_complete_site_data = pd.concat([df_complete_site_data.reset_index(drop=True), df_single_iteration.reset_index(drop=True)], axis=0)\n",
    "        \n",
    "    return df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra\n",
    "\n",
    "# For testing purposes, to run this cell independently\n",
    "df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "\n",
    "'''\n",
    "try:\n",
    "    with pd.ExcelWriter('df_single_site.xlsx') as writer:\n",
    "        df_complete_site_data.to_excel(writer, sheet_name='site_metrics', index=False)\n",
    "        df_complete_mk_mag.to_excel(writer, sheet_name='mk_magnitude', index=False)\n",
    "        df_complete_mk_intra.to_excel(writer, sheet_name='mk_intra', index=False)\n",
    "        df_complete_mk_dur.to_excel(writer, sheet_name='mk_duration', index=False)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "'''\n",
    "   \n",
    "fn.single_site_report(df_complete_site_data)\n",
    "df_complete_site_data = fn.gages_2_filtering(df_complete_site_data)\n",
    "#fn.save_data(df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra, 'TEST')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Multi-Site Filtering<br>\n",
    "This function creates the list of sites to analyze by filtering the complete list of state sites with 00060_Mean data down to those that lie within a specific watershed boundary using decimal long/lat positional data and a region shapefile (e.g. state or watershed boundary)<br><br>\n",
    "\n",
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed when running the following cell independently\n",
    "shapefile_path = 'ShapeFiles/Aquifers/Central_Valley/HUC4/NHD_H_1802'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site List Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sites: 57 in the state of CA in the given WB\n",
      "['11337080', '11342000', '11345500', '11348500', '11355010', '11355500', '11370500', '11370700', '11372000', '11374000', '11376000', '11376550', '11377100', '11379500', '11381500', '11383500', '11389500', '11390000', '11390500', '11401920', '11402000', '11413000', '11418500', '11421000', '11424000', '11425500', '11427000', '11446500', '11447360', '11447650', '11447830', '11447850', '11447890', '11447905', '11448750', '11448800', '11449255', '11449500', '11451000', '11451100', '11451300', '11451715', '11451800', '11452500', '11452800', '11452900', '11453000', '11453500', '11453590', '11454000', '11455095', '11455140', '11455280', '11455315', '11455338', '11455385', '11455420']\n"
     ]
    }
   ],
   "source": [
    "def filter_state_site(shapefile_path: str, state_uri: str):\n",
    "    \"\"\"Creates a list of sites with over 50 years of 00060_Mean streamflow data for a given region\"\"\"\n",
    "    # Request page from USGS site, ignore all informational lines\n",
    "    response = requests.get(state_uri)\n",
    "    data = response.text\n",
    "    lines = data.splitlines()\n",
    "    lines = [line for line in lines if not line.startswith('#')]\n",
    "\n",
    "    # Create dataframe where site_no is a list of all sites in a state with 00060 data\n",
    "    tsd = \"\\n\".join(lines)\n",
    "    df = pd.read_csv(StringIO(tsd), sep='\\t')\n",
    "    df_state_sites = df.iloc[1:]\n",
    "        \n",
    "    # Filter out sites outside of HU boundary\n",
    "    if fn.SORT_BY_WB:\n",
    "        shapefile = gpd.read_file(shapefile_path)\n",
    "        df_state_sites['geometry'] = [Point(lon, lat) for lon, lat in zip(df_state_sites['dec_long_va'], df_state_sites['dec_lat_va'])]\n",
    "        gdf_data = gpd.GeoDataFrame(df_state_sites, crs=shapefile.crs)\n",
    "        df_state_sites = gpd.sjoin(gdf_data, shapefile, predicate='within')\n",
    "            \n",
    "    #print(df_state_sites.columns.to_list())\n",
    "    #print(df_state_sites)\n",
    "    \n",
    "    return df_state_sites\n",
    "\n",
    "df_state_sites = filter_state_site(shapefile_path, fn.SITES_URI)\n",
    "print(f'Total Sites: {len(df_state_sites)} in the state of {fn.STATE_CODE.upper()} in the given WB')\n",
    "site_list = df_state_sites['site_no'].to_list()\n",
    "print(site_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Multi-Site Data Creation<br>\n",
    "This function uses the list generated by the previous section to generate single site data for every site in the list, and validate that no site is missing more than 10% of its data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGNORED: Site 11337080 is in aquifer blacklist\n",
      "Added site 2 of 57\n",
      "Added site 3 of 57\n",
      "Max HMF for this region: 0.2\n",
      "8 site(s) valid out of 8\n"
     ]
    }
   ],
   "source": [
    "# REQUIRES: 'df_state_sites' from 'Multi-Site Filtering'\n",
    "# Used for now to limit runtime when running independently\n",
    "site_limit = 3\n",
    "\n",
    "def create_multi_site_data(df_state_sites: pd.DataFrame, site_limit: int, aquifer: cl.Aquifer):\n",
    "\t\"\"\"Generates detailed HMF, MK, and POS information for each site in the passed dataframe\"\"\"\n",
    "\t# Necessary for proper iterrows() behavior\n",
    "\tdf_state_sites.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\t# Creating the dataframe that will hold final results for mapping\n",
    "\tdf_multi_site_metric = pd.DataFrame()\n",
    "\tdf_multi_site_mk_mag = pd.DataFrame()\n",
    "\tdf_multi_site_mk_dur = pd.DataFrame()\n",
    "\tdf_multi_site_mk_intra = pd.DataFrame()\n",
    " \n",
    "\tsite_blacklist = []\n",
    "\n",
    "\tfor index, row in df_state_sites.iterrows():\n",
    "\t\twhile index < site_limit:\n",
    "      \n",
    "\t\t\tif str(row['site_no']) in aquifer.blacklist:\n",
    "\t\t\t\tprint(f'IGNORED: Site {row[\"site_no\"]} is in aquifer blacklist')\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\t# Create a dataframe for the current site in the iteration to perform calculations on\n",
    "\t\t\tdf = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "\t\t\tdf = df.reset_index()\n",
    "\t\t\t\n",
    "\t\t\t# Confirm that dataframe is not empty and has the required streamflow data before continuing\n",
    "\t\t\tif df.empty:\n",
    "\t\t\t\tsite_blacklist.append(row['site_no'])\n",
    "\t\t\t\tprint(f'IGNORED: No data for site {row[\"site_no\"]}')\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tif '00060_Mean' not in df.columns:\n",
    "\t\t\t\tsite_blacklist.append(row['site_no'])\n",
    "\t\t\t\tprint(f'IGNORED: No 00060_Mean data for site {row[\"site_no\"]}')\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\t# Filter out sites with less than the minimum required range of data\n",
    "\t\t\tstart = df['datetime'].min().date()\n",
    "\t\t\tend = df['datetime'].max().date()\n",
    "\t\t\trange = round((end - start).days / 365.25, 1)\n",
    "\t\t\t\n",
    "\t\t\t# Ignore sites with less than the minimum required years of data\n",
    "\t\t\tif range < fn.MIN_DATA_PERIOD:\n",
    "\t\t\t\tsite_blacklist.append(row['site_no'])\n",
    "\t\t\t\tprint(f'IGNORED: Not enough data for site {row[\"site_no\"]}')\n",
    "\t\t\t\tbreak\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\tdf_single_site_metric, df_mk_mag, df_mk_dur, df_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\t\t\t\n",
    "\t\t\t\n",
    "\t\t\t# Append positional data to dataframe created by single_site_data()\n",
    "\t\t\tadd_data = {'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va'], 'data_start': start, 'data_end': end, 'total_record': range}\t\t\t\n",
    "\t\t\tadd_data = pd.DataFrame(add_data, index=['0'])\n",
    "   \n",
    "\t\t\t# Duplicates the rows of add_data so that positional information is passed to each individual dataframe when this frame is split\n",
    "\t\t\tadd_data = pd.DataFrame(np.tile(add_data.values, (len(df_single_site_metric), 1)), columns=add_data.columns)\n",
    "\t\t\tdf_single_site_metric = pd.concat([df_single_site_metric.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\t\t\t# Append single site data to multi-site dataframes\n",
    "\t\t\tdf_multi_site_metric = pd.concat([df_multi_site_metric, df_single_site_metric], ignore_index=True)\n",
    "\t\t\tdf_multi_site_mk_mag = pd.concat([df_multi_site_mk_mag, df_mk_mag], ignore_index=True)\n",
    "\t\t\tdf_multi_site_mk_dur = pd.concat([df_multi_site_mk_dur, df_mk_dur], ignore_index=True)\n",
    "\t\t\tdf_multi_site_mk_intra = pd.concat([df_multi_site_mk_intra, df_mk_intra], ignore_index=True)   \n",
    "   \t\t\n",
    "\t\t\tprint(f'Added site {index + 1} of {len(df_state_sites)}')\n",
    "\t\t\t\n",
    "\t\t\t#clear_output(wait=True)\n",
    "\t\t\t#time.sleep(0.500)\n",
    "\t\t\tbreak\n",
    "\n",
    "\treturn df_multi_site_metric, df_multi_site_mk_mag, df_multi_site_mk_dur, df_multi_site_mk_intra, site_blacklist\n",
    "\n",
    "\n",
    "df_multi_site_metric, df_multi_site_mk_mag, df_multi_site_mk_dur, df_multi_site_mk_intra, site_blacklist = create_multi_site_data(df_state_sites, site_limit, cl.central_valley_aquifer)\n",
    "df_multi_site, df_invalid_site = fn.filter_by_valid(df_multi_site_metric)\n",
    "#df_multi_site.to_csv('df_multi_site.csv')\n",
    "#print(df_multi_site)\n",
    "\n",
    "print(f'Max HMF for this region: {df_multi_site[\"annual_hmf\"].max():.1f}')\n",
    "print(f'{len(df_multi_site)} site(s) valid out of {len(df_multi_site_metric)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Aquifer Analysis\n",
    "This section generates aquifer-wide data based on all watershed boundary shapefiles that intersect with the aquifer boundary. In addition it contains a section to generate a map of this aquifer boundary, the intersecting watershed boundaries, and their relevant water gauge sites and annual high magnitude flows<br><br>\n",
    "Aquifer Analysis Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the aquifer to collect data for\n",
    "curr_aquifer = cl.arizona_alluvial_aquifer\n",
    "\n",
    "# Limit saved sites for testing purposes\n",
    "# Set to >=999 to analyze all sites\n",
    "site_limit = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Creation: \n",
    "<li>This code is time-consuming and can be bypassed if a spreadsheet for an aquifer already exists and all that is desired is a plot based on that data</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1501\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "Added site 1 of 11\n",
      "IGNORED: Site 09403850 is in aquifer blacklist\n",
      "IGNORED: Site 09404104 is in aquifer blacklist\n",
      "IGNORED: Site 09404115 is in aquifer blacklist\n",
      "IGNORED: Site 09404200 is in aquifer blacklist\n",
      "IGNORED: Site 09404208 is in aquifer blacklist\n",
      "IGNORED: Site 09404222 is in aquifer blacklist\n",
      "IGNORED: Site 09404343 is in aquifer blacklist\n",
      "IGNORED: Site 09413700 is in aquifer blacklist\n",
      "Added site 10 of 11\n",
      "Added site 11 of 11\n",
      "Trying:  CA\n",
      "Trying:  NM\n",
      "Trying:  UT\n",
      "IGNORED: Not enough data for site 09403600\n",
      "Added site 2 of 18\n",
      "IGNORED: Not enough data for site 09404900\n",
      "Added site 4 of 18\n",
      "Added site 5 of 18\n",
      "IGNORED: Not enough data for site 09406100\n",
      "Added site 7 of 18\n",
      "IGNORED: Not enough data for site 09408135\n",
      "Added site 9 of 18\n",
      "IGNORED: Not enough data for site 09408195\n",
      "Added site 11 of 18\n",
      "IGNORED: Not enough data for site 09409100\n",
      "IGNORED: Not enough data for site 09410100\n",
      "Added site 14 of 18\n",
      "IGNORED: Not enough data for site 09413200\n",
      "Added site 16 of 18\n",
      "IGNORED: Site 09413700 is in aquifer blacklist\n",
      "IGNORED: Not enough data for site 09413900\n",
      "Trying:  NV\n",
      "Added site 1 of 41\n",
      "IGNORED: Site 09404200 is in aquifer blacklist\n",
      "Added site 3 of 41\n",
      "IGNORED: Site 09413700 is in aquifer blacklist\n",
      "IGNORED: Not enough data for site 09413900\n",
      "Added site 6 of 41\n",
      "Added site 7 of 41\n",
      "IGNORED: Not enough data for site 09415510\n",
      "IGNORED: Not enough data for site 09415558\n",
      "IGNORED: Not enough data for site 09415645\n",
      "IGNORED: Not enough data for site 09415900\n",
      "IGNORED: Not enough data for site 09415908\n",
      "IGNORED: Not enough data for site 09415910\n",
      "IGNORED: Not enough data for site 09415915\n",
      "IGNORED: Not enough data for site 09415920\n",
      "IGNORED: Not enough data for site 09415927\n",
      "Added site 17 of 41\n",
      "Added site 18 of 41\n",
      "Added site 19 of 41\n",
      "IGNORED: Not enough data for site 09418700\n",
      "Added site 21 of 41\n",
      "IGNORED: Not enough data for site 09419530\n",
      "IGNORED: Not enough data for site 09419550\n",
      "IGNORED: Not enough data for site 09419625\n",
      "IGNORED: Not enough data for site 09419665\n",
      "IGNORED: Not enough data for site 094196781\n",
      "IGNORED: Not enough data for site 094196783\n",
      "IGNORED: Not enough data for site 094196784\n",
      "IGNORED: Not enough data for site 09419679\n",
      "IGNORED: Not enough data for site 09419696\n",
      "IGNORED: Not enough data for site 09419698\n",
      "IGNORED: Not enough data for site 09419700\n",
      "IGNORED: Not enough data for site 09419740\n",
      "IGNORED: Not enough data for site 09419745\n",
      "IGNORED: Not enough data for site 09419747\n",
      "IGNORED: Not enough data for site 09419749\n",
      "IGNORED: Not enough data for site 09419753\n",
      "IGNORED: Not enough data for site 09419756\n",
      "Added site 39 of 41\n",
      "IGNORED: No data for site 361600114163301\n",
      "IGNORED: No data for site 362734114124201\n",
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1503\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "Added site 1 of 49\n",
      "IGNORED: Site 09423560 is in aquifer blacklist\n",
      "IGNORED: Site 09424380 is in aquifer blacklist\n",
      "IGNORED: Site 09424447 is in aquifer blacklist\n",
      "Added site 5 of 49\n",
      "IGNORED: Site 09424580 is in aquifer blacklist\n",
      "IGNORED: Site 09424600 is in aquifer blacklist\n",
      "Added site 8 of 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added site 9 of 49\n",
      "IGNORED: Site 09426620 is in aquifer blacklist\n",
      "Added site 11 of 49\n",
      "Added site 12 of 49\n",
      "Added site 13 of 49\n",
      "Added site 14 of 49\n",
      "IGNORED: Site 09429030 is in aquifer blacklist\n",
      "IGNORED: Site 09429070 is in aquifer blacklist\n",
      "Added site 17 of 49\n",
      "Added site 18 of 49\n",
      "Added site 19 of 49\n",
      "Added site 20 of 49\n",
      "IGNORED: Site 09429500 is in aquifer blacklist\n",
      "IGNORED: Site 09429600 is in aquifer blacklist\n",
      "IGNORED: Site 09520900 is in aquifer blacklist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added site 24 of 49\n",
      "IGNORED: Site 09522400 is in aquifer blacklist\n",
      "Added site 26 of 49\n",
      "Added site 27 of 49\n",
      "Added site 28 of 49\n",
      "IGNORED: Site 09522860 is in aquifer blacklist\n",
      "IGNORED: Site 09522880 is in aquifer blacklist\n",
      "IGNORED: Site 09522900 is in aquifer blacklist\n",
      "Added site 32 of 49\n",
      "IGNORED: Site 09523200 is in aquifer blacklist\n",
      "Added site 34 of 49\n",
      "Added site 35 of 49\n",
      "Added site 36 of 49\n",
      "Added site 37 of 49\n",
      "IGNORED: Site 09524700 is in aquifer blacklist\n",
      "Added site 39 of 49\n",
      "Added site 40 of 49\n",
      "IGNORED: Site 09526000 is in aquifer blacklist\n",
      "IGNORED: Site 09526200 is in aquifer blacklist\n",
      "IGNORED: Site 09528200 is in aquifer blacklist\n",
      "Added site 44 of 49\n",
      "IGNORED: Site 09529000 is in aquifer blacklist\n",
      "Added site 46 of 49\n",
      "Added site 47 of 49\n",
      "IGNORED: Site 09530100 is in aquifer blacklist\n",
      "IGNORED: Site 09530500 is in aquifer blacklist\n",
      "Trying:  CA\n",
      "Added site 1 of 20\n",
      "Added site 2 of 20\n",
      "Added site 3 of 20\n",
      "Added site 4 of 20\n",
      "Added site 5 of 20\n",
      "Added site 6 of 20\n",
      "IGNORED: Site 09429500 is in aquifer blacklist\n",
      "IGNORED: Site 09429600 is in aquifer blacklist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added site 9 of 20\n",
      "Added site 10 of 20\n",
      "IGNORED: Site 09523200 is in aquifer blacklist\n",
      "Added site 12 of 20\n",
      "Added site 13 of 20\n",
      "Added site 14 of 20\n",
      "Added site 15 of 20\n",
      "IGNORED: Site 09524700 is in aquifer blacklist\n",
      "Added site 17 of 20\n",
      "IGNORED: Site 09526200 is in aquifer blacklist\n",
      "Added site 19 of 20\n",
      "IGNORED: Site 09530500 is in aquifer blacklist\n",
      "Trying:  NM\n",
      "Trying:  UT\n",
      "Trying:  NV\n",
      "Added site 1 of 1\n",
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1504\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "Added site 1 of 15\n",
      "Added site 2 of 15\n",
      "IGNORED: Site 09439000 is in aquifer blacklist\n",
      "Added site 4 of 15\n",
      "Added site 5 of 15\n",
      "Added site 6 of 15\n",
      "Added site 7 of 15\n",
      "IGNORED: Site 09446320 is in aquifer blacklist\n",
      "Added site 9 of 15\n",
      "Added site 10 of 15\n",
      "IGNORED: Site 09447800 is in aquifer blacklist\n",
      "Added site 12 of 15\n",
      "IGNORED: Site 09457000 is in aquifer blacklist\n",
      "Added site 14 of 15\n",
      "Added site 15 of 15\n",
      "Trying:  CA\n",
      "Trying:  NM\n",
      "IGNORED: Site 09430010 is in aquifer blacklist\n",
      "IGNORED: Site 09430020 is in aquifer blacklist\n",
      "IGNORED: Site 09430030 is in aquifer blacklist\n",
      "Added site 4 of 9\n",
      "Added site 5 of 9\n",
      "Added site 6 of 9\n",
      "Added site 7 of 9\n",
      "IGNORED: Site 09442681 is in aquifer blacklist\n",
      "Added site 9 of 9\n",
      "Trying:  UT\n",
      "Trying:  NV\n",
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1505\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "Added site 1 of 39\n",
      "Added site 2 of 39\n",
      "IGNORED: Site 09470700 is in aquifer blacklist\n",
      "IGNORED: Site 09470750 is in aquifer blacklist\n",
      "Added site 5 of 39\n",
      "Added site 6 of 39\n",
      "IGNORED: Site 09471310 is in aquifer blacklist\n",
      "IGNORED: Site 09471380 is in aquifer blacklist\n",
      "IGNORED: Site 09471400 is in aquifer blacklist\n",
      "Added site 10 of 39\n",
      "IGNORED: Site 09472050 is in aquifer blacklist\n",
      "Added site 12 of 39\n",
      "Added site 13 of 39\n",
      "IGNORED: Site 09475500 is in aquifer blacklist\n",
      "IGNORED: Site 09478500 is in aquifer blacklist\n",
      "IGNORED: Site 09479350 is in aquifer blacklist\n",
      "Added site 17 of 39\n",
      "Added site 18 of 39\n",
      "IGNORED: Site 09481740 is in aquifer blacklist\n",
      "Added site 20 of 39\n",
      "IGNORED: Site 09482440 is in aquifer blacklist\n",
      "IGNORED: Site 09482490 is in aquifer blacklist\n",
      "IGNORED: Site 09482495 is in aquifer blacklist\n",
      "IGNORED: Site 09482500 is in aquifer blacklist\n",
      "Added site 25 of 39\n",
      "Added site 26 of 39\n",
      "IGNORED: Site 09484550 is in aquifer blacklist\n",
      "IGNORED: Site 09484580 is in aquifer blacklist\n",
      "Added site 29 of 39\n",
      "Added site 30 of 39\n",
      "IGNORED: Site 09485450 is in aquifer blacklist\n",
      "IGNORED: Site 09485700 is in aquifer blacklist\n",
      "IGNORED: Site 09486055 is in aquifer blacklist\n",
      "IGNORED: Site 09486350 is in aquifer blacklist\n",
      "Added site 35 of 39\n",
      "IGNORED: Site 09486520 is in aquifer blacklist\n",
      "Added site 37 of 39\n",
      "IGNORED: Site 09487000 is in aquifer blacklist\n",
      "Added site 39 of 39\n",
      "Trying:  CA\n",
      "Trying:  NM\n",
      "Trying:  UT\n",
      "Trying:  NV\n",
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1506\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "Added site 1 of 43\n",
      "Added site 2 of 43\n",
      "Added site 3 of 43\n",
      "Added site 4 of 43\n",
      "Added site 5 of 43\n",
      "Added site 6 of 43\n",
      "Added site 7 of 43\n",
      "Added site 8 of 43\n",
      "IGNORED: Site 09498400 is in aquifer blacklist\n",
      "Added site 10 of 43\n",
      "IGNORED: Site 094985005 is in aquifer blacklist\n",
      "IGNORED: Site 09498501 is in aquifer blacklist\n",
      "IGNORED: Site 09498502 is in aquifer blacklist\n",
      "IGNORED: Site 09498503 is in aquifer blacklist\n",
      "Added site 15 of 43\n",
      "Added site 16 of 43\n",
      "Added site 17 of 43\n",
      "IGNORED: Site 09502830 is in aquifer blacklist\n",
      "IGNORED: Site 09502900 is in aquifer blacklist\n",
      "IGNORED: Site 09502960 is in aquifer blacklist\n",
      "Added site 21 of 43\n",
      "IGNORED: Site 09503300 is in aquifer blacklist\n",
      "Added site 23 of 43\n",
      "IGNORED: Site 09503990 is in aquifer blacklist\n",
      "Added site 25 of 43\n",
      "IGNORED: Site 09504350 is in aquifer blacklist\n",
      "IGNORED: Site 09504420 is in aquifer blacklist\n",
      "Added site 28 of 43\n",
      "IGNORED: Site 09504950 is in aquifer blacklist\n",
      "Added site 30 of 43\n",
      "Added site 31 of 43\n",
      "Added site 32 of 43\n",
      "Added site 33 of 43\n",
      "IGNORED: Site 09507480 is in aquifer blacklist\n",
      "Added site 35 of 43\n",
      "Added site 36 of 43\n",
      "Added site 37 of 43\n",
      "Added site 38 of 43\n",
      "Added site 39 of 43\n",
      "Added site 40 of 43\n",
      "Added site 41 of 43\n",
      "IGNORED: Site 09512162 is in aquifer blacklist\n",
      "IGNORED: Site 09512165 is in aquifer blacklist\n",
      "Trying:  CA\n",
      "Trying:  NM\n",
      "Trying:  UT\n",
      "Trying:  NV\n",
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1507\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "IGNORED: Site 09512280 is in aquifer blacklist\n",
      "IGNORED: Site 09512450 is in aquifer blacklist\n",
      "Added site 3 of 29\n",
      "Added site 4 of 29\n",
      "Added site 5 of 29\n",
      "Added site 6 of 29\n",
      "IGNORED: Site 09514100 is in aquifer blacklist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n",
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added site 8 of 29\n",
      "IGNORED: Site 09517490 is in aquifer blacklist\n",
      "IGNORED: Site 09519000 is in aquifer blacklist\n",
      "IGNORED: Site 09519501 is in aquifer blacklist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added site 12 of 29\n",
      "IGNORED: Site 09520280 is in aquifer blacklist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n",
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added site 14 of 29\n",
      "IGNORED: Site 09522660 is in aquifer blacklist\n",
      "IGNORED: Site 09522680 is in aquifer blacklist\n",
      "IGNORED: Site 09522700 is in aquifer blacklist\n",
      "IGNORED: Site 09522710 is in aquifer blacklist\n",
      "IGNORED: Site 09522720 is in aquifer blacklist\n",
      "IGNORED: Site 09522730 is in aquifer blacklist\n",
      "IGNORED: Site 09522740 is in aquifer blacklist\n",
      "IGNORED: Site 09522750 is in aquifer blacklist\n",
      "IGNORED: Site 09522760 is in aquifer blacklist\n",
      "IGNORED: Site 09522770 is in aquifer blacklist\n",
      "IGNORED: Site 09522800 is in aquifer blacklist\n",
      "IGNORED: Site 09522850 is in aquifer blacklist\n",
      "Added site 27 of 29\n",
      "Added site 28 of 29\n",
      "Added site 29 of 29\n",
      "Trying:  CA\n",
      "Trying:  NM\n",
      "Trying:  UT\n",
      "Trying:  NV\n",
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1508\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "IGNORED: Site 09535900 is in aquifer blacklist\n",
      "Added site 2 of 3\n",
      "Added site 3 of 3\n",
      "Trying:  CA\n",
      "Trying:  NM\n",
      "Trying:  UT\n",
      "Trying:  NV\n"
     ]
    }
   ],
   "source": [
    "# Data Creation\n",
    "df_aq_sites_metric = pd.DataFrame()\n",
    "df_aq_sites_mk_mag = pd.DataFrame()\n",
    "df_aq_sites_mk_dur = pd.DataFrame()\n",
    "df_aq_sites_mk_intra = pd.DataFrame()\n",
    "master_blacklist = []\n",
    "\n",
    "outer = \"outer\"\n",
    "for root, dirs, files in os.walk(curr_aquifer.wb_dir):\n",
    "    #print(root, dirs, files)\n",
    "    if os.path.basename(root).startswith('NHD_H_'):\n",
    "        if curr_aquifer.wb_shapefiles in files:\n",
    "            shapefile_aq_path = os.path.join(root, curr_aquifer.wb_shapefiles)\n",
    "            ws_gdf = gpd.read_file(shapefile_aq_path)\n",
    "            ws_gdf = ws_gdf.to_crs(4269)           \n",
    "            print(\"Path: \", shapefile_aq_path)\n",
    "            \n",
    "            for state_code in curr_aquifer.states:\n",
    "                print(\"Trying: \", state_code)\n",
    "                try:\n",
    "                    state_uri = fn.create_state_uri(state_code, fn.PARAM_CODE)\n",
    "                    df_state_sites = filter_state_site(shapefile_aq_path, state_uri)\n",
    "                    df_temp_metric, df_temp_mk_mag, df_temp_mk_dur, df_temp_mk_intra, blacklist = create_multi_site_data(df_state_sites, site_limit, curr_aquifer)\n",
    "                    master_blacklist.extend(blacklist)\n",
    "                    df_temp_metric['State'] = state_code\n",
    "                    df_aq_sites_metric = pd.concat([df_aq_sites_metric, df_temp_metric], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "                    df_aq_sites_mk_mag = pd.concat([df_aq_sites_mk_mag, df_temp_mk_mag], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "                    df_aq_sites_mk_dur = pd.concat([df_aq_sites_mk_dur, df_temp_mk_dur], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "                    df_aq_sites_mk_intra = pd.concat([df_aq_sites_mk_intra, df_temp_mk_intra], axis=0, ignore_index=True).reset_index(drop=True)                    \n",
    "                     \n",
    "                except Exception as e:\n",
    "                    print(\"ERROR: \", e)\n",
    "                    print(state_uri)\n",
    "\n",
    "# Filtering out invalid sites by data range and duplicates as some sites are listed in 2+ states   \n",
    "# Also adds a column to indicate presence in HCDN-2009\n",
    "#df_aq_sites = df_aq_sites.drop_duplicates(subset=['site_no'])\n",
    "df_aq_sites_metric = fn.gages_2_filtering(df_aq_sites_metric)\n",
    "\n",
    "try:\n",
    "    with open(f'Prelim_Data/{curr_aquifer.name}_Blacklist.txt', 'w') as f:\n",
    "        master_blacklist = [\"'\" + str(x) + \"'\" for x in master_blacklist]        \n",
    "        f.write(', '.join(master_blacklist))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "#df_aq_sites_metric.to_csv(f'{curr_aquifer.name}_Raw.csv') \n",
    "\n",
    "fn.save_data(df_aq_sites_metric, df_aq_sites_mk_mag, df_aq_sites_mk_dur, df_aq_sites_mk_intra, curr_aquifer.name)  \n",
    "#print(df_aq_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: Nationwide Validity Dataset\n",
    "Generates a dataset for every water gauge with 00060_Mean data in the contiguous US indicating gauge validity for this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = 30\n",
    "shapefile_path = 'ShapeFiles/Lower48/lower48.shp'\n",
    "test_limit = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on AL---]\n",
      "Total Sites: 173 in the state of AL\n",
      "Ignored 35 sites (20.23%)\n",
      "[---Working on AZ---]\n",
      "Total Sites: 218 in the state of AZ\n",
      "Ignored 11 sites (5.05%)\n",
      "[---Working on AR---]\n",
      "Total Sites: 139 in the state of AR\n",
      "Ignored 16 sites (11.51%)\n",
      "[---Working on CA---]\n",
      "Total Sites: 481 in the state of CA\n",
      "Ignored 54 sites (11.23%)\n",
      "[---Working on CO---]\n",
      "Total Sites: 349 in the state of CO\n",
      "Ignored 14 sites (4.01%)\n",
      "[---Working on CT---]\n",
      "Total Sites: 74 in the state of CT\n",
      "Ignored 4 sites (5.41%)\n",
      "[---Working on DE---]\n",
      "Total Sites: 36 in the state of DE\n",
      "Ignored 3 sites (8.33%)\n",
      "[---Working on FL---]\n",
      "Total Sites: 434 in the state of FL\n",
      "Ignored 39 sites (8.99%)\n",
      "[---Working on GA---]\n",
      "Total Sites: 308 in the state of GA\n",
      "Ignored 17 sites (5.52%)\n",
      "[---Working on ID---]\n",
      "Total Sites: 229 in the state of ID\n",
      "Ignored 9 sites (3.93%)\n",
      "[---Working on IL---]\n",
      "Total Sites: 193 in the state of IL\n",
      "Ignored 7 sites (3.63%)\n",
      "[---Working on IN---]\n",
      "Total Sites: 205 in the state of IN\n",
      "Ignored 20 sites (9.76%)\n",
      "[---Working on IA---]\n",
      "Total Sites: 143 in the state of IA\n",
      "Ignored 7 sites (4.9%)\n",
      "[---Working on KS---]\n",
      "Total Sites: 195 in the state of KS\n",
      "Ignored 5 sites (2.56%)\n",
      "[---Working on KY---]\n",
      "Total Sites: 169 in the state of KY\n",
      "Ignored 8 sites (4.73%)\n",
      "[---Working on LA---]\n",
      "Total Sites: 98 in the state of LA\n",
      "Ignored 27 sites (27.55%)\n",
      "[---Working on ME---]\n",
      "Total Sites: 68 in the state of ME\n",
      "Ignored 0 sites (0.0%)\n",
      "[---Working on MD---]\n",
      "Total Sites: 190 in the state of MD\n",
      "Ignored 8 sites (4.21%)\n",
      "[---Working on MA---]\n",
      "Total Sites: 146 in the state of MA\n",
      "Ignored 15 sites (10.27%)\n",
      "[---Working on MI---]\n",
      "Total Sites: 197 in the state of MI\n",
      "Ignored 16 sites (8.12%)\n",
      "[---Working on MN---]\n",
      "Total Sites: 130 in the state of MN\n",
      "Ignored 7 sites (5.38%)\n",
      "[---Working on MS---]\n",
      "Total Sites: 99 in the state of MS\n",
      "Ignored 16 sites (16.16%)\n",
      "[---Working on MO---]\n",
      "Total Sites: 244 in the state of MO\n",
      "Ignored 2 sites (0.82%)\n",
      "[---Working on MT---]\n",
      "Total Sites: 226 in the state of MT\n",
      "Ignored 7 sites (3.1%)\n",
      "[---Working on NE---]\n",
      "Total Sites: 120 in the state of NE\n",
      "Ignored 1 sites (0.83%)\n",
      "[---Working on NV---]\n",
      "Total Sites: 185 in the state of NV\n",
      "Ignored 9 sites (4.86%)\n",
      "[---Working on NH---]\n",
      "Total Sites: 101 in the state of NH\n",
      "Ignored 2 sites (1.98%)\n",
      "[---Working on NJ---]\n",
      "Total Sites: 141 in the state of NJ\n",
      "Ignored 10 sites (7.09%)\n",
      "[---Working on NM---]\n",
      "Total Sites: 174 in the state of NM\n",
      "Ignored 10 sites (5.75%)\n",
      "[---Working on NY---]\n",
      "Total Sites: 321 in the state of NY\n",
      "Ignored 16 sites (4.98%)\n",
      "[---Working on NC---]\n",
      "Total Sites: 240 in the state of NC\n",
      "Ignored 10 sites (4.17%)\n",
      "[---Working on ND---]\n",
      "Total Sites: 123 in the state of ND\n",
      "Ignored 9 sites (7.32%)\n",
      "[---Working on OH---]\n",
      "Total Sites: 222 in the state of OH\n",
      "Ignored 16 sites (7.21%)\n",
      "[---Working on OK---]\n",
      "Total Sites: 178 in the state of OK\n",
      "Ignored 9 sites (5.06%)\n",
      "[---Working on OR---]\n",
      "Total Sites: 213 in the state of OR\n",
      "Ignored 7 sites (3.29%)\n",
      "[---Working on PA---]\n",
      "Total Sites: 311 in the state of PA\n",
      "Ignored 6 sites (1.93%)\n",
      "[---Working on RI---]\n",
      "Total Sites: 34 in the state of RI\n",
      "Ignored 2 sites (5.88%)\n",
      "[---Working on SC---]\n",
      "Total Sites: 131 in the state of SC\n",
      "Ignored 12 sites (9.16%)\n",
      "[---Working on SD---]\n",
      "Total Sites: 122 in the state of SD\n",
      "Ignored 4 sites (3.28%)\n",
      "[---Working on TN---]\n",
      "Total Sites: 136 in the state of TN\n",
      "Ignored 15 sites (11.03%)\n",
      "[---Working on TX---]\n",
      "Total Sites: 597 in the state of TX\n",
      "Ignored 64 sites (10.72%)\n",
      "[---Working on UT---]\n",
      "Total Sites: 153 in the state of UT\n",
      "Ignored 4 sites (2.61%)\n",
      "[---Working on VT---]\n",
      "Total Sites: 98 in the state of VT\n",
      "Ignored 2 sites (2.04%)\n",
      "[---Working on VA---]\n",
      "Total Sites: 200 in the state of VA\n",
      "Ignored 8 sites (4.0%)\n",
      "[---Working on WA---]\n",
      "Total Sites: 265 in the state of WA\n",
      "Ignored 11 sites (4.15%)\n",
      "[---Working on WV---]\n",
      "Total Sites: 110 in the state of WV\n",
      "Ignored 2 sites (1.82%)\n",
      "[---Working on WI---]\n",
      "Total Sites: 220 in the state of WI\n",
      "Ignored 31 sites (14.09%)\n",
      "[---Working on WY---]\n",
      "Total Sites: 116 in the state of WY\n",
      "Ignored 1 sites (0.86%)\n"
     ]
    }
   ],
   "source": [
    "df_natl_validity = pd.DataFrame()\n",
    "\n",
    "for i, state in enumerate(fn.STATE_LIST):\n",
    "    if i >= test_limit: break\n",
    "    print(f'[---Working on {state}---]')\n",
    "    state_uri = fn.create_state_uri(state, fn.PARAM_CODE)\n",
    "    df_state_sites = filter_state_site(shapefile_path, state_uri)\n",
    "    print(f'Total Sites: {len(df_state_sites)} in the state of {state}')\n",
    "    \n",
    "    ignored_count = 0\n",
    "    for loc, row in df_state_sites.iterrows():\n",
    "        df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        if '00060_Mean' not in df.columns:\n",
    "            ignored_count += 1\n",
    "            continue\n",
    "        \n",
    "        start = df['datetime'].min().date()\n",
    "        end = df['datetime'].max().date()\n",
    "        range = round((end - start).days / 365.25, 1)\n",
    "        \n",
    "        valid_range = range > date_range\n",
    "        \n",
    "        date_threshold = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * date_range)\n",
    "        df = df[df['datetime'].dt.date >= date_threshold]\n",
    "        missing = fn.validate(df, date_threshold, fn.DEFAULT_END)\n",
    "        valid = missing < fn.MAX_MISSING_THRESHOLD\n",
    "        \n",
    "        data = {'site_no': row['site_no'], 'state': state, 'data_range': valid_range, 'data_cont': valid, 'start': start, 'end': end, \n",
    "                'total_record': range, 'missing': missing, 'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va']}\n",
    "        \n",
    "        df_natl_validity = pd.concat([df_natl_validity, pd.DataFrame(data, index=['0'])], axis=0, ignore_index=True)\n",
    "        \n",
    "    print(f'Ignored {ignored_count} sites ({round((ignored_count / len(df_state_sites)) * 100, 2)}%)')        \n",
    "        \n",
    "try:\n",
    "    with pd.ExcelWriter('Prelim_Data/National_Validity_30.xlsx') as writer:\n",
    "        df_natl_validity.to_excel(writer, sheet_name='national_validity', index=False) \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
