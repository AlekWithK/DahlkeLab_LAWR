{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES:\n",
    "<ul>\n",
    "<li>Sections labeled <strong>CORE</strong> must be run in order to begin Aquifer Analysis</li>\n",
    "<li>Cells labeled <strong>Control</strong> contain inputs for the immeadiately proceeding section(s)</li>\n",
    "<li>Sections labeled <strong>EXTRA</strong> contain additional plotting or analysis tools but are not necessary for Aquifer Analysis</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nadp.py:44: UserWarning: GDAL not installed. Some functions will not work.\n",
      "  warnings.warn('GDAL not installed. Some functions will not work.')\n"
     ]
    }
   ],
   "source": [
    "#Python3.10\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import contextily as cx\n",
    "import requests\n",
    "import calendar\n",
    "from importlib import reload\n",
    "from typing import IO\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import Point\n",
    "from io import StringIO\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# USGS Data retreival tool\n",
    "from dataretrieval import nwis, utils, codes\n",
    "\n",
    "# Custom modules are imported in multiple locations to faciliate easy reloading when edits are made to their respective files\n",
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "reload(fn)\n",
    "\n",
    "# TODO: Look into the warning that this is disabling. It doesn't appear to be significant for the purposes of this code but should be understood\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#pd.options.mode.chained_assignment = 'warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Single Site Data<br>\n",
    "This function produces the streamflow analysis (using the functions above) for a single site given a list of thresholds and time windows to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "\n",
    "# df2 holds all-time data, df is analyzed range\n",
    "curr_guage = cl.SRB_Guage\n",
    "df = nwis.get_record(sites='03592000', service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2020-09-30')\n",
    "df2 = nwis.get_record(sites=curr_guage.id, service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2014-09-30')\n",
    "\n",
    "# Set to true if running indepedently to verify data with Kocis paper (or other source)\n",
    "# Set to false if running Aquifer Analysis\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Site Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If looking at a post impairment period, but calculating threshold based on the full record of data, pass a second dataframe with a different \n",
    "# start/end date as the final parameter. This method was used in Kocis 2017 and is needed for some data verification, but is not the methodology\n",
    "# used for the Aquifer Analysis and so *args will most often be empty.\n",
    "import Src.func as fn\n",
    "reload(fn)\n",
    "\n",
    "def single_site_data(df: pd.DataFrame, quantiles_list: list, data_ranges_list: list, *args):\n",
    "    df = df.reset_index()    \n",
    "    threshold = None\n",
    "    df_complete_site_data = pd.DataFrame()    \n",
    "    df_complete_mk_mag = pd.DataFrame()\n",
    "    df_complete_mk_dur = pd.DataFrame()\n",
    "    df_complete_mk_intra = pd.DataFrame()\n",
    "    \n",
    "    for data_range in data_ranges_list:\n",
    "        for quantile in quantiles_list:\n",
    "            # Copying original dataframe to avoid any conflicts\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Validate that site is not missing > 10% of data\n",
    "            date_threshold = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * data_range)              \n",
    "            \n",
    "            # Filter dataframe based on current data_range (don't do this during testing if testing unique dataset/range)\n",
    "            if not testing:\n",
    "                df_copy = df_copy[df_copy['datetime'].dt.date >= date_threshold]\n",
    "            \n",
    "            # Validate <10% missing data based on filtered dataframe\n",
    "            missing = fn.validate(df_copy, date_threshold, fn.DEFAULT_END)\n",
    "            valid = missing < fn.MAX_MISSING_THRESHOLD                               \n",
    "    \n",
    "            # Check for optional second dataframe containing full record\n",
    "            if args and isinstance(args[0], pd.DataFrame) and not args[0].empty:\n",
    "                #print('Threshold calculation across the full record')\n",
    "                df2 = args[0].reset_index()\n",
    "                threshold = fn.calc_threshold(df2, quantile)\n",
    "                # threshold = 52350 # SRB_Guage post-impairment threshold for verification with Kocis_2017 \n",
    "            else:\n",
    "                #print('Threshold calculation across a limited record')\n",
    "                threshold = fn.calc_threshold(df_copy, quantile)                             \n",
    "\n",
    "            # Create a dataframe with only days over HMF threshold as well as continuous dataframe for MK trend test\n",
    "            hmf_series_defl, hmf_series_cont = fn.filter_hmf(df_copy, threshold)\n",
    "            #print(zero_deflated_hmf)\n",
    "            \n",
    "            # Aggregate data before performing MK magnitude test\n",
    "            df_agg_cont = hmf_series_cont.copy()\n",
    "            df_agg_cont = fn.convert_hmf(df_agg_cont, threshold)\n",
    "            df_agg_cont['00060_Mean'] = df_agg_cont['00060_Mean'].apply(lambda x: x * fn.CUBIC_FT_KM_FACTOR if x >= 0 else 0)\n",
    "            df_agg_cont.set_index('datetime', inplace=True)\n",
    "            df_agg_cont = df_agg_cont.resample(fn.HYDRO_YEAR).agg({'00060_Mean': ['sum', 'count']})\n",
    "            df_agg_cont.columns = ['Sum', 'Count']\n",
    "            df_agg_cont_defl = df_agg_cont[df_agg_cont['Sum'] > 0]          \n",
    "            \n",
    "            # MK Magnitude\n",
    "            df_mk_mag = fn.mann_kendall(df_agg_cont_defl['Sum'], df_agg_cont['Sum'], fn.MK_TREND_ALPHA)\n",
    "\n",
    "            # Find number of years with HMF\n",
    "            hmf_years = fn.num_hmf_years(hmf_series_defl)            \n",
    "\n",
    "            # Mask out months that don't fall within 3 and 6 month Winter range\n",
    "            df_six_month, df_three_month = fn.three_six_range(hmf_series_defl, 12, 2, 11, 4)\n",
    "\n",
    "            # Convert to daily average flow in cfpd, and take only flow above the threshold\n",
    "            hmf_series_defl = fn.convert_hmf(hmf_series_defl, threshold)\n",
    "            total_hmf_flow = hmf_series_defl[\"00060_Mean\"].sum()\n",
    "            hmf_per_month = fn.monthly_hmf(hmf_series_defl, data_range, quantile)\n",
    "\n",
    "            # Calculate 3 and 6 month HMF\n",
    "            df_six_month = fn.convert_hmf(df_six_month, threshold)\n",
    "            six_month_hmf = df_six_month[\"00060_Mean\"].sum()\n",
    "            df_three_month = fn.convert_hmf(df_three_month, threshold)\n",
    "            three_month_hmf = df_three_month[\"00060_Mean\"].sum()\n",
    "            \n",
    "            total_hmf_flow = (total_hmf_flow * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            six_month_hmf = (six_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            three_month_hmf = (three_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "\n",
    "            # Inter-annual\n",
    "            inter_annual, delta = fn.calc_inter_annual(hmf_series_cont, hmf_years)            \n",
    "\n",
    "            # Average Duration and Intra-annual Frequency\n",
    "            hmf_series_cont = fn.convert_hmf(hmf_series_cont, threshold)\n",
    "            event_duration, annual_duration, intra_annual, event_hmf, df_results = fn.calc_duration_intra_annual(hmf_series_cont, hmf_years)\n",
    "            dur_series_defl = df_results['duration'][df_results['duration'] > 0]\n",
    "            dur_series_cont = df_results['duration']\n",
    "            df_mk_dur = fn.mann_kendall(dur_series_defl, dur_series_cont, fn.MK_TREND_ALPHA)\n",
    "                        \n",
    "            intra_series_defl = df_results['total_events'][df_results['total_events'] > 0]\n",
    "            intra_series_cont = df_results['total_events']\n",
    "            df_mk_intra = fn.mann_kendall(intra_series_defl, intra_series_cont, fn.MK_TREND_ALPHA) \n",
    "            \n",
    "            # Timing Calculation using DOHY\n",
    "            timing = fn.calc_timing(hmf_series_defl)           \n",
    "\n",
    "            # TODO: One-day peaks (avg. # of times hmf occurs on one day only)          \n",
    "            \n",
    "            # Merging site dataframe with Mann-Kendall dataframe. This start date is the beginning of the actual data, not necessarily \n",
    "            # the beginning of the analyzed range. Validation (above) starts from the start of the official range (1970/90-2020)\n",
    "            start = df_copy['datetime'].min().date()\n",
    "            end = df_copy['datetime'].max().date()\n",
    "            data = {'dataset_ID': (data_range * quantile), 'site_no': df_copy.iloc[0]['site_no'], 'analyze_start': start, 'analyze_end': end, 'analyze_range': delta, 'quantile': quantile, \n",
    "                    'valid': valid, 'missing': missing, 'threshold': threshold, 'hmf_years': hmf_years, 'annual_hmf': total_hmf_flow, 'six_mo_hmf': six_month_hmf, 'three_mo_hmf': three_month_hmf, \n",
    "                    'annual_duration': annual_duration, 'event_duration': event_duration, 'event_hmf': event_hmf, 'inter_annual%': inter_annual, 'intra_annual': intra_annual, 'timing': timing}              \n",
    "            \n",
    "            # Merging MK magnitiude\n",
    "            df_mk_mag.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_mag.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_mag = pd.concat([df_complete_mk_mag.reset_index(drop=True), df_mk_mag.reset_index(drop=True)], axis=0) \n",
    "            \n",
    "            # Merging MK duration\n",
    "            df_mk_dur.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_dur.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_dur = pd.concat([df_complete_mk_dur.reset_index(drop=True), df_mk_dur.reset_index(drop=True)], axis=0)   \n",
    "            \n",
    "            # Merging MK intra-annual\n",
    "            df_mk_intra.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_intra.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_intra = pd.concat([df_complete_mk_intra.reset_index(drop=True), df_mk_intra.reset_index(drop=True)], axis=0)        \n",
    "            \n",
    "            # Merging metric results\n",
    "            df_single_iteration = pd.DataFrame(data, index=['0'])\n",
    "            df_single_iteration = pd.concat([df_single_iteration.reset_index(drop=True), hmf_per_month.reset_index(drop=True)], axis=1)\n",
    "            df_complete_site_data = pd.concat([df_complete_site_data.reset_index(drop=True), df_single_iteration.reset_index(drop=True)], axis=0)\n",
    "        \n",
    "    return df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra\n",
    "\n",
    "# For testing purposes, to run this cell independently\n",
    "df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "\n",
    "'''\n",
    "try:\n",
    "    with pd.ExcelWriter('df_single_site.xlsx') as writer:\n",
    "        df_complete_site_data.to_excel(writer, sheet_name='site_metrics', index=False)\n",
    "        df_complete_mk_mag.to_excel(writer, sheet_name='mk_magnitude', index=False)\n",
    "        df_complete_mk_intra.to_excel(writer, sheet_name='mk_intra', index=False)\n",
    "        df_complete_mk_dur.to_excel(writer, sheet_name='mk_duration', index=False)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "'''\n",
    "   \n",
    "fn.single_site_report(df_complete_site_data)\n",
    "df_complete_site_data = fn.gages_2_filtering(df_complete_site_data)\n",
    "#fn.save_data(df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra, 'TEST')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Multi-Site Filtering<br>\n",
    "This function creates the list of sites to analyze by filtering the complete list of state sites with 00060_Mean data down to those that lie within a specific watershed boundary using decimal long/lat positional data and a region shapefile (e.g. state or watershed boundary)<br><br>\n",
    "\n",
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed when running the following cell independently\n",
    "shapefile_path = 'ShapeFiles/Aquifers/Central_Valley/HUC4/NHD_H_1802'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site List Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sites: 57 in the state of CA in the given WB\n",
      "['11337080', '11342000', '11345500', '11348500', '11355010', '11355500', '11370500', '11370700', '11372000', '11374000', '11376000', '11376550', '11377100', '11379500', '11381500', '11383500', '11389500', '11390000', '11390500', '11401920', '11402000', '11413000', '11418500', '11421000', '11424000', '11425500', '11427000', '11446500', '11447360', '11447650', '11447830', '11447850', '11447890', '11447905', '11448750', '11448800', '11449255', '11449500', '11451000', '11451100', '11451300', '11451715', '11451800', '11452500', '11452800', '11452900', '11453000', '11453500', '11453590', '11454000', '11455095', '11455140', '11455280', '11455315', '11455338', '11455385', '11455420']\n"
     ]
    }
   ],
   "source": [
    "def filter_state_site(shapefile_path: str, state_uri: str):\n",
    "    \"\"\"Creates a list of sites with over 50 years of 00060_Mean streamflow data for a given region\"\"\"\n",
    "    # Request page from USGS site, ignore all informational lines\n",
    "    response = requests.get(state_uri)\n",
    "    data = response.text\n",
    "    lines = data.splitlines()\n",
    "    lines = [line for line in lines if not line.startswith('#')]\n",
    "\n",
    "    # Create dataframe where site_no is a list of all sites in a state with 00060 data\n",
    "    tsd = \"\\n\".join(lines)\n",
    "    df = pd.read_csv(StringIO(tsd), sep='\\t')\n",
    "    df_state_sites = df.iloc[1:]\n",
    "        \n",
    "    # Filter out sites outside of HU boundary\n",
    "    if fn.SORT_BY_WB:\n",
    "        shapefile = gpd.read_file(shapefile_path)\n",
    "        df_state_sites['geometry'] = [Point(lon, lat) for lon, lat in zip(df_state_sites['dec_long_va'], df_state_sites['dec_lat_va'])]\n",
    "        gdf_data = gpd.GeoDataFrame(df_state_sites, crs=shapefile.crs)\n",
    "        df_state_sites = gpd.sjoin(gdf_data, shapefile, predicate='within')\n",
    "            \n",
    "    #print(df_state_sites.columns.to_list())\n",
    "    #print(df_state_sites)\n",
    "    \n",
    "    return df_state_sites\n",
    "\n",
    "df_state_sites = filter_state_site(shapefile_path, fn.SITES_URI)\n",
    "print(f'Total Sites: {len(df_state_sites)} in the state of {fn.STATE_CODE.upper()} in the given WB')\n",
    "site_list = df_state_sites['site_no'].to_list()\n",
    "print(site_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Multi-Site Data Creation<br>\n",
    "This function uses the list generated by the previous section to generate single site data for every site in the list, and validate that no site is missing more than 10% of its data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGNORED: Site 11337080 is in aquifer blacklist\n",
      "Added site 2 of 57\n",
      "Added site 3 of 57\n",
      "Max HMF for this region: 0.2\n",
      "8 site(s) valid out of 8\n"
     ]
    }
   ],
   "source": [
    "# REQUIRES: 'df_state_sites' from 'Multi-Site Filtering'\n",
    "# Used for now to limit runtime when running independently\n",
    "site_limit = 3\n",
    "\n",
    "def create_multi_site_data(df_state_sites: pd.DataFrame, site_limit: int, aquifer: cl.Aquifer):\n",
    "\t\"\"\"Generates detailed HMF, MK, and POS information for each site in the passed dataframe\"\"\"\n",
    "\t# Necessary for proper iterrows() behavior\n",
    "\tdf_state_sites.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\t# Creating the dataframe that will hold final results for mapping\n",
    "\tdf_multi_site_metric = pd.DataFrame()\n",
    "\tdf_multi_site_mk_mag = pd.DataFrame()\n",
    "\tdf_multi_site_mk_dur = pd.DataFrame()\n",
    "\tdf_multi_site_mk_intra = pd.DataFrame()\n",
    " \n",
    "\tsite_blacklist = []\n",
    "\n",
    "\tfor index, row in df_state_sites.iterrows():\n",
    "\t\twhile index < site_limit:\n",
    "      \n",
    "\t\t\tif str(row['site_no']) in aquifer.blacklist:\n",
    "\t\t\t\tprint(f'IGNORED: Site {row[\"site_no\"]} is in aquifer blacklist')\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\t# Create a dataframe for the current site in the iteration to perform calculations on\n",
    "\t\t\tdf = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "\t\t\tdf = df.reset_index()\n",
    "\t\t\t\n",
    "\t\t\t# Confirm that dataframe is not empty and has the required streamflow data before continuing\n",
    "\t\t\tif df.empty:\n",
    "\t\t\t\tsite_blacklist.append(row['site_no'])\n",
    "\t\t\t\tprint(f'IGNORED: No data for site {row[\"site_no\"]}')\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tif '00060_Mean' not in df.columns:\n",
    "\t\t\t\tsite_blacklist.append(row['site_no'])\n",
    "\t\t\t\tprint(f'IGNORED: No 00060_Mean data for site {row[\"site_no\"]}')\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\t# Filter out sites with less than the minimum required range of data\n",
    "\t\t\tstart = df['datetime'].min().date()\n",
    "\t\t\tend = df['datetime'].max().date()\n",
    "\t\t\trange = round((end - start).days / 365.25, 1)\n",
    "\t\t\t\n",
    "\t\t\t# Ignore sites with less than the minimum required years of data\n",
    "\t\t\tif range < fn.MIN_DATA_PERIOD:\n",
    "\t\t\t\tsite_blacklist.append(row['site_no'])\n",
    "\t\t\t\tprint(f'IGNORED: Not enough data for site {row[\"site_no\"]}')\n",
    "\t\t\t\tbreak\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\tdf_single_site_metric, df_mk_mag, df_mk_dur, df_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\t\t\t\n",
    "\t\t\t\n",
    "\t\t\t# Append positional data to dataframe created by single_site_data()\n",
    "\t\t\tadd_data = {'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va'], 'data_start': start, 'data_end': end, 'total_record': range}\t\t\t\n",
    "\t\t\tadd_data = pd.DataFrame(add_data, index=['0'])\n",
    "   \n",
    "\t\t\t# Duplicates the rows of add_data so that positional information is passed to each individual dataframe when this frame is split\n",
    "\t\t\tadd_data = pd.DataFrame(np.tile(add_data.values, (len(df_single_site_metric), 1)), columns=add_data.columns)\n",
    "\t\t\tdf_single_site_metric = pd.concat([df_single_site_metric.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\t\t\t# Append single site data to multi-site dataframes\n",
    "\t\t\tdf_multi_site_metric = pd.concat([df_multi_site_metric, df_single_site_metric], ignore_index=True)\n",
    "\t\t\tdf_multi_site_mk_mag = pd.concat([df_multi_site_mk_mag, df_mk_mag], ignore_index=True)\n",
    "\t\t\tdf_multi_site_mk_dur = pd.concat([df_multi_site_mk_dur, df_mk_dur], ignore_index=True)\n",
    "\t\t\tdf_multi_site_mk_intra = pd.concat([df_multi_site_mk_intra, df_mk_intra], ignore_index=True)   \n",
    "   \t\t\n",
    "\t\t\tprint(f'Added site {index + 1} of {len(df_state_sites)}')\n",
    "\t\t\t\n",
    "\t\t\t#clear_output(wait=True)\n",
    "\t\t\t#time.sleep(0.500)\n",
    "\t\t\tbreak\n",
    "\n",
    "\treturn df_multi_site_metric, df_multi_site_mk_mag, df_multi_site_mk_dur, df_multi_site_mk_intra, site_blacklist\n",
    "\n",
    "\n",
    "df_multi_site_metric, df_multi_site_mk_mag, df_multi_site_mk_dur, df_multi_site_mk_intra, site_blacklist = create_multi_site_data(df_state_sites, site_limit, cl.central_valley_aquifer)\n",
    "df_multi_site, df_invalid_site = fn.filter_by_valid(df_multi_site_metric)\n",
    "#df_multi_site.to_csv('df_multi_site.csv')\n",
    "#print(df_multi_site)\n",
    "\n",
    "print(f'Max HMF for this region: {df_multi_site[\"annual_hmf\"].max():.1f}')\n",
    "print(f'{len(df_multi_site)} site(s) valid out of {len(df_multi_site_metric)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Aquifer Analysis\n",
    "This section generates aquifer-wide data based on all watershed boundary shapefiles that intersect with the aquifer boundary. In addition it contains a section to generate a map of this aquifer boundary, the intersecting watershed boundaries, and their relevant water gauge sites and annual high magnitude flows<br><br>\n",
    "Aquifer Analysis Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the aquifer to collect data for\n",
    "curr_aquifer = cl.arizona_alluvial_aquifer\n",
    "\n",
    "# Limit saved sites for testing purposes\n",
    "# Set to >=999 to analyze all sites\n",
    "site_limit = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Creation: \n",
    "<li>This code is time-consuming and can be bypassed if a spreadsheet for an aquifer already exists and all that is desired is a plot based on that data</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1501\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "Added site 1 of 11\n",
      "IGNORED: Site 09403850 is in aquifer blacklist\n",
      "IGNORED: Site 09404104 is in aquifer blacklist\n",
      "IGNORED: Site 09404115 is in aquifer blacklist\n",
      "IGNORED: Site 09404200 is in aquifer blacklist\n",
      "IGNORED: Site 09404208 is in aquifer blacklist\n",
      "IGNORED: Site 09404222 is in aquifer blacklist\n",
      "IGNORED: Site 09404343 is in aquifer blacklist\n",
      "IGNORED: Site 09413700 is in aquifer blacklist\n",
      "Added site 10 of 11\n",
      "Added site 11 of 11\n",
      "Trying:  CA\n",
      "Trying:  NM\n",
      "Trying:  UT\n",
      "IGNORED: Site 09403600 is in aquifer blacklist\n",
      "Added site 2 of 18\n",
      "IGNORED: Site 09404900 is in aquifer blacklist\n",
      "Added site 4 of 18\n",
      "Added site 5 of 18\n",
      "IGNORED: Site 09406100 is in aquifer blacklist\n",
      "Added site 7 of 18\n",
      "IGNORED: Site 09408135 is in aquifer blacklist\n",
      "Added site 9 of 18\n",
      "IGNORED: Site 09408195 is in aquifer blacklist\n",
      "Added site 11 of 18\n",
      "IGNORED: Site 09409100 is in aquifer blacklist\n",
      "IGNORED: Site 09410100 is in aquifer blacklist\n",
      "Added site 14 of 18\n",
      "IGNORED: Site 09413200 is in aquifer blacklist\n",
      "Added site 16 of 18\n",
      "IGNORED: Site 09413700 is in aquifer blacklist\n",
      "IGNORED: Site 09413900 is in aquifer blacklist\n",
      "Trying:  NV\n",
      "Added site 1 of 41\n",
      "IGNORED: Site 09404200 is in aquifer blacklist\n",
      "Added site 3 of 41\n",
      "IGNORED: Site 09413700 is in aquifer blacklist\n",
      "IGNORED: Site 09413900 is in aquifer blacklist\n",
      "Added site 6 of 41\n",
      "Added site 7 of 41\n",
      "IGNORED: Site 09415510 is in aquifer blacklist\n",
      "IGNORED: Site 09415558 is in aquifer blacklist\n",
      "IGNORED: Site 09415645 is in aquifer blacklist\n",
      "IGNORED: Site 09415900 is in aquifer blacklist\n",
      "IGNORED: Site 09415908 is in aquifer blacklist\n",
      "IGNORED: Site 09415910 is in aquifer blacklist\n",
      "IGNORED: Site 09415915 is in aquifer blacklist\n",
      "IGNORED: Site 09415920 is in aquifer blacklist\n",
      "IGNORED: Site 09415927 is in aquifer blacklist\n",
      "Added site 17 of 41\n",
      "Added site 18 of 41\n",
      "Added site 19 of 41\n",
      "IGNORED: Site 09418700 is in aquifer blacklist\n",
      "Added site 21 of 41\n",
      "IGNORED: Site 09419530 is in aquifer blacklist\n",
      "IGNORED: Site 09419550 is in aquifer blacklist\n",
      "IGNORED: Site 09419625 is in aquifer blacklist\n",
      "IGNORED: Site 09419665 is in aquifer blacklist\n",
      "IGNORED: Site 094196781 is in aquifer blacklist\n",
      "IGNORED: Site 094196783 is in aquifer blacklist\n",
      "IGNORED: Site 094196784 is in aquifer blacklist\n",
      "IGNORED: Site 09419679 is in aquifer blacklist\n",
      "IGNORED: Site 09419696 is in aquifer blacklist\n",
      "IGNORED: Site 09419698 is in aquifer blacklist\n",
      "IGNORED: Site 09419700 is in aquifer blacklist\n",
      "IGNORED: Site 09419740 is in aquifer blacklist\n",
      "IGNORED: Site 09419745 is in aquifer blacklist\n",
      "IGNORED: Site 09419747 is in aquifer blacklist\n",
      "IGNORED: Site 09419749 is in aquifer blacklist\n",
      "IGNORED: Site 09419753 is in aquifer blacklist\n",
      "IGNORED: Site 09419756 is in aquifer blacklist\n",
      "Added site 39 of 41\n",
      "IGNORED: Site 361600114163301 is in aquifer blacklist\n",
      "IGNORED: Site 362734114124201 is in aquifer blacklist\n",
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1503\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "Added site 1 of 49\n",
      "IGNORED: Site 09423560 is in aquifer blacklist\n",
      "IGNORED: Site 09424380 is in aquifer blacklist\n",
      "IGNORED: Site 09424447 is in aquifer blacklist\n",
      "Added site 5 of 49\n",
      "IGNORED: Site 09424580 is in aquifer blacklist\n",
      "IGNORED: Site 09424600 is in aquifer blacklist\n",
      "Added site 8 of 49\n",
      "Added site 9 of 49\n",
      "IGNORED: Site 09426620 is in aquifer blacklist\n",
      "Added site 11 of 49\n",
      "Added site 12 of 49\n",
      "Added site 13 of 49\n",
      "Added site 14 of 49\n",
      "IGNORED: Site 09429030 is in aquifer blacklist\n",
      "IGNORED: Site 09429070 is in aquifer blacklist\n",
      "Added site 17 of 49\n",
      "Added site 18 of 49\n",
      "Added site 19 of 49\n",
      "Added site 20 of 49\n",
      "IGNORED: Site 09429500 is in aquifer blacklist\n",
      "IGNORED: Site 09429600 is in aquifer blacklist\n",
      "IGNORED: Site 09520900 is in aquifer blacklist\n",
      "Added site 24 of 49\n",
      "IGNORED: Site 09522400 is in aquifer blacklist\n",
      "Added site 26 of 49\n",
      "Added site 27 of 49\n",
      "Added site 28 of 49\n",
      "IGNORED: Site 09522860 is in aquifer blacklist\n",
      "IGNORED: Site 09522880 is in aquifer blacklist\n",
      "IGNORED: Site 09522900 is in aquifer blacklist\n",
      "Added site 32 of 49\n",
      "IGNORED: Site 09523200 is in aquifer blacklist\n",
      "Added site 34 of 49\n",
      "Added site 35 of 49\n",
      "Added site 36 of 49\n",
      "Added site 37 of 49\n",
      "IGNORED: Site 09524700 is in aquifer blacklist\n",
      "Added site 39 of 49\n",
      "Added site 40 of 49\n",
      "IGNORED: Site 09526000 is in aquifer blacklist\n",
      "IGNORED: Site 09526200 is in aquifer blacklist\n",
      "IGNORED: Site 09528200 is in aquifer blacklist\n",
      "Added site 44 of 49\n",
      "IGNORED: Site 09529000 is in aquifer blacklist\n",
      "Added site 46 of 49\n",
      "Added site 47 of 49\n",
      "IGNORED: Site 09530100 is in aquifer blacklist\n",
      "IGNORED: Site 09530500 is in aquifer blacklist\n",
      "Trying:  CA\n",
      "Added site 1 of 20\n",
      "Added site 2 of 20\n",
      "Added site 3 of 20\n",
      "Added site 4 of 20\n",
      "Added site 5 of 20\n",
      "Added site 6 of 20\n",
      "IGNORED: Site 09429500 is in aquifer blacklist\n",
      "IGNORED: Site 09429600 is in aquifer blacklist\n",
      "Added site 9 of 20\n",
      "Added site 10 of 20\n",
      "IGNORED: Site 09523200 is in aquifer blacklist\n",
      "Added site 12 of 20\n",
      "Added site 13 of 20\n",
      "Added site 14 of 20\n",
      "Added site 15 of 20\n",
      "IGNORED: Site 09524700 is in aquifer blacklist\n",
      "Added site 17 of 20\n",
      "IGNORED: Site 09526200 is in aquifer blacklist\n",
      "Added site 19 of 20\n",
      "IGNORED: Site 09530500 is in aquifer blacklist\n",
      "Trying:  NM\n",
      "Trying:  UT\n",
      "Trying:  NV\n",
      "Added site 1 of 1\n",
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1504\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "Added site 1 of 15\n",
      "Added site 2 of 15\n",
      "IGNORED: Site 09439000 is in aquifer blacklist\n",
      "Added site 4 of 15\n",
      "Added site 5 of 15\n",
      "Added site 6 of 15\n",
      "Added site 7 of 15\n",
      "IGNORED: Site 09446320 is in aquifer blacklist\n",
      "Added site 9 of 15\n",
      "Added site 10 of 15\n",
      "IGNORED: Site 09447800 is in aquifer blacklist\n",
      "Added site 12 of 15\n",
      "IGNORED: Site 09457000 is in aquifer blacklist\n",
      "Added site 14 of 15\n",
      "Added site 15 of 15\n",
      "Trying:  CA\n",
      "Trying:  NM\n",
      "IGNORED: Site 09430010 is in aquifer blacklist\n",
      "IGNORED: Site 09430020 is in aquifer blacklist\n",
      "IGNORED: Site 09430030 is in aquifer blacklist\n",
      "Added site 4 of 9\n",
      "Added site 5 of 9\n",
      "Added site 6 of 9\n",
      "Added site 7 of 9\n",
      "IGNORED: Site 09442681 is in aquifer blacklist\n",
      "Added site 9 of 9\n",
      "Trying:  UT\n",
      "Trying:  NV\n",
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1505\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "Added site 1 of 39\n",
      "Added site 2 of 39\n",
      "IGNORED: Site 09470700 is in aquifer blacklist\n",
      "IGNORED: Site 09470750 is in aquifer blacklist\n",
      "Added site 5 of 39\n",
      "Added site 6 of 39\n",
      "IGNORED: Site 09471310 is in aquifer blacklist\n",
      "IGNORED: Site 09471380 is in aquifer blacklist\n",
      "IGNORED: Site 09471400 is in aquifer blacklist\n",
      "Added site 10 of 39\n",
      "IGNORED: Site 09472050 is in aquifer blacklist\n",
      "Added site 12 of 39\n",
      "Added site 13 of 39\n",
      "IGNORED: Site 09475500 is in aquifer blacklist\n",
      "IGNORED: Site 09478500 is in aquifer blacklist\n",
      "IGNORED: Site 09479350 is in aquifer blacklist\n",
      "Added site 17 of 39\n",
      "Added site 18 of 39\n",
      "IGNORED: Site 09481740 is in aquifer blacklist\n",
      "Added site 20 of 39\n",
      "IGNORED: Site 09482440 is in aquifer blacklist\n",
      "IGNORED: Site 09482490 is in aquifer blacklist\n",
      "IGNORED: Site 09482495 is in aquifer blacklist\n",
      "IGNORED: Site 09482500 is in aquifer blacklist\n",
      "Added site 25 of 39\n",
      "Added site 26 of 39\n",
      "IGNORED: Site 09484550 is in aquifer blacklist\n",
      "IGNORED: Site 09484580 is in aquifer blacklist\n",
      "Added site 29 of 39\n",
      "Added site 30 of 39\n",
      "IGNORED: Site 09485450 is in aquifer blacklist\n",
      "IGNORED: Site 09485700 is in aquifer blacklist\n",
      "IGNORED: Site 09486055 is in aquifer blacklist\n",
      "IGNORED: Site 09486350 is in aquifer blacklist\n",
      "Added site 35 of 39\n",
      "IGNORED: Site 09486520 is in aquifer blacklist\n",
      "Added site 37 of 39\n",
      "IGNORED: Site 09487000 is in aquifer blacklist\n",
      "Added site 39 of 39\n",
      "Trying:  CA\n",
      "Trying:  NM\n",
      "Trying:  UT\n",
      "Trying:  NV\n",
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1506\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "Added site 1 of 43\n",
      "Added site 2 of 43\n",
      "Added site 3 of 43\n",
      "Added site 4 of 43\n",
      "Added site 5 of 43\n",
      "Added site 6 of 43\n",
      "Added site 7 of 43\n",
      "Added site 8 of 43\n",
      "IGNORED: Site 09498400 is in aquifer blacklist\n",
      "Added site 10 of 43\n",
      "IGNORED: Site 094985005 is in aquifer blacklist\n",
      "IGNORED: Site 09498501 is in aquifer blacklist\n",
      "IGNORED: Site 09498502 is in aquifer blacklist\n",
      "IGNORED: Site 09498503 is in aquifer blacklist\n",
      "Added site 15 of 43\n",
      "Added site 16 of 43\n",
      "Added site 17 of 43\n",
      "IGNORED: Site 09502830 is in aquifer blacklist\n",
      "IGNORED: Site 09502900 is in aquifer blacklist\n",
      "IGNORED: Site 09502960 is in aquifer blacklist\n",
      "Added site 21 of 43\n",
      "IGNORED: Site 09503300 is in aquifer blacklist\n",
      "Added site 23 of 43\n",
      "IGNORED: Site 09503990 is in aquifer blacklist\n",
      "Added site 25 of 43\n",
      "IGNORED: Site 09504350 is in aquifer blacklist\n",
      "IGNORED: Site 09504420 is in aquifer blacklist\n",
      "Added site 28 of 43\n",
      "IGNORED: Site 09504950 is in aquifer blacklist\n",
      "Added site 30 of 43\n",
      "Added site 31 of 43\n",
      "Added site 32 of 43\n",
      "Added site 33 of 43\n",
      "IGNORED: Site 09507480 is in aquifer blacklist\n",
      "Added site 35 of 43\n",
      "Added site 36 of 43\n",
      "Added site 37 of 43\n",
      "Added site 38 of 43\n",
      "Added site 39 of 43\n",
      "Added site 40 of 43\n",
      "Added site 41 of 43\n",
      "IGNORED: Site 09512162 is in aquifer blacklist\n",
      "IGNORED: Site 09512165 is in aquifer blacklist\n",
      "Trying:  CA\n",
      "Trying:  NM\n",
      "Trying:  UT\n",
      "Trying:  NV\n",
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1507\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "IGNORED: Site 09512280 is in aquifer blacklist\n",
      "IGNORED: Site 09512450 is in aquifer blacklist\n",
      "Added site 3 of 29\n",
      "Added site 4 of 29\n",
      "Added site 5 of 29\n",
      "Added site 6 of 29\n",
      "IGNORED: Site 09514100 is in aquifer blacklist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n",
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added site 8 of 29\n",
      "IGNORED: Site 09517490 is in aquifer blacklist\n",
      "IGNORED: Site 09519000 is in aquifer blacklist\n",
      "IGNORED: Site 09519501 is in aquifer blacklist\n",
      "Added site 12 of 29\n",
      "IGNORED: Site 09520280 is in aquifer blacklist\n",
      "Added site 14 of 29\n",
      "IGNORED: Site 09522660 is in aquifer blacklist\n",
      "IGNORED: Site 09522680 is in aquifer blacklist\n",
      "IGNORED: Site 09522700 is in aquifer blacklist\n",
      "IGNORED: Site 09522710 is in aquifer blacklist\n",
      "IGNORED: Site 09522720 is in aquifer blacklist\n",
      "IGNORED: Site 09522730 is in aquifer blacklist\n",
      "IGNORED: Site 09522740 is in aquifer blacklist\n",
      "IGNORED: Site 09522750 is in aquifer blacklist\n",
      "IGNORED: Site 09522760 is in aquifer blacklist\n",
      "IGNORED: Site 09522770 is in aquifer blacklist\n",
      "IGNORED: Site 09522800 is in aquifer blacklist\n",
      "IGNORED: Site 09522850 is in aquifer blacklist\n",
      "Added site 27 of 29\n",
      "Added site 28 of 29\n",
      "Added site 29 of 29\n",
      "Trying:  CA\n",
      "Trying:  NM\n",
      "Trying:  UT\n",
      "Trying:  NV\n",
      "Path:  ShapeFiles/Aquifers/Arizona_Alluvial/HUC4\\NHD_H_1508\\WBDHU4.shp\n",
      "Trying:  AZ\n",
      "IGNORED: Site 09535900 is in aquifer blacklist\n",
      "Added site 2 of 3\n",
      "Added site 3 of 3\n",
      "Trying:  CA\n",
      "Trying:  NM\n",
      "Trying:  UT\n",
      "Trying:  NV\n"
     ]
    }
   ],
   "source": [
    "# Data Creation\n",
    "df_aq_sites_metric = pd.DataFrame()\n",
    "df_aq_sites_mk_mag = pd.DataFrame()\n",
    "df_aq_sites_mk_dur = pd.DataFrame()\n",
    "df_aq_sites_mk_intra = pd.DataFrame()\n",
    "master_blacklist = []\n",
    "\n",
    "outer = \"outer\"\n",
    "for root, dirs, files in os.walk(curr_aquifer.wb_dir):\n",
    "    #print(root, dirs, files)\n",
    "    if os.path.basename(root).startswith('NHD_H_'):\n",
    "        if curr_aquifer.wb_shapefiles in files:\n",
    "            shapefile_aq_path = os.path.join(root, curr_aquifer.wb_shapefiles)\n",
    "            ws_gdf = gpd.read_file(shapefile_aq_path)\n",
    "            ws_gdf = ws_gdf.to_crs(4269)           \n",
    "            print(\"Path: \", shapefile_aq_path)\n",
    "            \n",
    "            for state_code in curr_aquifer.states:\n",
    "                print(\"Trying: \", state_code)\n",
    "                try:\n",
    "                    state_uri = fn.create_state_uri(state_code, fn.PARAM_CODE)\n",
    "                    df_state_sites = filter_state_site(shapefile_aq_path, state_uri)\n",
    "                    df_temp_metric, df_temp_mk_mag, df_temp_mk_dur, df_temp_mk_intra, blacklist = create_multi_site_data(df_state_sites, site_limit, curr_aquifer)\n",
    "                    master_blacklist.extend(blacklist)\n",
    "                    df_temp_metric['State'] = state_code\n",
    "                    df_aq_sites_metric = pd.concat([df_aq_sites_metric, df_temp_metric], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "                    df_aq_sites_mk_mag = pd.concat([df_aq_sites_mk_mag, df_temp_mk_mag], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "                    df_aq_sites_mk_dur = pd.concat([df_aq_sites_mk_dur, df_temp_mk_dur], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "                    df_aq_sites_mk_intra = pd.concat([df_aq_sites_mk_intra, df_temp_mk_intra], axis=0, ignore_index=True).reset_index(drop=True)                    \n",
    "                     \n",
    "                except Exception as e:\n",
    "                    print(\"ERROR: \", e)\n",
    "                    print(state_uri)\n",
    "\n",
    "# Filtering out invalid sites by data range and duplicates as some sites are listed in 2+ states   \n",
    "# Also adds a column to indicate presence in HCDN-2009\n",
    "#df_aq_sites = df_aq_sites.drop_duplicates(subset=['site_no'])\n",
    "df_aq_sites_metric = fn.gages_2_filtering(df_aq_sites_metric)\n",
    "\n",
    "try:\n",
    "    with open(f'Prelim_Data/{curr_aquifer.name}_Blacklist.txt', 'w') as f:\n",
    "        master_blacklist = [\"'\" + str(x) + \"'\" for x in master_blacklist]        \n",
    "        f.write(', '.join(master_blacklist))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "#df_aq_sites_metric.to_csv(f'{curr_aquifer.name}_Raw.csv') \n",
    "\n",
    "fn.save_data(df_aq_sites_metric, df_aq_sites_mk_mag, df_aq_sites_mk_dur, df_aq_sites_mk_intra, curr_aquifer.name)  \n",
    "#print(df_aq_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: National Metrics Dataset\n",
    "Generates a nationwide dataset for all sites meeting datarange criteria (lengthy runtime!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Prelim_Data/_National_Metrics/National_Sites_Blacklist.txt'\n"
     ]
    }
   ],
   "source": [
    "state_limit = 50\n",
    "dataset_name = 'National_Metrics'\n",
    "shapefile_path = 'ShapeFiles/Lower48/lower48.shp'\n",
    "\n",
    "# States with few sites for testing purposes\n",
    "test_state_list = ['ME', 'DE']\n",
    "\n",
    "# Set to False to ignore the blacklist and analyze all sites. This could be occasionally\n",
    "# worth doing as USGS could update site data making it valid in the future\n",
    "allow_blacklist = True\n",
    "\n",
    "curr_blacklist = []\n",
    "try:\n",
    "    with open('Prelim_Data/_National_Metrics/National_Sites_Blacklist.txt', 'r') as file:\n",
    "        curr_blacklist = file.read().split(', ')\n",
    "        curr_blacklist = [x.replace(\"'\", \"\") for x in curr_blacklist]\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on AL---]\n",
      "Total Sites: 173 in the state of AL\n",
      "Working on AL site 1/173 (02339495)\n",
      "IGNORED: Not enough data for site 02339495\n",
      "Working on AL site 2/173 (02342500)\n",
      "Working on AL site 3/173 (02342937)\n",
      "IGNORED: No data for site 02342937\n",
      "Working on AL site 4/173 (02361000)\n",
      "Working on AL site 5/173 (02361500)\n",
      "Working on AL site 6/173 (02363000)\n",
      "Working on AL site 7/173 (02364500)\n",
      "Working on AL site 8/173 (02369800)\n",
      "Working on AL site 9/173 (02371500)\n",
      "Working on AL site 10/173 (02372250)\n",
      "IGNORED: Not enough data for site 02372250\n",
      "Working on AL site 11/173 (02372422)\n",
      "IGNORED: Not enough data for site 02372422\n",
      "Working on AL site 12/173 (02372430)\n",
      "IGNORED: Not enough data for site 02372430\n",
      "Working on AL site 13/173 (02373000)\n",
      "Working on AL site 14/173 (02374250)\n",
      "IGNORED: Not enough data for site 02374250\n",
      "Working on AL site 15/173 (02374500)\n",
      "Working on AL site 16/173 (02374700)\n",
      "IGNORED: Not enough data for site 02374700\n",
      "Working on AL site 17/173 (02374745)\n",
      "IGNORED: Not enough data for site 02374745\n",
      "Working on AL site 18/173 (02374950)\n",
      "IGNORED: Not enough data for site 02374950\n",
      "Working on AL site 19/173 (02375000)\n",
      "Working on AL site 20/173 (02376500)\n",
      "Working on AL site 21/173 (02377570)\n",
      "IGNORED: Not enough data for site 02377570\n",
      "Working on AL site 22/173 (02378170)\n",
      "IGNORED: Not enough data for site 02378170\n",
      "Working on AL site 23/173 (02378185)\n",
      "IGNORED: No data for site 02378185\n",
      "Working on AL site 24/173 (02378300)\n",
      "IGNORED: Not enough data for site 02378300\n",
      "Working on AL site 25/173 (02378500)\n",
      "Working on AL site 26/173 (0237854520)\n",
      "IGNORED: No data for site 0237854520\n",
      "Working on AL site 27/173 (02397530)\n",
      "IGNORED: Not enough data for site 02397530\n",
      "Working on AL site 28/173 (02398300)\n",
      "Working on AL site 29/173 (02399200)\n",
      "Working on AL site 30/173 (02400100)\n",
      "Working on AL site 31/173 (02400680)\n",
      "IGNORED: Not enough data for site 02400680\n",
      "Working on AL site 32/173 (02401000)\n",
      "Working on AL site 33/173 (02401390)\n",
      "Working on AL site 34/173 (02401895)\n",
      "IGNORED: Not enough data for site 02401895\n",
      "Working on AL site 35/173 (02403170)\n",
      "IGNORED: No data for site 02403170\n",
      "Working on AL site 36/173 (02403310)\n",
      "IGNORED: Not enough data for site 02403310\n",
      "Working on AL site 37/173 (02404400)\n",
      "Working on AL site 38/173 (02405500)\n",
      "IGNORED: No 00060_Mean data for site 02405500\n",
      "Working on AL site 39/173 (02406500)\n",
      "Working on AL site 40/173 (02406930)\n",
      "IGNORED: Not enough data for site 02406930\n",
      "Working on AL site 41/173 (02407000)\n",
      "Working on AL site 42/173 (02407514)\n",
      "IGNORED: Not enough data for site 02407514\n",
      "Working on AL site 43/173 (02408150)\n",
      "IGNORED: Not enough data for site 02408150\n",
      "Working on AL site 44/173 (02408540)\n",
      "IGNORED: Not enough data for site 02408540\n",
      "Working on AL site 45/173 (02411590)\n",
      "IGNORED: No data for site 02411590\n",
      "Working on AL site 46/173 (02412000)\n",
      "Working on AL site 47/173 (02413300)\n",
      "IGNORED: Not enough data for site 02413300\n",
      "Working on AL site 48/173 (02414300)\n",
      "IGNORED: No data for site 02414300\n",
      "Working on AL site 49/173 (02414500)\n",
      "Working on AL site 50/173 (02414715)\n",
      "IGNORED: Not enough data for site 02414715\n",
      "Working on AL site 51/173 (02415000)\n",
      "Working on AL site 52/173 (02416360)\n",
      "IGNORED: No data for site 02416360\n",
      "Working on AL site 53/173 (02418230)\n",
      "IGNORED: Not enough data for site 02418230\n",
      "Working on AL site 54/173 (02418760)\n",
      "IGNORED: Not enough data for site 02418760\n",
      "Working on AL site 55/173 (02419000)\n",
      "Working on AL site 56/173 (02419890)\n",
      "IGNORED: Not enough data for site 02419890\n",
      "Working on AL site 57/173 (02420000)\n",
      "Working on AL site 58/173 (02421000)\n",
      "Working on AL site 59/173 (02421480)\n",
      "IGNORED: No data for site 02421480\n",
      "Working on AL site 60/173 (02422500)\n",
      "Working on AL site 61/173 (02423110)\n",
      "IGNORED: No data for site 02423110\n",
      "Working on AL site 62/173 (02423130)\n",
      "IGNORED: Not enough data for site 02423130\n",
      "Working on AL site 63/173 (02423160)\n",
      "IGNORED: Not enough data for site 02423160\n",
      "Working on AL site 64/173 (02423380)\n",
      "IGNORED: Not enough data for site 02423380\n",
      "Working on AL site 65/173 (02423397)\n",
      "IGNORED: Not enough data for site 02423397\n",
      "Working on AL site 66/173 (02423400)\n",
      "IGNORED: Not enough data for site 02423400\n",
      "Working on AL site 67/173 (02423414)\n",
      "IGNORED: Not enough data for site 02423414\n",
      "Working on AL site 68/173 (02423425)\n",
      "IGNORED: Not enough data for site 02423425\n",
      "Working on AL site 69/173 (02423496)\n",
      "IGNORED: Not enough data for site 02423496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIGNORED: Site \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msite_no\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is in aquifer blacklist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mnwis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43msites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msite_no\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSERVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameterCD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPARAM_CODE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT_START\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT_END\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorking on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m site \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msite_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_state_sites)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msite_no\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nwis.py:1007\u001b[0m, in \u001b[0;36mget_record\u001b[1;34m(sites, start, end, multi_index, wide_format, datetime_index, state, service, **kwargs)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m service \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1007\u001b[0m     df, _ \u001b[38;5;241m=\u001b[39m get_dv(sites\u001b[38;5;241m=\u001b[39msites, startDT\u001b[38;5;241m=\u001b[39mstart, endDT\u001b[38;5;241m=\u001b[39mend,\n\u001b[0;32m   1008\u001b[0m                    multi_index\u001b[38;5;241m=\u001b[39mmulti_index, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m service \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqwdata\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nwis.py:544\u001b[0m, in \u001b[0;36mget_dv\u001b[1;34m(sites, start, end, multi_index, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m end \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendDT\u001b[39m\u001b[38;5;124m'\u001b[39m, end)\n\u001b[0;32m    543\u001b[0m sites \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msites\u001b[39m\u001b[38;5;124m'\u001b[39m, sites)\n\u001b[1;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _dv(startDT\u001b[38;5;241m=\u001b[39mstart, endDT\u001b[38;5;241m=\u001b[39mend, sites\u001b[38;5;241m=\u001b[39msites,\n\u001b[0;32m    545\u001b[0m            multi_index\u001b[38;5;241m=\u001b[39mmulti_index, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nwis.py:549\u001b[0m, in \u001b[0;36m_dv\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dv\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 549\u001b[0m     response \u001b[38;5;241m=\u001b[39m query_waterservices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    550\u001b[0m     df \u001b[38;5;241m=\u001b[39m _read_json(response\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m format_response(df, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), _set_metadata(response, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nwis.py:498\u001b[0m, in \u001b[0;36mquery_waterservices\u001b[1;34m(service, **kwargs)\u001b[0m\n\u001b[0;32m    494\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrdb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    496\u001b[0m url \u001b[38;5;241m=\u001b[39m WATERSERVICE_URL \u001b[38;5;241m+\u001b[39m service\n\u001b[1;32m--> 498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\utils.py:196\u001b[0m, in \u001b[0;36mquery\u001b[1;34m(url, payload, delimiter)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m#for index in range(len(payload)):\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m#    key, value = payload[index]\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m#    payload[index] = (key, to_str(value))\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# define the user agent for the query\u001b[39;00m\n\u001b[0;32m    193\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython-dataretrieval/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataretrieval\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m--> 196\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad Request, check that your parameters are correct. URL: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(response\u001b[38;5;241m.\u001b[39murl))\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1053\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[0;32m   1056\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1057\u001b[0m         (\n\u001b[0;32m   1058\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1063\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1064\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_default_certs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    416\u001b[0m ):\n\u001b[0;32m    417\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs()\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# for the host.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    436\u001b[0m     default_ssl_context\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39mversion() \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    440\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\util\\ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    437\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    445\u001b[0m         SNIMissingWarning,\n\u001b[0;32m    446\u001b[0m     )\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m send_sni:\n\u001b[1;32m--> 449\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\util\\ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m server_hostname:\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    508\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    510\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1071\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   1069\u001b[0m             \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1071\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1342\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[0;32m   1341\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1342\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_natl_metrics = pd.DataFrame()\n",
    "df_natl_mk_mag = pd.DataFrame()\n",
    "df_natl_mk_dur = pd.DataFrame()\n",
    "df_natl_mk_intra = pd.DataFrame()\n",
    "\n",
    "natl_blacklist = []\n",
    "\n",
    "for state_index, state in enumerate(fn.STATE_LIST):\n",
    "    if state_index >= state_limit: break\n",
    "    \n",
    "    # Initial site list per state. Modified version of filter_state_site() function\n",
    "    print(f'[---Working on {state}---]')\n",
    "    state_uri = fn.create_state_uri(state, fn.PARAM_CODE)\n",
    "    df_state_sites = filter_state_site(shapefile_path, state_uri)\n",
    "    df_state_sites = df_state_sites.reset_index()\n",
    "    print(f'Total Sites: {len(df_state_sites)} in the state of {state}')\n",
    "    \n",
    "    # Modified version of the create_multi_site_data() function\n",
    "    for site_index, row in df_state_sites.iterrows():\n",
    "        \n",
    "        if allow_blacklist and str(row['site_no']) in curr_blacklist:\n",
    "            print(f'IGNORED: Site {row[\"site_no\"]} is in aquifer blacklist')\n",
    "            continue\n",
    "        \n",
    "        df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "        df = df.reset_index()\n",
    "        print(f'Working on {state} site {site_index + 1}/{len(df_state_sites)} ({row[\"site_no\"]})')\n",
    "        \n",
    "        if df.empty:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: No data for site {row[\"site_no\"]}')\n",
    "            continue\n",
    "        \n",
    "        if '00060_Mean' not in df.columns:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: No 00060_Mean data for site {row[\"site_no\"]}')\n",
    "            continue\n",
    "        \n",
    "        start = df['datetime'].min().date()\n",
    "        end = df['datetime'].max().date()\n",
    "        range = round((end - start).days / 365.25, 1)\n",
    "        \n",
    "        if range < fn.MIN_DATA_PERIOD:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f'IGNORED: Not enough data for site {row[\"site_no\"]}')\n",
    "            continue\n",
    "        \n",
    "        # A few very broken sites with almost no data can have 0 hmf years and cause errors (i.e. '03592000')\n",
    "        # so we'll catch these and add them to the site blacklist\n",
    "        try:\n",
    "            df_single_site_metric, df_mk_mag, df_mk_dur, df_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "            add_data = {'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va'], 'data_start': start, 'data_end': end, 'total_record': range, 'state': row['STUSPS']}\t\t\t\n",
    "            add_data = pd.DataFrame(add_data, index=['0'])\n",
    "        except Exception as e:\n",
    "            natl_blacklist.append(row['site_no'])\n",
    "            print(f\"ERROR: Single site data failure for site {row['site_no']}:\\n{e}\")\n",
    "            continue\n",
    "        \n",
    "        add_data = pd.DataFrame(np.tile(add_data.values, (len(df_single_site_metric), 1)), columns=add_data.columns)\n",
    "        df_single_site_metric = pd.concat([df_single_site_metric.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "        # Append single site data to multi-site dataframes\n",
    "        df_natl_metrics = pd.concat([df_natl_metrics, df_single_site_metric], ignore_index=True)\n",
    "        df_natl_mk_mag = pd.concat([df_natl_mk_mag, df_mk_mag], ignore_index=True)\n",
    "        df_natl_mk_dur = pd.concat([df_natl_mk_dur, df_mk_dur], ignore_index=True)\n",
    "        df_natl_mk_intra = pd.concat([df_natl_mk_intra, df_mk_intra], ignore_index=True)\n",
    "        \n",
    "df_natl_metrics = fn.gages_2_filtering(df_natl_metrics)\n",
    "\n",
    "# Create national site blacklist\n",
    "try:\n",
    "    with open(f'Prelim_Data/National_Sites_Blacklist.txt', 'w') as f:\n",
    "        natl_blacklist = [\"'\" + str(x) + \"'\" for x in natl_blacklist]        \n",
    "        f.write(', '.join(natl_blacklist))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:    \n",
    "    fn.save_data(df_natl_metrics, df_natl_mk_mag, df_natl_mk_dur, df_natl_mk_intra, dataset_name)\n",
    "except Exception as e:\n",
    "    print(f'Error saving data: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: Nationwide Validity Dataset\n",
    "Generates a dataset for every water gauge with 00060_Mean data in the contiguous US indicating gauge validity for this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = 30\n",
    "shapefile_path = 'ShapeFiles/Lower48/lower48.shp'\n",
    "test_limit = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on AL---]\n",
      "Total Sites: 173 in the state of AL\n",
      "Ignored 35 sites (20.23%)\n",
      "[---Working on AZ---]\n",
      "Total Sites: 218 in the state of AZ\n",
      "Ignored 11 sites (5.05%)\n",
      "[---Working on AR---]\n",
      "Total Sites: 139 in the state of AR\n",
      "Ignored 16 sites (11.51%)\n",
      "[---Working on CA---]\n",
      "Total Sites: 481 in the state of CA\n",
      "Ignored 54 sites (11.23%)\n",
      "[---Working on CO---]\n",
      "Total Sites: 349 in the state of CO\n",
      "Ignored 14 sites (4.01%)\n",
      "[---Working on CT---]\n",
      "Total Sites: 74 in the state of CT\n",
      "Ignored 4 sites (5.41%)\n",
      "[---Working on DE---]\n",
      "Total Sites: 36 in the state of DE\n",
      "Ignored 3 sites (8.33%)\n",
      "[---Working on FL---]\n",
      "Total Sites: 434 in the state of FL\n",
      "Ignored 39 sites (8.99%)\n",
      "[---Working on GA---]\n",
      "Total Sites: 308 in the state of GA\n",
      "Ignored 17 sites (5.52%)\n",
      "[---Working on ID---]\n",
      "Total Sites: 229 in the state of ID\n",
      "Ignored 9 sites (3.93%)\n",
      "[---Working on IL---]\n",
      "Total Sites: 193 in the state of IL\n",
      "Ignored 7 sites (3.63%)\n",
      "[---Working on IN---]\n",
      "Total Sites: 205 in the state of IN\n",
      "Ignored 20 sites (9.76%)\n",
      "[---Working on IA---]\n",
      "Total Sites: 143 in the state of IA\n",
      "Ignored 7 sites (4.9%)\n",
      "[---Working on KS---]\n",
      "Total Sites: 195 in the state of KS\n",
      "Ignored 5 sites (2.56%)\n",
      "[---Working on KY---]\n",
      "Total Sites: 169 in the state of KY\n",
      "Ignored 8 sites (4.73%)\n",
      "[---Working on LA---]\n",
      "Total Sites: 98 in the state of LA\n",
      "Ignored 27 sites (27.55%)\n",
      "[---Working on ME---]\n",
      "Total Sites: 68 in the state of ME\n",
      "Ignored 0 sites (0.0%)\n",
      "[---Working on MD---]\n",
      "Total Sites: 190 in the state of MD\n",
      "Ignored 8 sites (4.21%)\n",
      "[---Working on MA---]\n",
      "Total Sites: 146 in the state of MA\n",
      "Ignored 15 sites (10.27%)\n",
      "[---Working on MI---]\n",
      "Total Sites: 197 in the state of MI\n",
      "Ignored 16 sites (8.12%)\n",
      "[---Working on MN---]\n",
      "Total Sites: 130 in the state of MN\n",
      "Ignored 7 sites (5.38%)\n",
      "[---Working on MS---]\n",
      "Total Sites: 99 in the state of MS\n",
      "Ignored 16 sites (16.16%)\n",
      "[---Working on MO---]\n",
      "Total Sites: 244 in the state of MO\n",
      "Ignored 2 sites (0.82%)\n",
      "[---Working on MT---]\n",
      "Total Sites: 226 in the state of MT\n",
      "Ignored 7 sites (3.1%)\n",
      "[---Working on NE---]\n",
      "Total Sites: 120 in the state of NE\n",
      "Ignored 1 sites (0.83%)\n",
      "[---Working on NV---]\n",
      "Total Sites: 185 in the state of NV\n",
      "Ignored 9 sites (4.86%)\n",
      "[---Working on NH---]\n",
      "Total Sites: 101 in the state of NH\n",
      "Ignored 2 sites (1.98%)\n",
      "[---Working on NJ---]\n",
      "Total Sites: 141 in the state of NJ\n",
      "Ignored 10 sites (7.09%)\n",
      "[---Working on NM---]\n",
      "Total Sites: 174 in the state of NM\n",
      "Ignored 10 sites (5.75%)\n",
      "[---Working on NY---]\n",
      "Total Sites: 321 in the state of NY\n",
      "Ignored 16 sites (4.98%)\n",
      "[---Working on NC---]\n",
      "Total Sites: 240 in the state of NC\n",
      "Ignored 10 sites (4.17%)\n",
      "[---Working on ND---]\n",
      "Total Sites: 123 in the state of ND\n",
      "Ignored 9 sites (7.32%)\n",
      "[---Working on OH---]\n",
      "Total Sites: 222 in the state of OH\n",
      "Ignored 16 sites (7.21%)\n",
      "[---Working on OK---]\n",
      "Total Sites: 178 in the state of OK\n",
      "Ignored 9 sites (5.06%)\n",
      "[---Working on OR---]\n",
      "Total Sites: 213 in the state of OR\n",
      "Ignored 7 sites (3.29%)\n",
      "[---Working on PA---]\n",
      "Total Sites: 311 in the state of PA\n",
      "Ignored 6 sites (1.93%)\n",
      "[---Working on RI---]\n",
      "Total Sites: 34 in the state of RI\n",
      "Ignored 2 sites (5.88%)\n",
      "[---Working on SC---]\n",
      "Total Sites: 131 in the state of SC\n",
      "Ignored 12 sites (9.16%)\n",
      "[---Working on SD---]\n",
      "Total Sites: 122 in the state of SD\n",
      "Ignored 4 sites (3.28%)\n",
      "[---Working on TN---]\n",
      "Total Sites: 136 in the state of TN\n",
      "Ignored 15 sites (11.03%)\n",
      "[---Working on TX---]\n",
      "Total Sites: 597 in the state of TX\n",
      "Ignored 64 sites (10.72%)\n",
      "[---Working on UT---]\n",
      "Total Sites: 153 in the state of UT\n",
      "Ignored 4 sites (2.61%)\n",
      "[---Working on VT---]\n",
      "Total Sites: 98 in the state of VT\n",
      "Ignored 2 sites (2.04%)\n",
      "[---Working on VA---]\n",
      "Total Sites: 200 in the state of VA\n",
      "Ignored 8 sites (4.0%)\n",
      "[---Working on WA---]\n",
      "Total Sites: 265 in the state of WA\n",
      "Ignored 11 sites (4.15%)\n",
      "[---Working on WV---]\n",
      "Total Sites: 110 in the state of WV\n",
      "Ignored 2 sites (1.82%)\n",
      "[---Working on WI---]\n",
      "Total Sites: 220 in the state of WI\n",
      "Ignored 31 sites (14.09%)\n",
      "[---Working on WY---]\n",
      "Total Sites: 116 in the state of WY\n",
      "Ignored 1 sites (0.86%)\n"
     ]
    }
   ],
   "source": [
    "df_natl_validity = pd.DataFrame()\n",
    "\n",
    "for i, state in enumerate(fn.STATE_LIST):\n",
    "    if i >= test_limit: break\n",
    "    print(f'[---Working on {state}---]')\n",
    "    state_uri = fn.create_state_uri(state, fn.PARAM_CODE)\n",
    "    df_state_sites = filter_state_site(shapefile_path, state_uri)\n",
    "    print(f'Total Sites: {len(df_state_sites)} in the state of {state}')\n",
    "    \n",
    "    ignored_count = 0\n",
    "    for loc, row in df_state_sites.iterrows():\n",
    "        df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        if '00060_Mean' not in df.columns:\n",
    "            ignored_count += 1\n",
    "            continue\n",
    "        \n",
    "        start = df['datetime'].min().date()\n",
    "        end = df['datetime'].max().date()\n",
    "        range = round((end - start).days / 365.25, 1)\n",
    "        \n",
    "        valid_range = range > date_range\n",
    "        \n",
    "        date_threshold = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * date_range)\n",
    "        df = df[df['datetime'].dt.date >= date_threshold]\n",
    "        missing = fn.validate(df, date_threshold, fn.DEFAULT_END)\n",
    "        valid = missing < fn.MAX_MISSING_THRESHOLD\n",
    "        \n",
    "        data = {'site_no': row['site_no'], 'state': state, 'data_range': valid_range, 'data_cont': valid, 'start': start, 'end': end, \n",
    "                'total_record': range, 'missing': missing, 'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va']}\n",
    "        \n",
    "        df_natl_validity = pd.concat([df_natl_validity, pd.DataFrame(data, index=['0'])], axis=0, ignore_index=True)\n",
    "        \n",
    "    print(f'Ignored {ignored_count} sites ({round((ignored_count / len(df_state_sites)) * 100, 2)}%)')        \n",
    "        \n",
    "try:\n",
    "    with pd.ExcelWriter('Prelim_Data/National_Validity_30.xlsx') as writer:\n",
    "        df_natl_validity.to_excel(writer, sheet_name='national_validity', index=False) \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
