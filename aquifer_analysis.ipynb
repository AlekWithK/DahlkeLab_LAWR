{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES:\n",
    "<ul>\n",
    "<li>Sections labeled <strong>CORE</strong> must be run in order to begin Aquifer Analysis</li>\n",
    "<li>Cells labeled <strong>Control</strong> contain inputs for the immeadiately proceeding section(s)</li>\n",
    "<li>Sections labeled <strong>EXTRA</strong> contain additional plotting or analysis tools but are not necessary for Aquifer Analysis</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nadp.py:44: UserWarning: GDAL not installed. Some functions will not work.\n",
      "  warnings.warn('GDAL not installed. Some functions will not work.')\n"
     ]
    }
   ],
   "source": [
    "#Python3.10\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import contextily as cx\n",
    "import requests\n",
    "import calendar\n",
    "from importlib import reload\n",
    "from typing import IO\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import Point\n",
    "from io import StringIO\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# USGS Data retreival tool\n",
    "from dataretrieval import nwis, utils, codes\n",
    "\n",
    "# Custom modules are imported in multiple locations to faciliate easy reloading when edits are made to their respective files\n",
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "reload(fn)\n",
    "\n",
    "# TODO: Look into the warning that this is disabling. It doesn't appear to be significant for the purposes of this code but should be understood\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#pd.options.mode.chained_assignment = 'warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Single Site Data<br>\n",
    "This function produces the streamflow analysis (using the functions above) for a single site given a list of thresholds and time windows to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Src.classes as cl\n",
    "import Src.func as fn\n",
    "reload(cl)\n",
    "\n",
    "# df2 holds all-time data, df is analyzed range\n",
    "curr_guage = cl.SRB_Guage\n",
    "df = nwis.get_record(sites=cl.SRB_Guage.id, service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2014-09-30')\n",
    "df2 = nwis.get_record(sites=curr_guage.id, service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end='2014-09-30')\n",
    "\n",
    "# Set to true if running indepedently to verify data with Kocis paper (or other source)\n",
    "# Set to false if running Aquifer Analysis\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Site Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site No: 0    11447650\n",
      "1    11447650\n",
      "2    11447650\n",
      "0    11447650\n",
      "Name: site_no, dtype: object\n",
      "Analyzed Range: 24\n",
      "24\n",
      "44\n",
      "44\n",
      "Quantile: 0.90\n",
      "0.95\n",
      "0.90\n",
      "0.95\n",
      "Valid: False\n",
      "False\n",
      "False\n",
      "False\n",
      "% Missing: 0.199963\n",
      "0.199963\n",
      "0.119976\n",
      "0.119976\n",
      "90%: 47100.0\n",
      "68300.0\n",
      "49900.0\n",
      "69400.0\n",
      "HMF Years: 16\n",
      "12\n",
      "28\n",
      "23\n",
      "Annual Duration: 54.750000\n",
      "36.500000\n",
      "57.357143\n",
      "34.739130\n",
      "Event Duration: 20.776042\n",
      "16.370833\n",
      "23.264881\n",
      "13.034369\n",
      "Event HMF: 1.102463\n",
      "0.345665\n",
      "1.151843\n",
      "0.278920\n",
      "Inter-annual Frequency: 0.666667\n",
      "0.500000\n",
      "0.636364\n",
      "0.522727%\n",
      "Intra-annual Frequency: 2.750000\n",
      "2.250000\n",
      "2.714286\n",
      "2.826087\n",
      "Total HMF in km^3/year: 2.739721\n",
      "0.790387\n",
      "2.743677\n",
      "0.749886\n",
      "Center of Mass: 144.437500\n",
      "152.416667\n",
      "139.571429\n",
      "145.086957\n",
      "6 Month HMF in km^3/year: 2.491944\n",
      "0.745227\n",
      "2.584929\n",
      "0.726846\n",
      "3 Month HMF in km^3/year: 1.502870\n",
      "0.500732\n",
      "1.527563\n",
      "0.481837\n"
     ]
    }
   ],
   "source": [
    "# If looking at a post impairment period, but calculating threshold based on the full record of data, pass a second dataframe with a different \n",
    "# start/end date as the final parameter. This method was used in Kocis 2017 and is needed for some data verification, but is not the methodology\n",
    "# used for the Aquifer Analysis and so *args will most often be empty.\n",
    "import Src.func as fn\n",
    "reload(fn)\n",
    "\n",
    "def single_site_data(df: pd.DataFrame, quantiles_list: list, data_ranges_list: list, *args):\n",
    "    df = df.reset_index()    \n",
    "    threshold = None\n",
    "    df_complete_site_data = pd.DataFrame()    \n",
    "    df_complete_mk_mag = pd.DataFrame()\n",
    "    df_complete_mk_dur = pd.DataFrame()\n",
    "    df_complete_mk_intra = pd.DataFrame()\n",
    "    \n",
    "    for data_range in data_ranges_list:\n",
    "        for quantile in quantiles_list:\n",
    "            # Copying original dataframe to avoid any conflicts\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Validate that site is not missing > 10% of data\n",
    "            date_threshold = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * data_range)              \n",
    "            \n",
    "            # Filter dataframe based on current data_range (don't do this during testing if testing unique dataset/range)\n",
    "            if not testing:\n",
    "                df_copy = df_copy[df_copy['datetime'].dt.date >= date_threshold]\n",
    "            \n",
    "            # Validate <10% missing data based on filtered dataframe\n",
    "            missing = fn.validate(df_copy, date_threshold, fn.DEFAULT_END)\n",
    "            valid = missing < fn.MAX_MISSING_THRESHOLD                               \n",
    "    \n",
    "            # Check for optional second dataframe containing full record\n",
    "            if args and isinstance(args[0], pd.DataFrame) and not args[0].empty:\n",
    "                #print('Threshold calculation across the full record')\n",
    "                df2 = args[0].reset_index()\n",
    "                threshold = fn.calc_threshold(df2, quantile)\n",
    "                # threshold = 52350 # SRB_Guage post-impairment threshold for verification with Kocis_2017 \n",
    "            else:\n",
    "                #print('Threshold calculation across a limited record')\n",
    "                threshold = fn.calc_threshold(df_copy, quantile)                             \n",
    "\n",
    "            # Create a dataframe with only days over HMF threshold as well as continuous dataframe for MK trend test\n",
    "            hmf_series_defl, hmf_series_cont = fn.filter_hmf(df_copy, threshold)\n",
    "            #print(zero_deflated_hmf)\n",
    "            \n",
    "            # Aggregate data before performing MK magnitude test\n",
    "            df_agg_cont = hmf_series_cont.copy()\n",
    "            df_agg_cont = fn.convert_hmf(df_agg_cont, threshold)\n",
    "            df_agg_cont['00060_Mean'] = df_agg_cont['00060_Mean'].apply(lambda x: x * fn.CUBIC_FT_KM_FACTOR if x >= 0 else 0)\n",
    "            df_agg_cont.set_index('datetime', inplace=True)\n",
    "            df_agg_cont = df_agg_cont.resample(fn.HYDRO_YEAR).agg({'00060_Mean': ['sum', 'count']})\n",
    "            df_agg_cont.columns = ['Sum', 'Count']\n",
    "            df_agg_cont_defl = df_agg_cont[df_agg_cont['Sum'] > 0]          \n",
    "            \n",
    "            # MK Magnitude\n",
    "            df_mk_mag = fn.mann_kendall(df_agg_cont_defl['Sum'], df_agg_cont['Sum'], fn.MK_TREND_ALPHA)\n",
    "\n",
    "            # Find number of years with HMF (10 for October 1st)\n",
    "            hmf_years = fn.num_hmf_years(hmf_series_defl, 10)            \n",
    "\n",
    "            # Mask out months that don't fall within 3 and 6 month Winter range\n",
    "            df_six_month, df_three_month = fn.three_six_range(hmf_series_defl, 12, 2, 11, 4)\n",
    "\n",
    "            # Convert to daily average flow in cfpd, and take only flow above the threshold\n",
    "            hmf_series_defl = fn.convert_hmf(hmf_series_defl, threshold)\n",
    "            total_hmf_flow = hmf_series_defl[\"00060_Mean\"].sum()\n",
    "            hmf_per_month = fn.monthly_hmf(hmf_series_defl, data_range, quantile)\n",
    "\n",
    "            # Calculate 3 and 6 month HMF\n",
    "            df_six_month = fn.convert_hmf(df_six_month, threshold)\n",
    "            six_month_hmf = df_six_month[\"00060_Mean\"].sum()\n",
    "            df_three_month = fn.convert_hmf(df_three_month, threshold)\n",
    "            three_month_hmf = df_three_month[\"00060_Mean\"].sum()\n",
    "            \n",
    "            total_hmf_flow = (total_hmf_flow * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            six_month_hmf = (six_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "            three_month_hmf = (three_month_hmf * fn.CUBIC_FT_KM_FACTOR) / hmf_years\n",
    "\n",
    "            # Inter-annual\n",
    "            hmf_series_full, _ = fn.filter_hmf(df_copy, threshold)\n",
    "            hmf_years_full = fn.num_hmf_years(hmf_series_full, 10)\n",
    "\n",
    "            delta = df_copy['datetime'].max().year - df_copy['datetime'].min().year\n",
    "            inter_annual = (hmf_years_full / delta)\n",
    "\n",
    "            # Average Duration and Intra-annual Frequency\n",
    "            hmf_series_cont = fn.convert_hmf(hmf_series_cont, threshold)\n",
    "            event_duration, annual_duration, intra_annual, event_hmf, df_results = fn.calc_duration_intra_annual(hmf_series_cont, hmf_years)\n",
    "            dur_series_defl = df_results['duration'][df_results['duration'] > 0]\n",
    "            dur_series_cont = df_results['duration']\n",
    "            df_mk_dur = fn.mann_kendall(dur_series_defl, dur_series_cont, fn.MK_TREND_ALPHA)\n",
    "                        \n",
    "            intra_series_defl = df_results['total_events'][df_results['total_events'] > 0]\n",
    "            intra_series_cont = df_results['total_events']\n",
    "            df_mk_intra = fn.mann_kendall(intra_series_defl, intra_series_cont, fn.MK_TREND_ALPHA) \n",
    "            \n",
    "            # Timing Calculation using DOHY\n",
    "            timing = fn.calc_timing(hmf_series_defl)           \n",
    "\n",
    "            # TODO: One-day peaks (avg. # of times hmf occurs on one day only)          \n",
    "            \n",
    "            # Merging site dataframe with Mann-Kendall dataframe. This start date is the beginning of the actual data, not necessarily \n",
    "            # the beginning of the analyzed range. Validation (above) starts from the start of the official range (1970/90-2020)\n",
    "            start = df_copy['datetime'].min().date()\n",
    "            end = df_copy['datetime'].max().date()\n",
    "            data = {'dataset_ID': (data_range * quantile), 'site_no': df_copy.iloc[0]['site_no'], 'analyze_start': start, 'analyze_end': end, 'analyze_range': delta, 'quantile': quantile, \n",
    "                    'valid': valid, 'missing': missing, 'threshold': threshold, 'hmf_years': hmf_years, 'annual_hmf': total_hmf_flow, 'six_mo_hmf': six_month_hmf, 'three_mo_hmf': three_month_hmf, \n",
    "                    'annual_duration': annual_duration, 'event_duration': event_duration, 'event_hmf': event_hmf, 'inter_annual': inter_annual, 'intra_annual': intra_annual, 'timing': timing}              \n",
    "            \n",
    "            # Merging MK magnitiude\n",
    "            df_mk_mag.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_mag.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_mag = pd.concat([df_complete_mk_mag.reset_index(drop=True), df_mk_mag.reset_index(drop=True)], axis=0) \n",
    "            \n",
    "            # Merging MK duration\n",
    "            df_mk_dur.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_dur.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_dur = pd.concat([df_complete_mk_dur.reset_index(drop=True), df_mk_dur.reset_index(drop=True)], axis=0)   \n",
    "            \n",
    "            # Merging MK intra-annual\n",
    "            df_mk_intra.insert(0, 'dataset_ID', data_range * quantile)\n",
    "            df_mk_intra.insert(1, 'site_no', df_copy.iloc[0]['site_no'])\n",
    "            df_complete_mk_intra = pd.concat([df_complete_mk_intra.reset_index(drop=True), df_mk_intra.reset_index(drop=True)], axis=0)        \n",
    "            \n",
    "            # Merging metric results\n",
    "            df_single_iteration = pd.DataFrame(data, index=['0'])\n",
    "            df_single_iteration = pd.concat([df_single_iteration.reset_index(drop=True), hmf_per_month.reset_index(drop=True)], axis=1)\n",
    "            df_complete_site_data = pd.concat([df_complete_site_data.reset_index(drop=True), df_single_iteration.reset_index(drop=True)], axis=0)\n",
    "        \n",
    "    return df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra\n",
    "\n",
    "# For testing purposes, to run this cell independently\n",
    "df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\n",
    "\n",
    "'''\n",
    "try:\n",
    "    with pd.ExcelWriter('df_single_site.xlsx') as writer:\n",
    "        df_complete_site_data.to_excel(writer, sheet_name='site_metrics', index=False)\n",
    "        df_complete_mk_mag.to_excel(writer, sheet_name='mk_magnitude', index=False)\n",
    "        df_complete_mk_intra.to_excel(writer, sheet_name='mk_intra', index=False)\n",
    "        df_complete_mk_dur.to_excel(writer, sheet_name='mk_duration', index=False)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "'''\n",
    "   \n",
    "fn.single_site_report(df_complete_site_data)\n",
    "df_complete_site_data = fn.gages_2_filtering(df_complete_site_data)\n",
    "#fn.save_data(df_complete_site_data, df_complete_mk_mag, df_complete_mk_dur, df_complete_mk_intra, 'TEST')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Multi-Site Filtering<br>\n",
    "This function creates the list of sites to analyze by filtering the complete list of state sites with 00060_Mean data down to those that lie within a specific watershed boundary using decimal long/lat positional data and a region shapefile (e.g. state or watershed boundary)<br><br>\n",
    "\n",
    "Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed when running the following cell independently\n",
    "shapefile_path = 'ShapeFiles/Aquifers/Central_Valley/HUC4/NHD_H_1802'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site List Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sites: 57 in the state of CA in the given WB\n",
      "['11337080', '11342000', '11345500', '11348500', '11355010', '11355500', '11370500', '11370700', '11372000', '11374000', '11376000', '11376550', '11377100', '11379500', '11381500', '11383500', '11389500', '11390000', '11390500', '11401920', '11402000', '11413000', '11418500', '11421000', '11424000', '11425500', '11427000', '11446500', '11447360', '11447650', '11447830', '11447850', '11447890', '11447905', '11448750', '11448800', '11449255', '11449500', '11451000', '11451100', '11451300', '11451715', '11451800', '11452500', '11452800', '11452900', '11453000', '11453500', '11453590', '11454000', '11455095', '11455140', '11455280', '11455315', '11455338', '11455385', '11455420']\n"
     ]
    }
   ],
   "source": [
    "def filter_state_site(shapefile_path: str, state_uri: str):\n",
    "    \"\"\"Creates a list of sites with over 50 years of 00060_Mean streamflow data for a given region\"\"\"\n",
    "    # Request page from USGS site, ignore all informational lines\n",
    "    response = requests.get(state_uri)\n",
    "    data = response.text\n",
    "    lines = data.splitlines()\n",
    "    lines = [line for line in lines if not line.startswith('#')]\n",
    "\n",
    "    # Create dataframe where site_no is a list of all sites in a state with 00060 data\n",
    "    tsd = \"\\n\".join(lines)\n",
    "    df = pd.read_csv(StringIO(tsd), sep='\\t')\n",
    "    df_state_sites = df.iloc[1:]\n",
    "        \n",
    "    # Filter out sites outside of HU boundary\n",
    "    if fn.SORT_BY_WB:\n",
    "        shapefile = gpd.read_file(shapefile_path)\n",
    "        df_state_sites['geometry'] = [Point(lon, lat) for lon, lat in zip(df_state_sites['dec_long_va'], df_state_sites['dec_lat_va'])]\n",
    "        gdf_data = gpd.GeoDataFrame(df_state_sites, crs=shapefile.crs)\n",
    "        df_state_sites = gpd.sjoin(gdf_data, shapefile, predicate='within')\n",
    "            \n",
    "    #print(df_state_sites.columns.to_list())\n",
    "    #print(df_state_sites)\n",
    "    \n",
    "    return df_state_sites\n",
    "\n",
    "df_state_sites = filter_state_site(shapefile_path, fn.SITES_URI)\n",
    "print(f'Total Sites: {len(df_state_sites)} in the state of {fn.STATE_CODE.upper()} in the given WB')\n",
    "site_list = df_state_sites['site_no'].to_list()\n",
    "print(site_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Multi-Site Data Creation<br>\n",
    "This function uses the list generated by the previous section to generate single site data for every site in the list, and validate that no site is missing more than 10% of its data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGNORED: Site 11337080 is in aquifer blacklist\n",
      "Added site 2 of 57\n",
      "Added site 3 of 57\n",
      "Max HMF for this region: 0.2\n",
      "8 site(s) valid out of 8\n"
     ]
    }
   ],
   "source": [
    "# REQUIRES: 'df_state_sites' from 'Multi-Site Filtering'\n",
    "# Used for now to limit runtime when running independently\n",
    "site_limit = 3\n",
    "\n",
    "def create_multi_site_data(df_state_sites: pd.DataFrame, site_limit: int, aquifer: cl.Aquifer):\n",
    "\t\"\"\"Generates detailed HMF, MK, and POS information for each site in the passed dataframe\"\"\"\n",
    "\t# Necessary for proper iterrows() behavior\n",
    "\tdf_state_sites.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\t# Creating the dataframe that will hold final results for mapping\n",
    "\tdf_multi_site_metric = pd.DataFrame()\n",
    "\tdf_multi_site_mk_mag = pd.DataFrame()\n",
    "\tdf_multi_site_mk_dur = pd.DataFrame()\n",
    "\tdf_multi_site_mk_intra = pd.DataFrame()\n",
    " \n",
    "\tsite_blacklist = []\n",
    "\n",
    "\tfor index, row in df_state_sites.iterrows():\n",
    "\t\twhile index < site_limit:\n",
    "      \n",
    "\t\t\tif str(row['site_no']) in aquifer.blacklist:\n",
    "\t\t\t\tprint(f'IGNORED: Site {row[\"site_no\"]} is in aquifer blacklist')\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\t# Create a dataframe for the current site in the iteration to perform calculations on\n",
    "\t\t\tdf = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "\t\t\tdf = df.reset_index()\n",
    "\t\t\t\n",
    "\t\t\t# Confirm that dataframe is not empty and has the required streamflow data before continuing\n",
    "\t\t\tif df.empty:\n",
    "\t\t\t\tsite_blacklist.append(row['site_no'])\n",
    "\t\t\t\tprint(f'IGNORED: No data for site {row[\"site_no\"]}')\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tif '00060_Mean' not in df.columns:\n",
    "\t\t\t\tsite_blacklist.append(row['site_no'])\n",
    "\t\t\t\tprint(f'IGNORED: No 00060_Mean data for site {row[\"site_no\"]}')\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\t# Filter out sites with less than the minimum required range of data\n",
    "\t\t\tstart = df['datetime'].min().date()\n",
    "\t\t\tend = df['datetime'].max().date()\n",
    "\t\t\trange = round((end - start).days / 365.25, 1)\n",
    "\t\t\t\n",
    "\t\t\t# Ignore sites with less than the minimum required years of data\n",
    "\t\t\tif range < fn.MIN_DATA_PERIOD:\n",
    "\t\t\t\tsite_blacklist.append(row['site_no'])\n",
    "\t\t\t\tprint(f'IGNORED: Not enough data for site {row[\"site_no\"]}')\n",
    "\t\t\t\tbreak\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\tdf_single_site_metric, df_mk_mag, df_mk_dur, df_mk_intra = single_site_data(df, fn.QUANTILE_LIST, fn.DATA_RANGE_LIST)\t\t\t\n",
    "\t\t\t\n",
    "\t\t\t# Append positional data to dataframe created by single_site_data()\n",
    "\t\t\tadd_data = {'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va'], 'data_start': start, 'data_end': end, 'total_record': range}\t\t\t\n",
    "\t\t\tadd_data = pd.DataFrame(add_data, index=['0'])\n",
    "   \n",
    "\t\t\t# Duplicates the rows of add_data so that positional information is passed to each individual dataframe when this frame is split\n",
    "\t\t\tadd_data = pd.DataFrame(np.tile(add_data.values, (len(df_single_site_metric), 1)), columns=add_data.columns)\n",
    "\t\t\tdf_single_site_metric = pd.concat([df_single_site_metric.reset_index(drop=True), add_data.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\t\t\t# Append single site data to multi-site dataframes\n",
    "\t\t\tdf_multi_site_metric = pd.concat([df_multi_site_metric, df_single_site_metric], ignore_index=True)\n",
    "\t\t\tdf_multi_site_mk_mag = pd.concat([df_multi_site_mk_mag, df_mk_mag], ignore_index=True)\n",
    "\t\t\tdf_multi_site_mk_dur = pd.concat([df_multi_site_mk_dur, df_mk_dur], ignore_index=True)\n",
    "\t\t\tdf_multi_site_mk_intra = pd.concat([df_multi_site_mk_intra, df_mk_intra], ignore_index=True)   \n",
    "   \t\t\n",
    "\t\t\tprint(f'Added site {index + 1} of {len(df_state_sites)}')\n",
    "\t\t\t\n",
    "\t\t\t#clear_output(wait=True)\n",
    "\t\t\t#time.sleep(0.500)\n",
    "\t\t\tbreak\n",
    "\n",
    "\treturn df_multi_site_metric, df_multi_site_mk_mag, df_multi_site_mk_dur, df_multi_site_mk_intra, site_blacklist\n",
    "\n",
    "\n",
    "df_multi_site_metric, df_multi_site_mk_mag, df_multi_site_mk_dur, df_multi_site_mk_intra, site_blacklist = create_multi_site_data(df_state_sites, site_limit, cl.central_valley_aquifer)\n",
    "df_multi_site, df_invalid_site = fn.filter_by_valid(df_multi_site_metric)\n",
    "#df_multi_site.to_csv('df_multi_site.csv')\n",
    "#print(df_multi_site)\n",
    "\n",
    "print(f'Max HMF for this region: {df_multi_site[\"annual_hmf\"].max():.1f}')\n",
    "print(f'{len(df_multi_site)} site(s) valid out of {len(df_multi_site_metric)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE: Aquifer Analysis\n",
    "This section generates aquifer-wide data based on all watershed boundary shapefiles that intersect with the aquifer boundary. In addition it contains a section to generate a map of this aquifer boundary, the intersecting watershed boundaries, and their relevant water gauge sites and annual high magnitude flows<br><br>\n",
    "Aquifer Analysis Controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the aquifer to collect data for\n",
    "curr_aquifer = cl.coastal_lowlands_aquifer\n",
    "\n",
    "# Limit saved sites for testing purposes\n",
    "# Set to >=999 to analyze all sites\n",
    "site_limit = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Creation: \n",
    "<li>This code is time-consuming and can be bypassed if a spreadsheet for an aquifer already exists and all that is desired is a plot based on that data</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_0314\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Trying:  AR\n",
      "Trying:  OK\n",
      "Trying:  LA\n",
      "Trying:  MS\n",
      "Trying:  FL\n",
      "Added site 1 of 24\n",
      "IGNORED: Not enough data for site 02365200\n",
      "IGNORED: Not enough data for site 02365470\n",
      "Added site 4 of 24\n",
      "IGNORED: Not enough data for site 02365769\n",
      "Added site 6 of 24\n",
      "Added site 7 of 24\n",
      "IGNORED: Not enough data for site 02366996\n",
      "Added site 9 of 24\n",
      "IGNORED: Not enough data for site 02367900\n",
      "Added site 11 of 24\n",
      "Added site 12 of 24\n",
      "Added site 13 of 24\n",
      "IGNORED: Not enough data for site 02369600\n",
      "Added site 15 of 24\n",
      "Added site 16 of 24\n",
      "Added site 17 of 24\n",
      "IGNORED: Not enough data for site 02376033\n",
      "IGNORED: Not enough data for site 02376100\n",
      "IGNORED: Not enough data for site 02376115\n",
      "IGNORED: Not enough data for site 02376293\n",
      "Added site 22 of 24\n",
      "IGNORED: Not enough data for site 295308085143700\n",
      "IGNORED: No data for site 295323085151700\n",
      "Trying:  AL\n",
      "Added site 1 of 21\n",
      "Added site 2 of 21\n",
      "Added site 3 of 21\n",
      "Added site 4 of 21\n",
      "Added site 5 of 21\n",
      "Added site 6 of 21\n",
      "IGNORED: Not enough data for site 02372250\n",
      "IGNORED: Not enough data for site 02372422\n",
      "IGNORED: Not enough data for site 02372430\n",
      "Added site 10 of 21\n",
      "IGNORED: Not enough data for site 02374250\n",
      "Added site 12 of 21\n",
      "IGNORED: Not enough data for site 02374700\n",
      "IGNORED: Not enough data for site 02374745\n",
      "IGNORED: Not enough data for site 02374950\n",
      "Added site 16 of 21\n",
      "IGNORED: Not enough data for site 02376115\n",
      "Added site 18 of 21\n",
      "IGNORED: Not enough data for site 02377570\n",
      "IGNORED: Not enough data for site 02378170\n",
      "IGNORED: No data for site 02378185\n",
      "Trying:  GA\n",
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_0315\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Trying:  AR\n",
      "Trying:  OK\n",
      "Trying:  LA\n",
      "Trying:  MS\n",
      "Trying:  FL\n",
      "Trying:  AL\n",
      "IGNORED: Not enough data for site 02397530\n",
      "Added site 2 of 60\n",
      "Added site 3 of 60\n",
      "Added site 4 of 60\n",
      "IGNORED: Not enough data for site 02400680\n",
      "Added site 6 of 60\n",
      "Added site 7 of 60\n",
      "IGNORED: Not enough data for site 02401895\n",
      "IGNORED: No data for site 02403170\n",
      "IGNORED: Not enough data for site 02403310\n",
      "Added site 11 of 60\n",
      "IGNORED: No 00060_Mean data for site 02405500\n",
      "Added site 13 of 60\n",
      "IGNORED: Not enough data for site 02406930\n",
      "Added site 15 of 60\n",
      "IGNORED: Not enough data for site 02407514\n",
      "IGNORED: Not enough data for site 02408150\n",
      "IGNORED: Not enough data for site 02408540\n",
      "IGNORED: No data for site 02411590\n",
      "IGNORED: Not enough data for site 02411930\n",
      "Added site 21 of 60\n",
      "IGNORED: Not enough data for site 02413300\n",
      "IGNORED: No data for site 02414300\n",
      "Added site 24 of 60\n",
      "IGNORED: Not enough data for site 02414715\n",
      "Added site 26 of 60\n",
      "IGNORED: No data for site 02416360\n",
      "IGNORED: Not enough data for site 02418230\n",
      "IGNORED: Not enough data for site 02418760\n",
      "Added site 30 of 60\n",
      "IGNORED: Not enough data for site 02419890\n",
      "Added site 32 of 60\n",
      "Added site 33 of 60\n",
      "IGNORED: No data for site 02421480\n",
      "Added site 35 of 60\n",
      "IGNORED: No data for site 02423110\n",
      "IGNORED: Not enough data for site 02423130\n",
      "IGNORED: Not enough data for site 02423160\n",
      "IGNORED: Not enough data for site 02423380\n",
      "IGNORED: Not enough data for site 02423397\n",
      "IGNORED: Not enough data for site 02423400\n",
      "IGNORED: Not enough data for site 02423414\n",
      "IGNORED: Not enough data for site 02423425\n",
      "IGNORED: Not enough data for site 02423496\n",
      "Added site 45 of 60\n",
      "IGNORED: Not enough data for site 0242354650\n",
      "IGNORED: Not enough data for site 0242354750\n",
      "IGNORED: Not enough data for site 02423555\n",
      "IGNORED: Not enough data for site 02423571\n",
      "IGNORED: Not enough data for site 02423586\n",
      "Added site 51 of 60\n",
      "IGNORED: Not enough data for site 02423647\n",
      "IGNORED: No 00060_Mean data for site 02423800\n",
      "Added site 54 of 60\n",
      "Added site 55 of 60\n",
      "IGNORED: No data for site 02427020\n",
      "IGNORED: Not enough data for site 02427250\n",
      "IGNORED: Not enough data for site 02428400\n",
      "IGNORED: No data for site 02429000\n",
      "IGNORED: No data for site 02429540\n",
      "Trying:  GA\n",
      "Added site 1 of 49\n",
      "Added site 2 of 49\n",
      "IGNORED: Not enough data for site 02381090\n",
      "IGNORED: Not enough data for site 02381600\n",
      "IGNORED: Not enough data for site 02382200\n",
      "Added site 6 of 49\n",
      "Added site 7 of 49\n",
      "IGNORED: Not enough data for site 02384500\n",
      "IGNORED: Not enough data for site 02384540\n",
      "IGNORED: Not enough data for site 02385170\n",
      "Added site 11 of 49\n",
      "Added site 12 of 49\n",
      "Added site 13 of 49\n",
      "Added site 14 of 49\n",
      "IGNORED: Not enough data for site 02387600\n",
      "IGNORED: Not enough data for site 02388320\n",
      "IGNORED: Not enough data for site 02388350\n",
      "Added site 18 of 49\n",
      "IGNORED: Not enough data for site 02388975\n",
      "IGNORED: No data for site 02389050\n",
      "IGNORED: Not enough data for site 02389150\n",
      "Added site 22 of 49\n",
      "IGNORED: Not enough data for site 02390050\n",
      "IGNORED: Not enough data for site 02390140\n",
      "IGNORED: Not enough data for site 02391540\n",
      "IGNORED: Not enough data for site 02391840\n",
      "IGNORED: Not enough data for site 02391860\n",
      "Added site 28 of 49\n",
      "IGNORED: Not enough data for site 02392360\n",
      "IGNORED: Not enough data for site 02392780\n",
      "IGNORED: Not enough data for site 02392920\n",
      "IGNORED: Not enough data for site 02392950\n",
      "IGNORED: Not enough data for site 02392975\n",
      "IGNORED: Not enough data for site 02393377\n",
      "IGNORED: Not enough data for site 02393419\n",
      "Added site 36 of 49\n",
      "IGNORED: Not enough data for site 02394670\n",
      "IGNORED: No data for site 02394682\n",
      "IGNORED: Not enough data for site 02394820\n",
      "Added site 40 of 49\n",
      "IGNORED: Not enough data for site 02395120\n",
      "Added site 42 of 49\n",
      "Added site 43 of 49\n",
      "Added site 44 of 49\n",
      "IGNORED: Not enough data for site 02397530\n",
      "Added site 46 of 49\n",
      "IGNORED: Not enough data for site 02411930\n",
      "Added site 48 of 49\n",
      "IGNORED: Not enough data for site 02413210\n",
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_0316\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Trying:  AR\n",
      "Trying:  OK\n",
      "Trying:  LA\n",
      "Trying:  MS\n",
      "IGNORED: No data for site 02430005\n",
      "IGNORED: Not enough data for site 02430085\n",
      "IGNORED: No data for site 02430161\n",
      "IGNORED: No data for site 02430626\n",
      "IGNORED: Not enough data for site 02430680\n",
      "IGNORED: Not enough data for site 02430880\n",
      "Added site 7 of 21\n",
      "IGNORED: No data for site 02431011\n",
      "IGNORED: Not enough data for site 02432500\n",
      "IGNORED: No data for site 02433151\n",
      "IGNORED: No data for site 02433496\n",
      "Added site 12 of 21\n",
      "Added site 13 of 21\n",
      "Added site 14 of 21\n",
      "IGNORED: No data for site 02437000\n",
      "IGNORED: Not enough data for site 02437100\n",
      "Added site 17 of 21\n",
      "Added site 18 of 21\n",
      "Added site 19 of 21\n",
      "Added site 20 of 21\n",
      "Added site 21 of 21\n",
      "Trying:  FL\n",
      "Trying:  AL\n",
      "IGNORED: Not enough data for site 02378300\n",
      "Added site 2 of 54\n",
      "IGNORED: No data for site 0237854520\n",
      "IGNORED: Not enough data for site 02378780\n",
      "Added site 5 of 54\n",
      "Added site 6 of 54\n",
      "IGNORED: Not enough data for site 02444160\n",
      "Added site 8 of 54\n",
      "IGNORED: Not enough data for site 02447025\n",
      "Added site 10 of 54\n",
      "IGNORED: Not enough data for site 02448900\n",
      "IGNORED: Not enough data for site 02449838\n",
      "IGNORED: Not enough data for site 02449882\n",
      "Added site 14 of 54\n",
      "IGNORED: Not enough data for site 02450180\n",
      "Added site 16 of 54\n",
      "IGNORED: Not enough data for site 02450825\n",
      "Added site 18 of 54\n",
      "Added site 19 of 54\n",
      "IGNORED: Not enough data for site 02454000\n",
      "Added site 21 of 54\n",
      "IGNORED: Not enough data for site 02455185\n",
      "IGNORED: Not enough data for site 02455980\n",
      "Added site 24 of 54\n",
      "Added site 25 of 54\n",
      "IGNORED: Not enough data for site 02457595\n",
      "IGNORED: Not enough data for site 02457704\n",
      "IGNORED: Not enough data for site 02458148\n",
      "IGNORED: Not enough data for site 02458190\n",
      "IGNORED: Not enough data for site 02458450\n",
      "IGNORED: Not enough data for site 02458502\n",
      "IGNORED: Not enough data for site 02458600\n",
      "IGNORED: Not enough data for site 02460500\n",
      "IGNORED: No data for site 02461130\n",
      "IGNORED: Not enough data for site 02461192\n",
      "IGNORED: No data for site 02461405\n",
      "IGNORED: Not enough data for site 02461500\n",
      "Added site 38 of 54\n",
      "Added site 39 of 54\n",
      "Added site 40 of 54\n",
      "IGNORED: Not enough data for site 02465292\n",
      "IGNORED: Not enough data for site 02465493\n",
      "IGNORED: Not enough data for site 02466030\n",
      "IGNORED: No 00060_Mean data for site 02466500\n",
      "Added site 45 of 54\n",
      "Added site 46 of 54\n",
      "IGNORED: Not enough data for site 02469525\n",
      "Added site 48 of 54\n",
      "Added site 49 of 54\n",
      "IGNORED: Not enough data for site 02470072\n",
      "IGNORED: No data for site 02470629\n",
      "Added site 52 of 54\n",
      "IGNORED: No data for site 02471019\n",
      "IGNORED: Not enough data for site 02471078\n",
      "Trying:  GA\n",
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_0317\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Trying:  AR\n",
      "Trying:  OK\n",
      "Trying:  LA\n",
      "Trying:  MS\n",
      "Added site 1 of 25\n",
      "Added site 2 of 25\n",
      "IGNORED: Not enough data for site 02472850\n",
      "IGNORED: Not enough data for site 02472980\n",
      "Added site 5 of 25\n",
      "IGNORED: Not enough data for site 02473460\n",
      "Added site 7 of 25\n",
      "Added site 8 of 25\n",
      "IGNORED: Not enough data for site 02474560\n",
      "Added site 10 of 25\n",
      "Added site 11 of 25\n",
      "Added site 12 of 25\n",
      "Added site 13 of 25\n",
      "Added site 14 of 25\n",
      "IGNORED: No data for site 02477045\n",
      "Added site 16 of 25\n",
      "Added site 17 of 25\n",
      "Added site 18 of 25\n",
      "Added site 19 of 25\n",
      "Added site 20 of 25\n",
      "Added site 21 of 25\n",
      "IGNORED: No 00060_Mean data for site 02479310\n",
      "IGNORED: No 00060_Mean data for site 02479500\n",
      "Added site 24 of 25\n",
      "IGNORED: Not enough data for site 02481510\n",
      "Trying:  FL\n",
      "Trying:  AL\n",
      "IGNORED: No 00060_Mean data for site 02479500\n",
      "IGNORED: Not enough data for site 02479945\n",
      "IGNORED: Not enough data for site 02479980\n",
      "IGNORED: Not enough data for site 02480002\n",
      "Trying:  GA\n",
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_0318\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Trying:  AR\n",
      "Trying:  OK\n",
      "Trying:  LA\n",
      "IGNORED: Not enough data for site 02489000\n",
      "Added site 2 of 6\n",
      "IGNORED: No 00060_Mean data for site 02490500\n",
      "IGNORED: No data for site 02491800\n",
      "Added site 5 of 6\n",
      "IGNORED: No 00060_Mean data for site 02492111\n",
      "Trying:  MS\n",
      "IGNORED: Not enough data for site 02481880\n",
      "Added site 2 of 25\n",
      "IGNORED: Not enough data for site 02482470\n",
      "Added site 4 of 25\n",
      "IGNORED: Not enough data for site 02483000\n",
      "IGNORED: Not enough data for site 02483500\n",
      "Added site 7 of 25\n",
      "Added site 8 of 25\n",
      "IGNORED: No data for site 02484600\n",
      "IGNORED: Not enough data for site 02484760\n",
      "IGNORED: No data for site 02485498\n",
      "IGNORED: Not enough data for site 02485700\n",
      "IGNORED: Not enough data for site 02485735\n",
      "Added site 14 of 25\n",
      "IGNORED: No data for site 02486100\n",
      "Added site 16 of 25\n",
      "Added site 17 of 25\n",
      "Added site 18 of 25\n",
      "IGNORED: Not enough data for site 02489000\n",
      "Added site 20 of 25\n",
      "IGNORED: No 00060_Mean data for site 02490500\n",
      "IGNORED: No 00060_Mean data for site 02492111\n",
      "IGNORED: Not enough data for site 02492343\n",
      "IGNORED: Not enough data for site 02492360\n",
      "IGNORED: No data for site 02492620\n",
      "Trying:  FL\n",
      "Trying:  AL\n",
      "Trying:  GA\n",
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_0804\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Trying:  AR\n",
      "IGNORED: No data for site 07355860\n",
      "IGNORED: Not enough data for site 07355870\n",
      "Added site 3 of 29\n",
      "Added site 4 of 29\n",
      "IGNORED: Not enough data for site 07358253\n",
      "IGNORED: Not enough data for site 07358257\n",
      "IGNORED: No 00060_Mean data for site 07358280\n",
      "IGNORED: Not enough data for site 07358284\n",
      "IGNORED: Not enough data for site 07358550\n",
      "IGNORED: No data for site 07358570\n",
      "Added site 11 of 29\n",
      "IGNORED: Not enough data for site 07359610\n",
      "IGNORED: Not enough data for site 07360200\n",
      "Added site 14 of 29\n",
      "Added site 15 of 29\n",
      "Added site 16 of 29\n",
      "Added site 17 of 29\n",
      "IGNORED: Not enough data for site 07362579\n",
      "IGNORED: Not enough data for site 07362587\n",
      "Added site 20 of 29\n",
      "IGNORED: Not enough data for site 07363054\n",
      "Added site 22 of 29\n",
      "IGNORED: Not enough data for site 07363400\n",
      "Added site 24 of 29\n",
      "IGNORED: Not enough data for site 07364078\n",
      "IGNORED: No data for site 07364130\n",
      "IGNORED: Not enough data for site 07364133\n",
      "Added site 28 of 29\n",
      "IGNORED: Not enough data for site 07364185\n",
      "Trying:  OK\n",
      "Trying:  LA\n",
      "IGNORED: No data for site 073556009\n",
      "IGNORED: Not enough data for site 07364078\n",
      "Added site 3 of 12\n",
      "Added site 4 of 12\n",
      "IGNORED: No 00060_Mean data for site 07365000\n",
      "Added site 6 of 12\n",
      "IGNORED: Not enough data for site 07367005\n",
      "IGNORED: Not enough data for site 07370600\n",
      "IGNORED: Not enough data for site 07372050\n",
      "Added site 10 of 12\n",
      "Added site 11 of 12\n",
      "IGNORED: Not enough data for site 07381482\n",
      "Trying:  MS\n",
      "Trying:  FL\n",
      "Trying:  AL\n",
      "Trying:  GA\n",
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_0805\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Trying:  AR\n",
      "IGNORED: Not enough data for site 07367680\n",
      "IGNORED: Not enough data for site 07369680\n",
      "Trying:  OK\n",
      "Trying:  LA\n",
      "IGNORED: Not enough data for site 07367690\n",
      "IGNORED: No data for site 07367930\n",
      "Added site 3 of 7\n",
      "IGNORED: No data for site 07368503\n",
      "Added site 5 of 7\n",
      "Added site 6 of 7\n",
      "Added site 7 of 7\n",
      "Trying:  MS\n",
      "Trying:  FL\n",
      "Trying:  AL\n",
      "Trying:  GA\n",
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_0806\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Trying:  AR\n",
      "Trying:  OK\n",
      "Trying:  LA\n",
      "Trying:  MS\n",
      "IGNORED: Not enough data for site 07289350\n",
      "IGNORED: Not enough data for site 07289730\n",
      "Added site 3 of 7\n",
      "Added site 4 of 7\n",
      "Added site 5 of 7\n",
      "Added site 6 of 7\n",
      "Added site 7 of 7\n",
      "Trying:  FL\n",
      "Trying:  AL\n",
      "Trying:  GA\n",
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_0807\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Trying:  AR\n",
      "Trying:  OK\n",
      "Trying:  LA\n",
      "IGNORED: Not enough data for site 07374000\n",
      "IGNORED: Not enough data for site 07375280\n",
      "IGNORED: No data for site 07375300\n",
      "IGNORED: No data for site 07375450\n",
      "IGNORED: No data for site 07375480\n",
      "Added site 6 of 21\n",
      "Added site 7 of 21\n",
      "IGNORED: Not enough data for site 07375960\n",
      "Added site 9 of 21\n",
      "IGNORED: No data for site 07376300\n",
      "Added site 11 of 21\n",
      "Added site 12 of 21\n",
      "IGNORED: Not enough data for site 07377600\n",
      "Added site 14 of 21\n",
      "Added site 15 of 21\n",
      "IGNORED: Not enough data for site 07380120\n",
      "IGNORED: No data for site 07380210\n",
      "IGNORED: No data for site 073802280\n",
      "IGNORED: No data for site 073802284\n",
      "IGNORED: No data for site 07381427\n",
      "IGNORED: No data for site 07381429\n",
      "Trying:  MS\n",
      "IGNORED: Not enough data for site 07375280\n",
      "Added site 2 of 2\n",
      "Trying:  FL\n",
      "Trying:  AL\n",
      "Trying:  GA\n",
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_0808\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Trying:  AR\n",
      "Trying:  OK\n",
      "Trying:  LA\n",
      "IGNORED: Not enough data for site 07381490\n",
      "IGNORED: No data for site 07381588\n",
      "IGNORED: Not enough data for site 07381590\n",
      "IGNORED: Not enough data for site 07381600\n",
      "Added site 5 of 26\n",
      "Added site 6 of 26\n",
      "Added site 7 of 26\n",
      "Added site 8 of 26\n",
      "IGNORED: Not enough data for site 07385765\n",
      "IGNORED: No data for site 07386190\n",
      "IGNORED: No 00060_Mean data for site 07386700\n",
      "IGNORED: Not enough data for site 07386880\n",
      "IGNORED: No data for site 07386956\n",
      "IGNORED: Not enough data for site 07386980\n",
      "Added site 15 of 26\n",
      "IGNORED: No data for site 08010050\n",
      "IGNORED: No data for site 08010120\n",
      "Added site 18 of 26\n",
      "IGNORED: Not enough data for site 08012150\n",
      "IGNORED: Not enough data for site 08012470\n",
      "Added site 21 of 26\n",
      "IGNORED: No data for site 08013250\n",
      "Added site 23 of 26\n",
      "Added site 24 of 26\n",
      "Added site 25 of 26\n",
      "Added site 26 of 26\n",
      "Trying:  MS\n",
      "Trying:  FL\n",
      "Trying:  AL\n",
      "Trying:  GA\n",
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_0809\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Trying:  AR\n",
      "Trying:  OK\n",
      "Trying:  LA\n",
      "IGNORED: Not enough data for site 07374525\n",
      "Added site 2 of 8\n",
      "IGNORED: Not enough data for site 07381000\n",
      "IGNORED: Not enough data for site 07381324\n",
      "IGNORED: Not enough data for site 073814675\n",
      "IGNORED: No data for site 292939089544400\n",
      "IGNORED: Not enough data for site 295124089542100\n",
      "IGNORED: No data for site 295501090190400\n",
      "Trying:  MS\n",
      "Trying:  FL\n",
      "Trying:  AL\n",
      "Trying:  GA\n",
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_1114\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Added site 1 of 23\n",
      "IGNORED: Not enough data for site 07332605\n",
      "IGNORED: Not enough data for site 07332622\n",
      "IGNORED: Not enough data for site 07332655\n",
      "IGNORED: Not enough data for site 07336820\n",
      "IGNORED: Not enough data for site 07342470\n",
      "IGNORED: Not enough data for site 07342480\n",
      "Added site 8 of 23\n",
      "Added site 9 of 23\n",
      "Added site 10 of 23\n",
      "IGNORED: Not enough data for site 07343356\n",
      "IGNORED: Not enough data for site 07343450\n",
      "Added site 13 of 23\n",
      "IGNORED: Not enough data for site 07343840\n",
      "IGNORED: Not enough data for site 07344100\n",
      "IGNORED: Not enough data for site 07344210\n",
      "IGNORED: Not enough data for site 07344493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n",
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n",
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n",
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added site 18 of 23\n",
      "Added site 19 of 23\n",
      "Added site 20 of 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n",
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n",
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n",
      "c:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymannkendall\\pymannkendall.py:155: RuntimeWarning: invalid value encountered in subtract\n",
      "  d[idx : idx + len(j)] = (x[j] - x[i]) / (j - i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added site 21 of 23\n",
      "Added site 22 of 23\n",
      "IGNORED: Not enough data for site 07346080\n",
      "Trying:  AR\n",
      "Added site 1 of 6\n",
      "Added site 2 of 6\n",
      "Added site 3 of 6\n",
      "Added site 4 of 6\n",
      "ERROR:  float division by zero\n",
      "https://waterdata.usgs.gov/AR/nwis/current?index_pmcode_STATION_NM=1&index_pmcode_DATETIME=2&index_pmcode_00060=3&group_key=NONE&format=sitefile_output&sitefile_output_format=rdb&column_name=site_no&column_name=station_nm&column_name=dec_lat_va&column_name=dec_long_va&column_name=sv_begin_date&column_name=sv_end_date&sort_key_2=site_no&html_table_group_key=NONE&rdb_compression=file&list_of_search_criteria=realtime_parameter_selection\n",
      "Trying:  OK\n",
      "Added site 1 of 20\n",
      "IGNORED: Not enough data for site 07332307\n",
      "IGNORED: Not enough data for site 07332348\n",
      "IGNORED: Not enough data for site 07332389\n",
      "IGNORED: Not enough data for site 07332390\n",
      "Added site 6 of 20\n",
      "Added site 7 of 20\n",
      "IGNORED: Not enough data for site 07334238\n",
      "IGNORED: Not enough data for site 07334428\n",
      "IGNORED: Not enough data for site 07334800\n",
      "IGNORED: Not enough data for site 07335300\n",
      "Added site 12 of 20\n",
      "Added site 13 of 20\n",
      "IGNORED: Not enough data for site 07335790\n",
      "IGNORED: Not enough data for site 07336200\n",
      "IGNORED: Not enough data for site 07336820\n",
      "Added site 17 of 20\n",
      "Added site 18 of 20\n",
      "IGNORED: Not enough data for site 07338750\n",
      "Added site 20 of 20\n",
      "Trying:  LA\n",
      "IGNORED: Not enough data for site 07344370\n",
      "IGNORED: No 00060_Mean data for site 07344450\n",
      "Added site 3 of 8\n",
      "IGNORED: Not enough data for site 07349450\n",
      "IGNORED: Not enough data for site 07349860\n",
      "Added site 6 of 8\n",
      "IGNORED: Not enough data for site 07351750\n",
      "Added site 8 of 8\n",
      "Trying:  MS\n",
      "Trying:  FL\n",
      "Trying:  AL\n",
      "Trying:  GA\n",
      "Path:  ShapeFiles/Aquifers/Coastal_Lowlands/HUC4\\NHD_H_1201\\WBDHU4.shp\n",
      "Trying:  TX\n",
      "Added site 1 of 28\n",
      "IGNORED: No data for site 08017250\n",
      "Added site 3 of 28\n",
      "Added site 4 of 28\n",
      "IGNORED: No data for site 08017605\n",
      "IGNORED: No data for site 08017900\n",
      "IGNORED: No data for site 08018000\n",
      "Added site 8 of 28\n",
      "Added site 9 of 28\n",
      "IGNORED: Not enough data for site 08019200\n",
      "Added site 11 of 28\n",
      "Added site 12 of 28\n",
      "IGNORED: Not enough data for site 08020450\n",
      "Added site 14 of 28\n",
      "IGNORED: Not enough data for site 08020900\n",
      "Added site 16 of 28\n",
      "IGNORED: No data for site 08022120\n",
      "ERROR:  float division by zero\n",
      "https://waterdata.usgs.gov/TX/nwis/current?index_pmcode_STATION_NM=1&index_pmcode_DATETIME=2&index_pmcode_00060=3&group_key=NONE&format=sitefile_output&sitefile_output_format=rdb&column_name=site_no&column_name=station_nm&column_name=dec_lat_va&column_name=dec_long_va&column_name=sv_begin_date&column_name=sv_end_date&sort_key_2=site_no&html_table_group_key=NONE&rdb_compression=file&list_of_search_criteria=realtime_parameter_selection\n",
      "Trying:  AR\n",
      "Trying:  OK\n",
      "Trying:  LA\n",
      "ERROR:  float division by zero\n",
      "https://waterdata.usgs.gov/LA/nwis/current?index_pmcode_STATION_NM=1&index_pmcode_DATETIME=2&index_pmcode_00060=3&group_key=NONE&format=sitefile_output&sitefile_output_format=rdb&column_name=site_no&column_name=station_nm&column_name=dec_lat_va&column_name=dec_long_va&column_name=sv_begin_date&column_name=sv_end_date&sort_key_2=site_no&html_table_group_key=NONE&rdb_compression=file&list_of_search_criteria=realtime_parameter_selection\n",
      "Trying:  MS\n",
      "Trying:  FL\n",
      "Trying:  AL\n",
      "Trying:  GA\n"
     ]
    }
   ],
   "source": [
    "# Data Creation\n",
    "df_aq_sites_metric = pd.DataFrame()\n",
    "df_aq_sites_mk_mag = pd.DataFrame()\n",
    "df_aq_sites_mk_dur = pd.DataFrame()\n",
    "df_aq_sites_mk_intra = pd.DataFrame()\n",
    "master_blacklist = []\n",
    "\n",
    "outer = \"outer\"\n",
    "for root, dirs, files in os.walk(curr_aquifer.wb_dir):\n",
    "    #print(root, dirs, files)\n",
    "    if os.path.basename(root).startswith('NHD_H_'):\n",
    "        if curr_aquifer.wb_shapefiles in files:\n",
    "            shapefile_aq_path = os.path.join(root, curr_aquifer.wb_shapefiles)\n",
    "            ws_gdf = gpd.read_file(shapefile_aq_path)\n",
    "            ws_gdf = ws_gdf.to_crs(4269)           \n",
    "            print(\"Path: \", shapefile_aq_path)\n",
    "            \n",
    "            for state_code in curr_aquifer.states:\n",
    "                print(\"Trying: \", state_code)\n",
    "                try:\n",
    "                    state_uri = fn.create_state_uri(state_code, fn.PARAM_CODE)\n",
    "                    df_state_sites = filter_state_site(shapefile_aq_path, state_uri)\n",
    "                    df_temp_metric, df_temp_mk_mag, df_temp_mk_dur, df_temp_mk_intra, blacklist = create_multi_site_data(df_state_sites, site_limit, curr_aquifer)\n",
    "                    master_blacklist.extend(blacklist)\n",
    "                    df_temp_metric['State'] = state_code\n",
    "                    df_aq_sites_metric = pd.concat([df_aq_sites_metric, df_temp_metric], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "                    df_aq_sites_mk_mag = pd.concat([df_aq_sites_mk_mag, df_temp_mk_mag], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "                    df_aq_sites_mk_dur = pd.concat([df_aq_sites_mk_dur, df_temp_mk_dur], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "                    df_aq_sites_mk_intra = pd.concat([df_aq_sites_mk_intra, df_temp_mk_intra], axis=0, ignore_index=True).reset_index(drop=True)                    \n",
    "                     \n",
    "                except Exception as e:\n",
    "                    print(\"ERROR: \", e)\n",
    "                    print(state_uri)\n",
    "\n",
    "# Filtering out invalid sites by data range and duplicates as some sites are listed in 2+ states   \n",
    "# Also adds a column to indicate presence in HCDN-2009\n",
    "#df_aq_sites = df_aq_sites.drop_duplicates(subset=['site_no'])\n",
    "df_aq_sites_metric = fn.gages_2_filtering(df_aq_sites_metric)\n",
    "\n",
    "try:\n",
    "    with open(f'Prelim_Data/{curr_aquifer.name}_Blacklist.txt', 'w') as f:\n",
    "        master_blacklist = [\"'\" + str(x) + \"'\" for x in master_blacklist]        \n",
    "        f.write(', '.join(master_blacklist))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "#df_aq_sites_metric.to_csv(f'{curr_aquifer.name}_Raw.csv') \n",
    "\n",
    "fn.save_data(df_aq_sites_metric, df_aq_sites_mk_mag, df_aq_sites_mk_dur, df_aq_sites_mk_intra, curr_aquifer.name)  \n",
    "#print(df_aq_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: Nationwide Validity Dataset\n",
    "Generates a dataset for every water gauge with 00060_Mean data in the contiguous US indicating gauge validity for this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "date_range = 50\n",
    "shapefile_path = 'ShapeFiles/Lower48/lower48.shp'\n",
    "test_limit = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---Working on AL---]\n",
      "Total Sites: 173 in the state of AL\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m ignored_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loc, row \u001b[38;5;129;01min\u001b[39;00m df_state_sites\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 12\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mnwis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43msites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msite_no\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSERVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameterCD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPARAM_CODE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT_START\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT_END\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m00060_Mean\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nwis.py:1007\u001b[0m, in \u001b[0;36mget_record\u001b[1;34m(sites, start, end, multi_index, wide_format, datetime_index, state, service, **kwargs)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m service \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1007\u001b[0m     df, _ \u001b[38;5;241m=\u001b[39m get_dv(sites\u001b[38;5;241m=\u001b[39msites, startDT\u001b[38;5;241m=\u001b[39mstart, endDT\u001b[38;5;241m=\u001b[39mend,\n\u001b[0;32m   1008\u001b[0m                    multi_index\u001b[38;5;241m=\u001b[39mmulti_index, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m service \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqwdata\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nwis.py:544\u001b[0m, in \u001b[0;36mget_dv\u001b[1;34m(sites, start, end, multi_index, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m end \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendDT\u001b[39m\u001b[38;5;124m'\u001b[39m, end)\n\u001b[0;32m    543\u001b[0m sites \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msites\u001b[39m\u001b[38;5;124m'\u001b[39m, sites)\n\u001b[1;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _dv(startDT\u001b[38;5;241m=\u001b[39mstart, endDT\u001b[38;5;241m=\u001b[39mend, sites\u001b[38;5;241m=\u001b[39msites,\n\u001b[0;32m    545\u001b[0m            multi_index\u001b[38;5;241m=\u001b[39mmulti_index, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nwis.py:549\u001b[0m, in \u001b[0;36m_dv\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dv\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 549\u001b[0m     response \u001b[38;5;241m=\u001b[39m query_waterservices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    550\u001b[0m     df \u001b[38;5;241m=\u001b[39m _read_json(response\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m format_response(df, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), _set_metadata(response, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\nwis.py:498\u001b[0m, in \u001b[0;36mquery_waterservices\u001b[1;34m(service, **kwargs)\u001b[0m\n\u001b[0;32m    494\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrdb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    496\u001b[0m url \u001b[38;5;241m=\u001b[39m WATERSERVICE_URL \u001b[38;5;241m+\u001b[39m service\n\u001b[1;32m--> 498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dataretrieval\\utils.py:196\u001b[0m, in \u001b[0;36mquery\u001b[1;34m(url, payload, delimiter)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m#for index in range(len(payload)):\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m#    key, value = payload[index]\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m#    payload[index] = (key, to_str(value))\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# define the user agent for the query\u001b[39;00m\n\u001b[0;32m    193\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython-dataretrieval/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataretrieval\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m--> 196\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad Request, check that your parameters are correct. URL: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(response\u001b[38;5;241m.\u001b[39murl))\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content):\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:831\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 831\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    832\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m    833\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    834\u001b[0m )\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:784\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    782\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# amt > self.chunk_left\u001b[39;00m\n\u001b[1;32m--> 784\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_left\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Toss the CRLF at the end of the chunk.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:631\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[0;32m    625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\alekh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_natl_validity = pd.DataFrame()\n",
    "\n",
    "for i, state in enumerate(fn.STATE_LIST):\n",
    "    if i >= test_limit: break\n",
    "    print(f'[---Working on {state}---]')\n",
    "    state_uri = fn.create_state_uri(state, fn.PARAM_CODE)\n",
    "    df_state_sites = filter_state_site(shapefile_path, state_uri)\n",
    "    print(f'Total Sites: {len(df_state_sites)} in the state of {state.upper()}')\n",
    "    \n",
    "    ignored_count = 0\n",
    "    for loc, row in df_state_sites.iterrows():\n",
    "        df = nwis.get_record(sites=row['site_no'], service=fn.SERVICE, parameterCD=fn.PARAM_CODE, start=fn.DEFAULT_START, end=fn.DEFAULT_END)\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        if '00060_Mean' not in df.columns:\n",
    "            ignored_count += 1\n",
    "            continue\n",
    "        \n",
    "        start = df['datetime'].min().date()\n",
    "        end = df['datetime'].max().date()\n",
    "        range = round((end - start).days / 365.25, 1)\n",
    "        \n",
    "        if range < date_range: valid_range = False\n",
    "        else: valid_range = True\n",
    "        \n",
    "        date_threshold = pd.to_datetime(fn.DEFAULT_END).date() - timedelta(days=365.25 * date_range)\n",
    "        df = df[df['datetime'].dt.date >= date_threshold]\n",
    "        missing = fn.validate(df, date_threshold, fn.DEFAULT_END)\n",
    "        valid = missing < fn.MAX_MISSING_THRESHOLD\n",
    "        \n",
    "        data = {'site_no': row['site_no'], 'state': state, 'data_range': valid_range, 'data_cont': valid, 'start': start, 'end': end, 'total_record': range, 'missing': missing,\n",
    "                'dec_lat_va': row['dec_lat_va'], 'dec_long_va': row['dec_long_va']}\n",
    "        \n",
    "        df_natl_validity = pd.concat([df_natl_validity, pd.DataFrame(data, index=['0'])], axis=0, ignore_index=True)\n",
    "        \n",
    "    print(f'Ignored {ignored_count} sites ({round((ignored_count / len(df_state_sites)) * 100, 2)}%)')        \n",
    "        \n",
    "try:\n",
    "    with pd.ExcelWriter('Prelim_Data/National_Validity.xlsx') as writer:\n",
    "        df_natl_validity.to_excel(writer, sheet_name='national_validity', index=False) \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
